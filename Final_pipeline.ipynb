{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonanew/noise_detection/blob/main/Final_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JnrPJQx1Hrk",
        "outputId": "59f75a1e-89e9-4a87-c83d-c137866e68dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib) (3.2.3)\n",
            "Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/565.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdflib\n",
            "Successfully installed rdflib-7.1.4\n",
            "Collecting noise\n",
            "  Downloading noise-1.2.2.zip (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: noise\n",
            "  Building wheel for noise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for noise: filename=noise-1.2.2-cp311-cp311-linux_x86_64.whl size=56278 sha256=9c41a14f1f3428a8b523e5bc4605803eb92dd21c2919b0ad3a39d22a8400c881\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/25/2e/af6d1bcc91a8f99af0f651f8718b9ab999720a21c6d4149091\n",
            "Successfully built noise\n",
            "Installing collected packages: noise\n",
            "Successfully installed noise-1.2.2\n"
          ]
        }
      ],
      "source": [
        "# Install rdflib for KG\n",
        "!pip install rdflib\n",
        "\n",
        "# Install noise package for noise generation\n",
        "!pip install noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKAsTpiSzWhN",
        "outputId": "8134ce20-e9be-4396-8540-f5e9fff0ee25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N4boc4_ob5It"
      },
      "outputs": [],
      "source": [
        "import enum\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from rdflib import Graph, URIRef, Literal, Namespace\n",
        "from rdflib.namespace import RDF, RDFS, XSD\n",
        "import sys\n",
        "\n",
        "def networkx(rdf_graph):\n",
        "    \"\"\"\n",
        "    Convert an RDFLib graph to a NetworkX graph.\n",
        "    - Nodes: All individuals and ontology classes.\n",
        "    - Edges: Object properties create directed edges between nodes.\n",
        "    - Attributes: Data properties are stored as node attributes.\n",
        "    \"\"\"\n",
        "    if isinstance(rdf_graph, Graph) or isinstance(rdf_graph, str):\n",
        "        if isinstance(rdf_graph, Graph):\n",
        "            rdf_graph = rdf_graph\n",
        "        elif isinstance(rdf_graph, str):\n",
        "            if rdf_graph.endswith('.ttl'):\n",
        "                try:\n",
        "                    rdf_graph = Graph().parse(rdf_graph, format='turtle')\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing Turtle file: {e}\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Unexpected input format. Provide a Turtle (.ttl) file path.\")\n",
        "    else:\n",
        "        print(\"Unexpected input format. Provide an RDFLib Graph or a .ttl file path.\")\n",
        "        return None\n",
        "\n",
        "    # Initialize the directed graph\n",
        "    nx_graph = nx.DiGraph()\n",
        "\n",
        "    # Process the RDF graph\n",
        "    for s, p, o in rdf_graph:\n",
        "        s, p = s.toPython(), p.toPython()\n",
        "        # Ensure subject is a node\n",
        "        if s not in nx_graph:\n",
        "            nx_graph.add_node(s)\n",
        "\n",
        "        # Check if the object is a literal (data property) or an entity (object property)\n",
        "        if isinstance(o, Literal):  # Data property → store as a node attribute\n",
        "            nx_graph.nodes[s][p] = o.toPython()\n",
        "        else:  # Object property → add an edge\n",
        "            o = o.toPython()\n",
        "            # in case o is not a hashable type, parse it to string\n",
        "            if isinstance(o, enum.Enum):\n",
        "                o = str(o)\n",
        "            if p == RDF.type.toPython():\n",
        "                nx_graph.nodes[s][RDF.type.toPython()] = o\n",
        "            else:\n",
        "                nx_graph.add_edge(s, o, predicate=p)\n",
        "\n",
        "    return nx_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZnNQPI84DA2a"
      },
      "outputs": [],
      "source": [
        "import enum\n",
        "\n",
        "import networkx as nx\n",
        "from rdflib import Graph, URIRef, Literal, Namespace\n",
        "from rdflib.namespace import RDF, RDFS, XSD\n",
        "import sys\n",
        "\n",
        "def networkx(rdf_graph):\n",
        "    \"\"\"\n",
        "    Convert an RDFLib graph to a NetworkX graph.\n",
        "    - Nodes: All individuals and ontology classes.\n",
        "    - Edges: Object properties create directed edges between nodes.\n",
        "    - Attributes: Data properties are stored as node attributes.\n",
        "    \"\"\"\n",
        "    if isinstance(rdf_graph, Graph) or isinstance(rdf_graph, str):\n",
        "        if isinstance(rdf_graph, Graph):\n",
        "            rdf_graph = rdf_graph\n",
        "        elif isinstance(rdf_graph, str):\n",
        "            if rdf_graph.endswith('.ttl'):\n",
        "                try:\n",
        "                    rdf_graph = Graph().parse(rdf_graph, format='turtle')\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing Turtle file: {e}\")\n",
        "                    return None\n",
        "            else:\n",
        "                print(\"Unexpected input format. Provide a Turtle (.ttl) file path.\")\n",
        "    else:\n",
        "        print(\"Unexpected input format. Provide an RDFLib Graph or a .ttl file path.\")\n",
        "        return None\n",
        "\n",
        "    # Initialize the directed graph\n",
        "    nx_graph = nx.DiGraph()\n",
        "\n",
        "    # Process the RDF graph\n",
        "    for s, p, o in rdf_graph:\n",
        "        s, p = s.toPython(), p.toPython()\n",
        "        # Ensure subject is a node\n",
        "        if s not in nx_graph:\n",
        "            nx_graph.add_node(s)\n",
        "\n",
        "        # Check if the object is a literal (data property) or an entity (object property)\n",
        "        if isinstance(o, Literal):  # Data property → store as a node attribute\n",
        "            nx_graph.nodes[s][p] = o.toPython()\n",
        "        else:  # Object property → add an edge\n",
        "            o = o.toPython()\n",
        "            # in case o is not a hashable type, parse it to string\n",
        "            if isinstance(o, enum.Enum):\n",
        "                o = str(o)\n",
        "            if p == RDF.type.toPython():\n",
        "                nx_graph.nodes[s][RDF.type.toPython()] = o\n",
        "            else:\n",
        "                nx_graph.add_edge(s, o, predicate=p)\n",
        "\n",
        "    return nx_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j2j6QPOgcCqD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from rdflib import RDF\n",
        "\n",
        "def shorten_uri(uri, ontology_base):\n",
        "    \"\"\"\n",
        "    Extract the local name from a full URI.\n",
        "    Removes known ontology base and keeps suffix like Pipe_9 or diameter.\n",
        "    \"\"\"\n",
        "    if uri.startswith(ontology_base):\n",
        "        return uri.replace(ontology_base, \"\")\n",
        "    elif \"#\" in uri:\n",
        "        return uri.split(\"#\")[-1]\n",
        "    elif \"/\" in uri:\n",
        "        return uri.split(\"/\")[-1]\n",
        "    else:\n",
        "        return uri\n",
        "\n",
        "def extract_features_from_nx_graph(nx_graph, ontology_base, edge_predicates, node_type_filter, attribute_keys):\n",
        "\n",
        "    feature_dict = {}\n",
        "    rdf_type_key = shorten_uri(str(RDF.type), ontology_base)\n",
        "\n",
        "    for node, data in nx_graph.nodes(data=True):\n",
        "        #print(f\"Processing node: {node}, data: {data}\")\n",
        "\n",
        "        # Shorten RDF.type value and all keys\n",
        "        cleaned_data = {\n",
        "            shorten_uri(k, ontology_base): v if not isinstance(v, str) else shorten_uri(v, ontology_base)\n",
        "            for k, v in data.items()\n",
        "        }\n",
        "\n",
        "        #print(f\"Cleaned data: {cleaned_data}\")\n",
        "\n",
        "        # Filter by type if specified\n",
        "        node_type = cleaned_data.get(rdf_type_key)\n",
        "        if node_type_filter and node_type != node_type_filter:\n",
        "            continue\n",
        "\n",
        "        # Shorten node URI\n",
        "        node_id = shorten_uri(node, ontology_base)\n",
        "\n",
        "        # Select attributes\n",
        "        if attribute_keys:\n",
        "            features = {k: cleaned_data[k] for k in attribute_keys if k in cleaned_data}\n",
        "        else:\n",
        "            # Remove rdf:type from features\n",
        "            features = {k: v for k, v in cleaned_data.items() if k != rdf_type_key}\n",
        "\n",
        "        # Now add edges as attributes if requested\n",
        "        if edge_predicates:\n",
        "            for _, target, edge_data in nx_graph.out_edges(node, data=True):\n",
        "                pred = shorten_uri(edge_data.get(\"predicate\", \"\"), ontology_base)\n",
        "                if pred in edge_predicates:\n",
        "                    features[pred] = shorten_uri(target, ontology_base)\n",
        "\n",
        "        feature_dict[node_id] = features\n",
        "\n",
        "    return feature_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lKZGdmD0cNpU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize an empty NetworkX graph to store the merged knowledge graph\n",
        "kg_graph = nx.DiGraph()  # Assuming a directed graph; use nx.Graph() if undirected\n",
        "\n",
        "# Define the base path pattern and iterate over files 1 to 10\n",
        "base_path = \"/content/drive/My Drive/Master Thesis Semantics/LeakDB/Knowledge Graphs/knowledge_graph{}-LeakDB.ttl\"\n",
        "for i in range(1, 11):  # Iterate from 1 to 10\n",
        "    kg_path = base_path.format(i)\n",
        "\n",
        "    # Convert the KG file to a NetworkX graph\n",
        "    try:\n",
        "        single_kg_graph = networkx(kg_path)  # Assuming networkx() converts .ttl to NetworkX graph\n",
        "        if single_kg_graph is not None:\n",
        "            # Merge the single graph into the main graph\n",
        "            kg_graph = nx.compose(kg_graph, single_kg_graph)\n",
        "        else:\n",
        "            print(f\"Failed to load knowledge graph from {kg_path}. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {kg_path}: {str(e)}. Skipping.\")\n",
        "\n",
        "# Check if the merged graph has nodes/edges before proceeding\n",
        "if kg_graph.number_of_nodes() > 0:\n",
        "    pipe_features = extract_features_from_nx_graph(\n",
        "        kg_graph,  # Pass the merged NetworkX graph\n",
        "        \"https://raw.githubusercontent.com/DiTEC-project/wdn-knowledge-graph/refs/heads/main/wdn_ontology.ttl\",\n",
        "        node_type_filter=\"Pipe\",\n",
        "        attribute_keys=[\"diameter\", \"length\", \"roughness\", \"status\"],\n",
        "        edge_predicates=None\n",
        "    )\n",
        "else:\n",
        "    print(\"Failed to load any knowledge graphs. Cannot extract features.\")\n",
        "    pipe_features = {}  # Initialize pipe_features as an empty dictionary if loading failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBTL1Ba3cOqI",
        "outputId": "7c6f84f7-c356-4beb-e9f2-d34a63fcd15a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Pipe_27': {'diameter': 0.3393533,\n",
              "  'length': 256.3145,\n",
              "  'roughness': 120.9279,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_31': {'diameter': 0.2844319,\n",
              "  'length': 1777.14,\n",
              "  'roughness': 122.255,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_9': {'diameter': 1.055765,\n",
              "  'length': 766.6966,\n",
              "  'roughness': 134.3591,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_10': {'diameter': 0.8135806,\n",
              "  'length': 943.0429,\n",
              "  'roughness': 122.1229,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_25': {'diameter': 0.6257758,\n",
              "  'length': 1249.409,\n",
              "  'roughness': 126.3149,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_7': {'diameter': 0.8599014,\n",
              "  'length': 874.319,\n",
              "  'roughness': 148.771,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_1': {'diameter': 1.090263,\n",
              "  'length': 102.9672,\n",
              "  'roughness': 132.6247,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_32': {'diameter': 0.3227161,\n",
              "  'length': 156.6218,\n",
              "  'roughness': 131.7815,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_21': {'diameter': 0.4833052,\n",
              "  'length': 1353.826,\n",
              "  'roughness': 123.2031,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_14': {'diameter': 0.3435959,\n",
              "  'length': 486.0105,\n",
              "  'roughness': 121.7812,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_23': {'diameter': 1.006034,\n",
              "  'length': 2795.975,\n",
              "  'roughness': 119.1335,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_20': {'diameter': 0.9150203,\n",
              "  'length': 2075.576,\n",
              "  'roughness': 111.839,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_22': {'diameter': 0.3067705,\n",
              "  'length': 462.8089,\n",
              "  'roughness': 114.5218,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_19': {'diameter': 0.5709719,\n",
              "  'length': 386.9695,\n",
              "  'roughness': 112.8576,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_26': {'diameter': 0.5873128,\n",
              "  'length': 777.6697,\n",
              "  'roughness': 147.0625,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_15': {'diameter': 0.3094898,\n",
              "  'length': 520.8466,\n",
              "  'roughness': 115.3404,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_6': {'diameter': 1.023,\n",
              "  'length': 439.5466,\n",
              "  'roughness': 144.9987,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_12': {'diameter': 0.6181078,\n",
              "  'length': 4014.992,\n",
              "  'roughness': 134.9469,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_33': {'diameter': 0.4546421,\n",
              "  'length': 883.3792,\n",
              "  'roughness': 139.0549,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_5': {'diameter': 1.124516,\n",
              "  'length': 1613.829,\n",
              "  'roughness': 113.1797,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_8': {'diameter': 0.8487,\n",
              "  'length': 863.518,\n",
              "  'roughness': 148.8371,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_4': {'diameter': 0.9995752,\n",
              "  'length': 1186.678,\n",
              "  'roughness': 139.3634,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_24': {'diameter': 0.6198975,\n",
              "  'length': 1272.446,\n",
              "  'roughness': 134.549,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_16': {'diameter': 0.4420319,\n",
              "  'length': 2394.588,\n",
              "  'roughness': 138.9863,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_17': {'diameter': 0.4813545,\n",
              "  'length': 2002.789,\n",
              "  'roughness': 127.1883,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_28': {'diameter': 0.2864071,\n",
              "  'length': 783.7987,\n",
              "  'roughness': 118.9363,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_34': {'diameter': 0.5787292,\n",
              "  'length': 1017.823,\n",
              "  'roughness': 143.5448,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_29': {'diameter': 0.4095238,\n",
              "  'length': 1625.41,\n",
              "  'roughness': 124.6169,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_18': {'diameter': 0.5836749,\n",
              "  'length': 702.9573,\n",
              "  'roughness': 139.9244,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_13': {'diameter': 0.3482199,\n",
              "  'length': 853.9068,\n",
              "  'roughness': 113.3708,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_30': {'diameter': 0.438396,\n",
              "  'length': 1904.55,\n",
              "  'roughness': 144.8731,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_3': {'diameter': 0.8195023,\n",
              "  'length': 887.3613,\n",
              "  'roughness': 133.0811,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_2': {'diameter': 1.140161,\n",
              "  'length': 1215.863,\n",
              "  'roughness': 120.4282,\n",
              "  'status': 'Open'},\n",
              " 'Pipe_11': {'diameter': 0.7491347,\n",
              "  'length': 1372.928,\n",
              "  'roughness': 142.9782,\n",
              "  'status': 'Open'}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "pipe_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U75mx41Wk9db"
      },
      "outputs": [],
      "source": [
        "pipe_df = pd.DataFrame.from_dict(pipe_features, orient='index').reset_index()\n",
        "pipe_df.columns = ['pipe_id', 'diameter', 'length', 'roughness', 'status']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WKKIGZ33lqvg",
        "outputId": "4f3ac05a-461e-46fc-9d97-c71209b71c7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    pipe_id  diameter     length  roughness status\n",
              "0   Pipe_27  0.339353   256.3145   120.9279   Open\n",
              "1   Pipe_31  0.284432  1777.1400   122.2550   Open\n",
              "2    Pipe_9  1.055765   766.6966   134.3591   Open\n",
              "3   Pipe_10  0.813581   943.0429   122.1229   Open\n",
              "4   Pipe_25  0.625776  1249.4090   126.3149   Open\n",
              "5    Pipe_7  0.859901   874.3190   148.7710   Open\n",
              "6    Pipe_1  1.090263   102.9672   132.6247   Open\n",
              "7   Pipe_32  0.322716   156.6218   131.7815   Open\n",
              "8   Pipe_21  0.483305  1353.8260   123.2031   Open\n",
              "9   Pipe_14  0.343596   486.0105   121.7812   Open\n",
              "10  Pipe_23  1.006034  2795.9750   119.1335   Open\n",
              "11  Pipe_20  0.915020  2075.5760   111.8390   Open\n",
              "12  Pipe_22  0.306771   462.8089   114.5218   Open\n",
              "13  Pipe_19  0.570972   386.9695   112.8576   Open\n",
              "14  Pipe_26  0.587313   777.6697   147.0625   Open\n",
              "15  Pipe_15  0.309490   520.8466   115.3404   Open\n",
              "16   Pipe_6  1.023000   439.5466   144.9987   Open\n",
              "17  Pipe_12  0.618108  4014.9920   134.9469   Open\n",
              "18  Pipe_33  0.454642   883.3792   139.0549   Open\n",
              "19   Pipe_5  1.124516  1613.8290   113.1797   Open\n",
              "20   Pipe_8  0.848700   863.5180   148.8371   Open\n",
              "21   Pipe_4  0.999575  1186.6780   139.3634   Open\n",
              "22  Pipe_24  0.619897  1272.4460   134.5490   Open\n",
              "23  Pipe_16  0.442032  2394.5880   138.9863   Open\n",
              "24  Pipe_17  0.481355  2002.7890   127.1883   Open\n",
              "25  Pipe_28  0.286407   783.7987   118.9363   Open\n",
              "26  Pipe_34  0.578729  1017.8230   143.5448   Open\n",
              "27  Pipe_29  0.409524  1625.4100   124.6169   Open\n",
              "28  Pipe_18  0.583675   702.9573   139.9244   Open\n",
              "29  Pipe_13  0.348220   853.9068   113.3708   Open\n",
              "30  Pipe_30  0.438396  1904.5500   144.8731   Open\n",
              "31   Pipe_3  0.819502   887.3613   133.0811   Open\n",
              "32   Pipe_2  1.140161  1215.8630   120.4282   Open\n",
              "33  Pipe_11  0.749135  1372.9280   142.9782   Open"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df819b82-3062-4930-be05-4dde80edd698\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pipe_id</th>\n",
              "      <th>diameter</th>\n",
              "      <th>length</th>\n",
              "      <th>roughness</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pipe_27</td>\n",
              "      <td>0.339353</td>\n",
              "      <td>256.3145</td>\n",
              "      <td>120.9279</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pipe_31</td>\n",
              "      <td>0.284432</td>\n",
              "      <td>1777.1400</td>\n",
              "      <td>122.2550</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pipe_9</td>\n",
              "      <td>1.055765</td>\n",
              "      <td>766.6966</td>\n",
              "      <td>134.3591</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pipe_10</td>\n",
              "      <td>0.813581</td>\n",
              "      <td>943.0429</td>\n",
              "      <td>122.1229</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pipe_25</td>\n",
              "      <td>0.625776</td>\n",
              "      <td>1249.4090</td>\n",
              "      <td>126.3149</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Pipe_7</td>\n",
              "      <td>0.859901</td>\n",
              "      <td>874.3190</td>\n",
              "      <td>148.7710</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Pipe_1</td>\n",
              "      <td>1.090263</td>\n",
              "      <td>102.9672</td>\n",
              "      <td>132.6247</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Pipe_32</td>\n",
              "      <td>0.322716</td>\n",
              "      <td>156.6218</td>\n",
              "      <td>131.7815</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Pipe_21</td>\n",
              "      <td>0.483305</td>\n",
              "      <td>1353.8260</td>\n",
              "      <td>123.2031</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Pipe_14</td>\n",
              "      <td>0.343596</td>\n",
              "      <td>486.0105</td>\n",
              "      <td>121.7812</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Pipe_23</td>\n",
              "      <td>1.006034</td>\n",
              "      <td>2795.9750</td>\n",
              "      <td>119.1335</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Pipe_20</td>\n",
              "      <td>0.915020</td>\n",
              "      <td>2075.5760</td>\n",
              "      <td>111.8390</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Pipe_22</td>\n",
              "      <td>0.306771</td>\n",
              "      <td>462.8089</td>\n",
              "      <td>114.5218</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Pipe_19</td>\n",
              "      <td>0.570972</td>\n",
              "      <td>386.9695</td>\n",
              "      <td>112.8576</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Pipe_26</td>\n",
              "      <td>0.587313</td>\n",
              "      <td>777.6697</td>\n",
              "      <td>147.0625</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Pipe_15</td>\n",
              "      <td>0.309490</td>\n",
              "      <td>520.8466</td>\n",
              "      <td>115.3404</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Pipe_6</td>\n",
              "      <td>1.023000</td>\n",
              "      <td>439.5466</td>\n",
              "      <td>144.9987</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Pipe_12</td>\n",
              "      <td>0.618108</td>\n",
              "      <td>4014.9920</td>\n",
              "      <td>134.9469</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Pipe_33</td>\n",
              "      <td>0.454642</td>\n",
              "      <td>883.3792</td>\n",
              "      <td>139.0549</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Pipe_5</td>\n",
              "      <td>1.124516</td>\n",
              "      <td>1613.8290</td>\n",
              "      <td>113.1797</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Pipe_8</td>\n",
              "      <td>0.848700</td>\n",
              "      <td>863.5180</td>\n",
              "      <td>148.8371</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Pipe_4</td>\n",
              "      <td>0.999575</td>\n",
              "      <td>1186.6780</td>\n",
              "      <td>139.3634</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Pipe_24</td>\n",
              "      <td>0.619897</td>\n",
              "      <td>1272.4460</td>\n",
              "      <td>134.5490</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Pipe_16</td>\n",
              "      <td>0.442032</td>\n",
              "      <td>2394.5880</td>\n",
              "      <td>138.9863</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Pipe_17</td>\n",
              "      <td>0.481355</td>\n",
              "      <td>2002.7890</td>\n",
              "      <td>127.1883</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Pipe_28</td>\n",
              "      <td>0.286407</td>\n",
              "      <td>783.7987</td>\n",
              "      <td>118.9363</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Pipe_34</td>\n",
              "      <td>0.578729</td>\n",
              "      <td>1017.8230</td>\n",
              "      <td>143.5448</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Pipe_29</td>\n",
              "      <td>0.409524</td>\n",
              "      <td>1625.4100</td>\n",
              "      <td>124.6169</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Pipe_18</td>\n",
              "      <td>0.583675</td>\n",
              "      <td>702.9573</td>\n",
              "      <td>139.9244</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Pipe_13</td>\n",
              "      <td>0.348220</td>\n",
              "      <td>853.9068</td>\n",
              "      <td>113.3708</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Pipe_30</td>\n",
              "      <td>0.438396</td>\n",
              "      <td>1904.5500</td>\n",
              "      <td>144.8731</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Pipe_3</td>\n",
              "      <td>0.819502</td>\n",
              "      <td>887.3613</td>\n",
              "      <td>133.0811</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Pipe_2</td>\n",
              "      <td>1.140161</td>\n",
              "      <td>1215.8630</td>\n",
              "      <td>120.4282</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Pipe_11</td>\n",
              "      <td>0.749135</td>\n",
              "      <td>1372.9280</td>\n",
              "      <td>142.9782</td>\n",
              "      <td>Open</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df819b82-3062-4930-be05-4dde80edd698')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df819b82-3062-4930-be05-4dde80edd698 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df819b82-3062-4930-be05-4dde80edd698');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-083d4e8c-e895-4c46-8878-ff5cef54bd19\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-083d4e8c-e895-4c46-8878-ff5cef54bd19')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-083d4e8c-e895-4c46-8878-ff5cef54bd19 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d5b8fc5e-5523-4c55-9caa-1a26346a0f1b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pipe_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d5b8fc5e-5523-4c55-9caa-1a26346a0f1b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pipe_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pipe_df",
              "summary": "{\n  \"name\": \"pipe_df\",\n  \"rows\": 34,\n  \"fields\": [\n    {\n      \"column\": \"pipe_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 34,\n        \"samples\": [\n          \"Pipe_15\",\n          \"Pipe_5\",\n          \"Pipe_29\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diameter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2796229671882229,\n        \"min\": 0.2844319,\n        \"max\": 1.140161,\n        \"num_unique_values\": 34,\n        \"samples\": [\n          0.3094898,\n          1.124516,\n          0.4095238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 814.5307964810903,\n        \"min\": 102.9672,\n        \"max\": 4014.992,\n        \"num_unique_values\": 34,\n        \"samples\": [\n          520.8466,\n          1613.829,\n          1625.41\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roughness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.758778300912804,\n        \"min\": 111.839,\n        \"max\": 148.8371,\n        \"num_unique_values\": 34,\n        \"samples\": [\n          115.3404,\n          113.1797,\n          124.6169\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Open\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "pipe_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amQ-rG_9giK6",
        "outputId": "1ea19ee9-1aa0-4a4b-f6cc-75fc1ff68c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_1_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_2_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_3_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_4_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_5_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_6_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_7_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_8_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_9_LeakDB.csv with 1716960 rows.\n",
            "Loaded /content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_10_LeakDB.csv with 1716960 rows.\n",
            "Combined CSV data has 17169600 rows.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty list to store individual DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Define the base path pattern and iterate over files 1 to 10\n",
        "base_path = \"/content/drive/My Drive/Master Thesis Semantics/LeakDB/CSV Files/measurements_{}_LeakDB.csv\"\n",
        "# Load and combine CSV files\n",
        "for i in range(1, 11):\n",
        "    csv_path = base_path.format(i)\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        if not df.empty:\n",
        "            dataframes.append(df)\n",
        "            print(f\"Loaded {csv_path} with {len(df)} rows.\")\n",
        "        else:\n",
        "            print(f\"Warning: {csv_path} is empty. Skipping.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {csv_path} not found. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {csv_path}: {str(e)}. Skipping.\")\n",
        "\n",
        "# Combine CSV files into a single DataFrame\n",
        "if dataframes:\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "    print(f\"Combined CSV data has {len(combined_df)} rows.\")\n",
        "else:\n",
        "    print(\"No CSV files loaded. Exiting.\")\n",
        "    exit(1)\n",
        "\n",
        "# Filter for flow measurements and create pipe_id\n",
        "combined_df = combined_df[combined_df['sensor_type'] == 'flow'].copy()\n",
        "combined_df['pipe_id'] = combined_df['sensor_id'].str.replace('Link_', 'Pipe_').str.replace('_flow', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "UWUsBs_fgsFD",
        "outputId": "d410a255-f487-42ed-c6a2-1566e9802eaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Unnamed: 0            Timestamp  measurement    sensor_id  \\\n",
              "1121280      1121280  2017-01-01 00:00:00       3405.6  Link_1_flow   \n",
              "1121281      1121281  2017-01-01 00:30:00       2970.0  Link_1_flow   \n",
              "1121282      1121282  2017-01-01 01:00:00       2656.8  Link_1_flow   \n",
              "1121283      1121283  2017-01-01 01:30:00       2401.2  Link_1_flow   \n",
              "1121284      1121284  2017-01-01 02:00:00       2199.6  Link_1_flow   \n",
              "...              ...                  ...          ...          ...   \n",
              "17169595     1716955  2017-12-31 21:30:00        925.2  Link_9_flow   \n",
              "17169596     1716956  2017-12-31 22:00:00        864.0  Link_9_flow   \n",
              "17169597     1716957  2017-12-31 22:30:00        806.4  Link_9_flow   \n",
              "17169598     1716958  2017-12-31 23:00:00        781.2  Link_9_flow   \n",
              "17169599     1716959  2017-12-31 23:30:00        666.0  Link_9_flow   \n",
              "\n",
              "         sensor_type                        unique_id pipe_id  \n",
              "1121280         flow  Link_1_flow_2017-01-01 00:00:00  Pipe_1  \n",
              "1121281         flow  Link_1_flow_2017-01-01 00:30:00  Pipe_1  \n",
              "1121282         flow  Link_1_flow_2017-01-01 01:00:00  Pipe_1  \n",
              "1121283         flow  Link_1_flow_2017-01-01 01:30:00  Pipe_1  \n",
              "1121284         flow  Link_1_flow_2017-01-01 02:00:00  Pipe_1  \n",
              "...              ...                              ...     ...  \n",
              "17169595        flow  Link_9_flow_2017-12-31 21:30:00  Pipe_9  \n",
              "17169596        flow  Link_9_flow_2017-12-31 22:00:00  Pipe_9  \n",
              "17169597        flow  Link_9_flow_2017-12-31 22:30:00  Pipe_9  \n",
              "17169598        flow  Link_9_flow_2017-12-31 23:00:00  Pipe_9  \n",
              "17169599        flow  Link_9_flow_2017-12-31 23:30:00  Pipe_9  \n",
              "\n",
              "[5956800 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68fe95e6-acb8-4549-9f4b-988a31f1f51a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>measurement</th>\n",
              "      <th>sensor_id</th>\n",
              "      <th>sensor_type</th>\n",
              "      <th>unique_id</th>\n",
              "      <th>pipe_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1121280</th>\n",
              "      <td>1121280</td>\n",
              "      <td>2017-01-01 00:00:00</td>\n",
              "      <td>3405.6</td>\n",
              "      <td>Link_1_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_1_flow_2017-01-01 00:00:00</td>\n",
              "      <td>Pipe_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121281</th>\n",
              "      <td>1121281</td>\n",
              "      <td>2017-01-01 00:30:00</td>\n",
              "      <td>2970.0</td>\n",
              "      <td>Link_1_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_1_flow_2017-01-01 00:30:00</td>\n",
              "      <td>Pipe_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121282</th>\n",
              "      <td>1121282</td>\n",
              "      <td>2017-01-01 01:00:00</td>\n",
              "      <td>2656.8</td>\n",
              "      <td>Link_1_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_1_flow_2017-01-01 01:00:00</td>\n",
              "      <td>Pipe_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121283</th>\n",
              "      <td>1121283</td>\n",
              "      <td>2017-01-01 01:30:00</td>\n",
              "      <td>2401.2</td>\n",
              "      <td>Link_1_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_1_flow_2017-01-01 01:30:00</td>\n",
              "      <td>Pipe_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121284</th>\n",
              "      <td>1121284</td>\n",
              "      <td>2017-01-01 02:00:00</td>\n",
              "      <td>2199.6</td>\n",
              "      <td>Link_1_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_1_flow_2017-01-01 02:00:00</td>\n",
              "      <td>Pipe_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17169595</th>\n",
              "      <td>1716955</td>\n",
              "      <td>2017-12-31 21:30:00</td>\n",
              "      <td>925.2</td>\n",
              "      <td>Link_9_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_9_flow_2017-12-31 21:30:00</td>\n",
              "      <td>Pipe_9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17169596</th>\n",
              "      <td>1716956</td>\n",
              "      <td>2017-12-31 22:00:00</td>\n",
              "      <td>864.0</td>\n",
              "      <td>Link_9_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_9_flow_2017-12-31 22:00:00</td>\n",
              "      <td>Pipe_9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17169597</th>\n",
              "      <td>1716957</td>\n",
              "      <td>2017-12-31 22:30:00</td>\n",
              "      <td>806.4</td>\n",
              "      <td>Link_9_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_9_flow_2017-12-31 22:30:00</td>\n",
              "      <td>Pipe_9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17169598</th>\n",
              "      <td>1716958</td>\n",
              "      <td>2017-12-31 23:00:00</td>\n",
              "      <td>781.2</td>\n",
              "      <td>Link_9_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_9_flow_2017-12-31 23:00:00</td>\n",
              "      <td>Pipe_9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17169599</th>\n",
              "      <td>1716959</td>\n",
              "      <td>2017-12-31 23:30:00</td>\n",
              "      <td>666.0</td>\n",
              "      <td>Link_9_flow</td>\n",
              "      <td>flow</td>\n",
              "      <td>Link_9_flow_2017-12-31 23:30:00</td>\n",
              "      <td>Pipe_9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5956800 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68fe95e6-acb8-4549-9f4b-988a31f1f51a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68fe95e6-acb8-4549-9f4b-988a31f1f51a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68fe95e6-acb8-4549-9f4b-988a31f1f51a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-eb7c5d97-9af1-49c4-a965-e33dad30c176\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eb7c5d97-9af1-49c4-a965-e33dad30c176')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-eb7c5d97-9af1-49c4-a965-e33dad30c176 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a9704d3c-9c5c-4770-b599-a599c4d41570\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('combined_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a9704d3c-9c5c-4770-b599-a599c4d41570 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('combined_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "combined_df"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9hyNgnEAm8i",
        "outputId": "548d56a2-9d95-460b-c43f-23a181a66e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged data size: 5956800 rows.\n",
            "Global flow stats: Mean = 829.876609, Std = 1495.636123\n",
            "Per-pipe plausible ranges (fixed k):\n",
            "     pipe_id  actual_mean   actual_std  min_plausible  max_plausible\n",
            "0    Pipe_1  5627.037288  1718.679788     470.997924   10783.076651\n",
            "1   Pipe_10   550.314267   195.561336     -36.369742    1136.998276\n",
            "2   Pipe_11   412.248822   162.662078     -75.737413     900.235057\n",
            "3   Pipe_12   253.255767    83.374362       3.132682     503.378853\n",
            "4   Pipe_13   266.276404    99.849458     -33.271971     565.824779\n",
            "5   Pipe_14    88.363952    60.337333     -92.648046     269.375950\n",
            "6   Pipe_15    13.391692    56.662783    -156.596656     183.380040\n",
            "7   Pipe_16   156.229705    78.588859     -79.536872     391.996283\n",
            "8   Pipe_17  -405.530240   134.493077    -809.009471      -2.051009\n",
            "9   Pipe_18  -777.124479   242.406106   -1504.342797     -49.906162\n",
            "10  Pipe_19  -794.791479   247.168817   -1536.297930     -53.285029\n",
            "11   Pipe_2  5386.296760  1649.485821     437.839298   10334.754222\n",
            "12  Pipe_20  2207.171610   750.726857     -45.008962    4459.352181\n",
            "13  Pipe_21   408.035774   252.644128    -349.896611    1165.968159\n",
            "14  Pipe_22   128.769452    41.299230       4.871761     252.667143\n",
            "15  Pipe_23  1447.923884   520.511223    -113.609784    3009.457552\n",
            "16  Pipe_24   896.787986   276.812724      66.349816    1727.226157\n",
            "17  Pipe_25   656.141671   205.627531      39.259077    1273.024265\n",
            "18  Pipe_26  -273.569630   102.998892    -582.566305      35.427044\n",
            "19  Pipe_27   -20.996363    60.255449    -201.762709     159.769983\n",
            "20  Pipe_28    81.056301    67.437810    -121.257130     283.369732\n",
            "21  Pipe_29   258.814212   258.231663    -515.880777    1033.509201\n",
            "22   Pipe_3  2150.557541   648.409214     205.329901    4095.785182\n",
            "23  Pipe_30   107.857110   102.299318    -199.040846     414.755065\n",
            "24  Pipe_31     9.254527    96.846744    -281.285704     299.794759\n",
            "25  Pipe_32   -90.028870   100.600031    -391.828963     211.771223\n",
            "26  Pipe_33   119.069342   103.423985    -191.202612     429.341297\n",
            "27  Pipe_34   336.010973   139.678505     -83.024543     755.046488\n",
            "28   Pipe_4  2115.158836   637.697454     202.066475    4028.251197\n",
            "29   Pipe_5  1907.977644   579.325887     169.999984    3645.955304\n",
            "30   Pipe_6  1632.638527   497.608352     139.813471    3125.463583\n",
            "31   Pipe_7  1274.619699   395.602830      87.811209    2461.428188\n",
            "32   Pipe_8  1118.207281   349.674631      69.183389    2167.231172\n",
            "33   Pipe_9   968.378753   308.317622      43.425887    1893.331620\n",
            "Subsampled data size: 595680\n",
            "Total feature combinations to test: 8\n",
            "Testing feature combination: ['measurement']\n",
            "Fold 1 Results - Features: ['measurement']\n",
            "  Precision: 0.998209\n",
            "  Recall: 0.698679\n",
            "  F1 Score: 0.822008\n",
            "  Noise Detected (%): 69.867937\n",
            "Fold 2 Results - Features: ['measurement']\n",
            "  Precision: 0.998962\n",
            "  Recall: 0.686999\n",
            "  F1 Score: 0.814118\n",
            "  Noise Detected (%): 68.699887\n",
            "Fold 3 Results - Features: ['measurement']\n",
            "  Precision: 0.998428\n",
            "  Recall: 0.689151\n",
            "  F1 Score: 0.815450\n",
            "  Noise Detected (%): 68.915140\n",
            "Fold 4 Results - Features: ['measurement']\n",
            "  Precision: 0.998850\n",
            "  Recall: 0.694318\n",
            "  F1 Score: 0.819198\n",
            "  Noise Detected (%): 69.431818\n",
            "Fold 5 Results - Features: ['measurement']\n",
            "  Precision: 0.998310\n",
            "  Recall: 0.699214\n",
            "  F1 Score: 0.822412\n",
            "  Noise Detected (%): 69.921380\n",
            "Testing feature combination: ['measurement', 'diameter']\n",
            "Fold 1 Results - Features: ['measurement', 'diameter']\n",
            "  Precision: 0.972091\n",
            "  Recall: 0.702203\n",
            "  F1 Score: 0.815395\n",
            "  Noise Detected (%): 70.220268\n",
            "Fold 2 Results - Features: ['measurement', 'diameter']\n",
            "  Precision: 0.976365\n",
            "  Recall: 0.697118\n",
            "  F1 Score: 0.813443\n",
            "  Noise Detected (%): 69.711779\n",
            "Fold 3 Results - Features: ['measurement', 'diameter']\n",
            "  Precision: 0.974315\n",
            "  Recall: 0.697509\n",
            "  F1 Score: 0.812997\n",
            "  Noise Detected (%): 69.750905\n",
            "Fold 4 Results - Features: ['measurement', 'diameter']\n",
            "  Precision: 0.974060\n",
            "  Recall: 0.694786\n",
            "  F1 Score: 0.811055\n",
            "  Noise Detected (%): 69.478553\n",
            "Fold 5 Results - Features: ['measurement', 'diameter']\n",
            "  Precision: 0.980061\n",
            "  Recall: 0.695886\n",
            "  F1 Score: 0.813881\n",
            "  Noise Detected (%): 69.588641\n",
            "Testing feature combination: ['measurement', 'length']\n",
            "Fold 1 Results - Features: ['measurement', 'length']\n",
            "  Precision: 0.996769\n",
            "  Recall: 0.637998\n",
            "  F1 Score: 0.778015\n",
            "  Noise Detected (%): 63.799797\n",
            "Fold 2 Results - Features: ['measurement', 'length']\n",
            "  Precision: 0.998604\n",
            "  Recall: 0.607449\n",
            "  F1 Score: 0.755394\n",
            "  Noise Detected (%): 60.744925\n",
            "Fold 3 Results - Features: ['measurement', 'length']\n",
            "  Precision: 0.996185\n",
            "  Recall: 0.635505\n",
            "  F1 Score: 0.775982\n",
            "  Noise Detected (%): 63.550539\n",
            "Fold 4 Results - Features: ['measurement', 'length']\n",
            "  Precision: 0.992060\n",
            "  Recall: 0.632258\n",
            "  F1 Score: 0.772309\n",
            "  Noise Detected (%): 63.225806\n",
            "Fold 5 Results - Features: ['measurement', 'length']\n",
            "  Precision: 0.991823\n",
            "  Recall: 0.641501\n",
            "  F1 Score: 0.779093\n",
            "  Noise Detected (%): 64.150079\n",
            "Testing feature combination: ['measurement', 'roughness']\n",
            "Fold 1 Results - Features: ['measurement', 'roughness']\n",
            "  Precision: 0.999022\n",
            "  Recall: 0.641938\n",
            "  F1 Score: 0.781628\n",
            "  Noise Detected (%): 64.193805\n",
            "Fold 2 Results - Features: ['measurement', 'roughness']\n",
            "  Precision: 0.991382\n",
            "  Recall: 0.629075\n",
            "  F1 Score: 0.769726\n",
            "  Noise Detected (%): 62.907500\n",
            "Fold 3 Results - Features: ['measurement', 'roughness']\n",
            "  Precision: 0.990695\n",
            "  Recall: 0.628983\n",
            "  F1 Score: 0.769449\n",
            "  Noise Detected (%): 62.898257\n",
            "Fold 4 Results - Features: ['measurement', 'roughness']\n",
            "  Precision: 0.987616\n",
            "  Recall: 0.618772\n",
            "  F1 Score: 0.760849\n",
            "  Noise Detected (%): 61.877202\n",
            "Fold 5 Results - Features: ['measurement', 'roughness']\n",
            "  Precision: 0.997133\n",
            "  Recall: 0.615624\n",
            "  F1 Score: 0.761255\n",
            "  Noise Detected (%): 61.562447\n",
            "Testing feature combination: ['measurement', 'diameter', 'length']\n",
            "Fold 1 Results - Features: ['measurement', 'diameter', 'length']\n",
            "  Precision: 0.993437\n",
            "  Recall: 0.637037\n",
            "  F1 Score: 0.776285\n",
            "  Noise Detected (%): 63.703704\n",
            "Fold 2 Results - Features: ['measurement', 'diameter', 'length']\n",
            "  Precision: 0.986356\n",
            "  Recall: 0.646558\n",
            "  F1 Score: 0.781102\n",
            "  Noise Detected (%): 64.655754\n",
            "Fold 3 Results - Features: ['measurement', 'diameter', 'length']\n",
            "  Precision: 0.992482\n",
            "  Recall: 0.637497\n",
            "  F1 Score: 0.776335\n",
            "  Noise Detected (%): 63.749738\n",
            "Fold 4 Results - Features: ['measurement', 'diameter', 'length']\n",
            "  Precision: 0.992707\n",
            "  Recall: 0.638523\n",
            "  F1 Score: 0.777163\n",
            "  Noise Detected (%): 63.852265\n",
            "Fold 5 Results - Features: ['measurement', 'diameter', 'length']\n",
            "  Precision: 0.991929\n",
            "  Recall: 0.640753\n",
            "  F1 Score: 0.778573\n",
            "  Noise Detected (%): 64.075283\n",
            "Testing feature combination: ['measurement', 'diameter', 'roughness']\n",
            "Fold 1 Results - Features: ['measurement', 'diameter', 'roughness']\n",
            "  Precision: 0.986462\n",
            "  Recall: 0.654398\n",
            "  F1 Score: 0.786830\n",
            "  Noise Detected (%): 65.439819\n",
            "Fold 2 Results - Features: ['measurement', 'diameter', 'roughness']\n",
            "  Precision: 0.987356\n",
            "  Recall: 0.629664\n",
            "  F1 Score: 0.768948\n",
            "  Noise Detected (%): 62.966367\n",
            "Fold 3 Results - Features: ['measurement', 'diameter', 'roughness']\n",
            "  Precision: 0.985334\n",
            "  Recall: 0.641149\n",
            "  F1 Score: 0.776824\n",
            "  Noise Detected (%): 64.114852\n",
            "Fold 4 Results - Features: ['measurement', 'diameter', 'roughness']\n",
            "  Precision: 0.989307\n",
            "  Recall: 0.631552\n",
            "  F1 Score: 0.770948\n",
            "  Noise Detected (%): 63.155233\n",
            "Fold 5 Results - Features: ['measurement', 'diameter', 'roughness']\n",
            "  Precision: 0.985027\n",
            "  Recall: 0.635167\n",
            "  F1 Score: 0.772323\n",
            "  Noise Detected (%): 63.516676\n",
            "Testing feature combination: ['measurement', 'length', 'roughness']\n",
            "Fold 1 Results - Features: ['measurement', 'length', 'roughness']\n",
            "  Precision: 0.996428\n",
            "  Recall: 0.581356\n",
            "  F1 Score: 0.734295\n",
            "  Noise Detected (%): 58.135579\n",
            "Fold 2 Results - Features: ['measurement', 'length', 'roughness']\n",
            "  Precision: 0.997518\n",
            "  Recall: 0.606412\n",
            "  F1 Score: 0.754281\n",
            "  Noise Detected (%): 60.641241\n",
            "Fold 3 Results - Features: ['measurement', 'length', 'roughness']\n",
            "  Precision: 0.996708\n",
            "  Recall: 0.607676\n",
            "  F1 Score: 0.755026\n",
            "  Noise Detected (%): 60.767591\n",
            "Fold 4 Results - Features: ['measurement', 'length', 'roughness']\n",
            "  Precision: 0.996645\n",
            "  Recall: 0.607015\n",
            "  F1 Score: 0.754497\n",
            "  Noise Detected (%): 60.701515\n",
            "Fold 5 Results - Features: ['measurement', 'length', 'roughness']\n",
            "  Precision: 0.996450\n",
            "  Recall: 0.595410\n",
            "  F1 Score: 0.745413\n",
            "  Noise Detected (%): 59.540981\n",
            "Testing feature combination: ['measurement', 'diameter', 'length', 'roughness']\n",
            "Fold 1 Results - Features: ['measurement', 'diameter', 'length', 'roughness']\n",
            "  Precision: 0.983018\n",
            "  Recall: 0.611253\n",
            "  F1 Score: 0.753790\n",
            "  Noise Detected (%): 61.125256\n",
            "Fold 2 Results - Features: ['measurement', 'diameter', 'length', 'roughness']\n",
            "  Precision: 0.985346\n",
            "  Recall: 0.620047\n",
            "  F1 Score: 0.761135\n",
            "  Noise Detected (%): 62.004652\n",
            "Fold 3 Results - Features: ['measurement', 'diameter', 'length', 'roughness']\n",
            "  Precision: 0.983630\n",
            "  Recall: 0.624272\n",
            "  F1 Score: 0.763794\n",
            "  Noise Detected (%): 62.427213\n",
            "Fold 4 Results - Features: ['measurement', 'diameter', 'length', 'roughness']\n",
            "  Precision: 0.988412\n",
            "  Recall: 0.606998\n",
            "  F1 Score: 0.752113\n",
            "  Noise Detected (%): 60.699848\n",
            "Fold 5 Results - Features: ['measurement', 'diameter', 'length', 'roughness']\n",
            "  Precision: 0.985908\n",
            "  Recall: 0.613931\n",
            "  F1 Score: 0.756675\n",
            "  Noise Detected (%): 61.393073\n",
            "\n",
            "Feature Selection Results (sorted by Avg F1 Score):\n",
            "                                   Features  Avg F1 Score  Precision  \\\n",
            "0                               measurement      0.818637   0.998310   \n",
            "1                     measurement, diameter      0.813354   0.980061   \n",
            "2             measurement, diameter, length      0.777892   0.991929   \n",
            "3          measurement, diameter, roughness      0.775175   0.985027   \n",
            "4                       measurement, length      0.772158   0.991823   \n",
            "5                    measurement, roughness      0.768581   0.997133   \n",
            "6  measurement, diameter, length, roughness      0.757501   0.985908   \n",
            "7            measurement, length, roughness      0.748702   0.996450   \n",
            "\n",
            "     Recall  Noise Detected (%)  \n",
            "0  0.699214           69.921380  \n",
            "1  0.695886           69.588641  \n",
            "2  0.640753           64.075283  \n",
            "3  0.635167           63.516676  \n",
            "4  0.641501           64.150079  \n",
            "5  0.615624           61.562447  \n",
            "6  0.613931           61.393073  \n",
            "7  0.595410           59.540981  \n",
            "Feature selection results saved to '/content/drive/My Drive/feature_selection_results_kmeans_leakdb.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming combined_df and pipe_df are already defined from previous steps\n",
        "merged_df = pd.merge(combined_df, pipe_df, on='pipe_id', how='inner')\n",
        "print(f\"Merged data size: {len(merged_df)} rows.\")\n",
        "\n",
        "# Compute global flow stats\n",
        "global_flow_mean = merged_df['measurement'].mean()\n",
        "global_flow_std = merged_df['measurement'].std()\n",
        "print(f\"Global flow stats: Mean = {global_flow_mean:.6f}, Std = {global_flow_std:.6f}\")\n",
        "\n",
        "# Compute per-pipe statistics\n",
        "pipe_stats_df = merged_df.groupby('pipe_id')['measurement'].agg(['mean', 'std', 'count']).reset_index()\n",
        "pipe_stats_df.columns = ['pipe_id', 'actual_mean', 'actual_std', 'sample_count']\n",
        "pipe_stats_df = pipe_stats_df[pipe_stats_df['sample_count'] >= 100]\n",
        "\n",
        "# Compute global pipe std for minimum threshold\n",
        "global_pipe_std = pipe_stats_df['actual_std'].mean()\n",
        "\n",
        "# Apply a minimum std to avoid overly narrow ranges (10% of global pipe std)\n",
        "pipe_stats_df['actual_std'] = pipe_stats_df['actual_std'].apply(lambda x: max(x, global_pipe_std * 0.1))\n",
        "\n",
        "# Compute plausible ranges using pipe-level stats with fixed k\n",
        "k = 3  # Fixed number of standard deviations for plausible range\n",
        "pipe_stats_df['min_plausible'] = pipe_stats_df['actual_mean'] - k * pipe_stats_df['actual_std']\n",
        "pipe_stats_df['max_plausible'] = pipe_stats_df['actual_mean'] + k * pipe_stats_df['actual_std']\n",
        "print(\"Per-pipe plausible ranges (fixed k):\\n\", pipe_stats_df[['pipe_id', 'actual_mean', 'actual_std', 'min_plausible', 'max_plausible']])\n",
        "\n",
        "# Merge the plausible ranges into the main DataFrame\n",
        "merged_df = merged_df.merge(pipe_stats_df[['pipe_id', 'min_plausible', 'max_plausible']], on='pipe_id', how='left')\n",
        "\n",
        "# Subsample the data (10%)\n",
        "subsample_fraction = 0.1\n",
        "subsample_size = int(len(merged_df) * subsample_fraction)\n",
        "merged_df = merged_df.sample(n=subsample_size, random_state=42)\n",
        "print(f\"Subsampled data size: {len(merged_df)}\")\n",
        "\n",
        "# Define all possible feature combinations\n",
        "base_feature = ['measurement']\n",
        "semantic_features = ['diameter', 'length', 'roughness']\n",
        "all_combinations = []\n",
        "for r in range(1, len(semantic_features) + 1):\n",
        "    for combo in combinations(semantic_features, r):\n",
        "        all_combinations.append(base_feature + list(combo))\n",
        "all_combinations = [base_feature] + all_combinations\n",
        "print(f\"Total feature combinations to test: {len(all_combinations)}\")\n",
        "\n",
        "# Function to introduce General Normalised Noise (GNN)\n",
        "def introduce_gnn_noise(df, beta=1.5, noise_fraction=0.2):\n",
        "    noisy_df = df.copy()\n",
        "    mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "    noise = np.random.normal(0, beta * global_flow_std, size=len(noisy_df))\n",
        "    noisy_df['measurement'] = noisy_df['measurement'] + noise * mask\n",
        "    noisy_df['is_true_noise'] = mask.astype(int)\n",
        "    return noisy_df\n",
        "\n",
        "# Function to evaluate a feature combination with cross-validation\n",
        "def evaluate_feature_combination(features, df, n_clusters=7, threshold=2, n_folds=5):\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
        "        train_df = df.iloc[train_idx].copy()\n",
        "        test_df = df.iloc[test_idx].copy()\n",
        "\n",
        "        # Scale features\n",
        "        X_cluster_train = train_df[features]\n",
        "        scaler = RobustScaler()\n",
        "        X_scaled_train = scaler.fit_transform(X_cluster_train)\n",
        "\n",
        "        # Apply K-means clustering\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        train_cluster_labels = kmeans.fit_predict(X_scaled_train)\n",
        "\n",
        "        X_cluster_test = test_df[features]\n",
        "        X_scaled_test = scaler.transform(X_cluster_test)\n",
        "        test_cluster_labels = kmeans.predict(X_scaled_test)\n",
        "\n",
        "        train_df['cluster'] = train_cluster_labels\n",
        "        test_df['cluster'] = test_cluster_labels\n",
        "\n",
        "        # Compute cluster statistics\n",
        "        cluster_stats = train_df.groupby('cluster').agg({\n",
        "            'measurement': ['mean', 'std', 'count']\n",
        "        }).reset_index()\n",
        "        cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count']\n",
        "\n",
        "        # Map cluster stats to test set\n",
        "        cluster_means = cluster_stats.set_index('cluster')['cluster_mu'].to_dict()\n",
        "        cluster_stds = cluster_stats.set_index('cluster')['cluster_sigma'].to_dict()\n",
        "        test_df['cluster_mu'] = test_df['cluster'].map(cluster_means)\n",
        "        test_df['cluster_sigma'] = test_df['cluster'].map(cluster_stds).fillna(global_pipe_std)\n",
        "\n",
        "        # Introduce noise to test set\n",
        "        noisy_test_df = introduce_gnn_noise(test_df, beta=1.5, noise_fraction=0.2)\n",
        "\n",
        "        # Compute Z-scores and detect anomalies\n",
        "        noisy_test_df['Z_score'] = (noisy_test_df['measurement'] - noisy_test_df['cluster_mu']) / noisy_test_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_test_df['traditional_Z_score'] = (noisy_test_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "        noisy_test_df['outside_plausible'] = (noisy_test_df['measurement'] < noisy_test_df['min_plausible']) | (noisy_test_df['measurement'] > noisy_test_df['max_plausible'])\n",
        "        noisy_test_df['is_anomaly'] = np.abs(noisy_test_df['Z_score']) > threshold\n",
        "        noisy_test_df['is_noise'] = noisy_test_df['is_anomaly'] & noisy_test_df['outside_plausible']\n",
        "\n",
        "        # Evaluate performance\n",
        "        y_true = noisy_test_df['is_true_noise']\n",
        "        y_pred = noisy_test_df['is_noise'].astype(int)\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        total_noise_points = y_true.sum()\n",
        "        detected_noise = y_pred[y_true == 1].sum()\n",
        "        noise_detected_percentage = (detected_noise / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'noise_detected_percentage': noise_detected_percentage\n",
        "        })\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "# Evaluate all feature combinations\n",
        "results_list = []\n",
        "for idx, feature_combo in enumerate(all_combinations):\n",
        "    print(f\"Testing feature combination: {feature_combo}\")\n",
        "    fold_results = evaluate_feature_combination(feature_combo, merged_df, n_clusters=7, threshold=2)\n",
        "\n",
        "    # Aggregate fold results\n",
        "    avg_f1 = np.mean([result['f1_score'] for result in fold_results])\n",
        "    last_fold = fold_results[-1]  # Use last fold for precision, recall, noise detected (as in DBSCAN setup)\n",
        "\n",
        "    results_list.append({\n",
        "        'Features': ', '.join(feature_combo),\n",
        "        'Avg F1 Score': avg_f1,\n",
        "        'Precision': last_fold['precision'],\n",
        "        'Recall': last_fold['recall'],\n",
        "        'Noise Detected (%)': last_fold['noise_detected_percentage']\n",
        "    })\n",
        "\n",
        "    # Print fold-level results\n",
        "    for result in fold_results:\n",
        "        print(f\"Fold {result['fold']} Results - Features: {feature_combo}\")\n",
        "        print(f\"  Precision: {result['precision']:.6f}\")\n",
        "        print(f\"  Recall: {result['recall']:.6f}\")\n",
        "        print(f\"  F1 Score: {result['f1_score']:.6f}\")\n",
        "        print(f\"  Noise Detected (%): {result['noise_detected_percentage']:.6f}\")\n",
        "\n",
        "# Create results DataFrame and sort by Avg F1 Score\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df = results_df.sort_values(by='Avg F1 Score', ascending=False).reset_index(drop=True)\n",
        "print(\"\\nFeature Selection Results (sorted by Avg F1 Score):\")\n",
        "print(results_df)\n",
        "\n",
        "# Save results\n",
        "try:\n",
        "    results_df.to_csv('/content/drive/My Drive/feature_selection_results_kmeans_leakdb.csv', index=False)\n",
        "    print(\"Feature selection results saved to '/content/drive/My Drive/feature_selection_results_kmeans_leakdb.csv'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q3HQNLdEbOX",
        "outputId": "19ce33c6-6c07-4f64-b816-c1c130e6f186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged data size: 5956800 rows.\n",
            "Global flow stats: Mean = 829.88, Std = 1495.64\n",
            "Per-pipe plausible ranges (fixed k):\n",
            "     pipe_id  actual_mean   actual_std  min_plausible  max_plausible\n",
            "0    Pipe_1  5627.037288  1718.679788     470.997924   10783.076651\n",
            "1   Pipe_10   550.314267   195.561336     -36.369742    1136.998276\n",
            "2   Pipe_11   412.248822   162.662078     -75.737413     900.235057\n",
            "3   Pipe_12   253.255767    83.374362       3.132682     503.378853\n",
            "4   Pipe_13   266.276404    99.849458     -33.271971     565.824779\n",
            "5   Pipe_14    88.363952    60.337333     -92.648046     269.375950\n",
            "6   Pipe_15    13.391692    56.662783    -156.596656     183.380040\n",
            "7   Pipe_16   156.229705    78.588859     -79.536872     391.996283\n",
            "8   Pipe_17  -405.530240   134.493077    -809.009471      -2.051009\n",
            "9   Pipe_18  -777.124479   242.406106   -1504.342797     -49.906162\n",
            "10  Pipe_19  -794.791479   247.168817   -1536.297930     -53.285029\n",
            "11   Pipe_2  5386.296760  1649.485821     437.839298   10334.754222\n",
            "12  Pipe_20  2207.171610   750.726857     -45.008962    4459.352181\n",
            "13  Pipe_21   408.035774   252.644128    -349.896611    1165.968159\n",
            "14  Pipe_22   128.769452    41.299230       4.871761     252.667143\n",
            "15  Pipe_23  1447.923884   520.511223    -113.609784    3009.457552\n",
            "16  Pipe_24   896.787986   276.812724      66.349816    1727.226157\n",
            "17  Pipe_25   656.141671   205.627531      39.259077    1273.024265\n",
            "18  Pipe_26  -273.569630   102.998892    -582.566305      35.427044\n",
            "19  Pipe_27   -20.996363    60.255449    -201.762709     159.769983\n",
            "20  Pipe_28    81.056301    67.437810    -121.257130     283.369732\n",
            "21  Pipe_29   258.814212   258.231663    -515.880777    1033.509201\n",
            "22   Pipe_3  2150.557541   648.409214     205.329901    4095.785182\n",
            "23  Pipe_30   107.857110   102.299318    -199.040846     414.755065\n",
            "24  Pipe_31     9.254527    96.846744    -281.285704     299.794759\n",
            "25  Pipe_32   -90.028870   100.600031    -391.828963     211.771223\n",
            "26  Pipe_33   119.069342   103.423985    -191.202612     429.341297\n",
            "27  Pipe_34   336.010973   139.678505     -83.024543     755.046488\n",
            "28   Pipe_4  2115.158836   637.697454     202.066475    4028.251197\n",
            "29   Pipe_5  1907.977644   579.325887     169.999984    3645.955304\n",
            "30   Pipe_6  1632.638527   497.608352     139.813471    3125.463583\n",
            "31   Pipe_7  1274.619699   395.602830      87.811209    2461.428188\n",
            "32   Pipe_8  1118.207281   349.674631      69.183389    2167.231172\n",
            "33   Pipe_9   968.378753   308.317622      43.425887    1893.331620\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Testing feature combination: ['measurement']\n",
            "Testing feature combination: ['diameter']\n",
            "Testing feature combination: ['length']\n",
            "Testing feature combination: ['roughness']\n",
            "Testing feature combination: ['measurement', 'diameter']\n",
            "Testing feature combination: ['measurement', 'length']\n",
            "Testing feature combination: ['measurement', 'roughness']\n",
            "Testing feature combination: ['diameter', 'length']\n",
            "Testing feature combination: ['diameter', 'roughness']\n",
            "Testing feature combination: ['length', 'roughness']\n",
            "Testing feature combination: ['measurement', 'diameter', 'length']\n",
            "Testing feature combination: ['measurement', 'diameter', 'roughness']\n",
            "Testing feature combination: ['measurement', 'length', 'roughness']\n",
            "Testing feature combination: ['diameter', 'length', 'roughness']\n",
            "Testing feature combination: ['measurement', 'diameter', 'length', 'roughness']\n",
            "\n",
            "Feature Selection Results:\n",
            "                           Feature Combination  Avg F1 Score  Std F1 Score\n",
            "0                                [measurement]      0.711156      0.006002\n",
            "4                      [measurement, diameter]      0.695968      0.001136\n",
            "10             [measurement, diameter, length]      0.599589      0.025787\n",
            "11          [measurement, diameter, roughness]      0.591774      0.015071\n",
            "6                     [measurement, roughness]      0.587929      0.022573\n",
            "5                        [measurement, length]      0.587690      0.000895\n",
            "1                                   [diameter]      0.556429      0.013631\n",
            "14  [measurement, diameter, length, roughness]      0.543551      0.009133\n",
            "12            [measurement, length, roughness]      0.524196      0.013648\n",
            "7                           [diameter, length]      0.488989      0.026027\n",
            "8                        [diameter, roughness]      0.482177      0.008828\n",
            "13               [diameter, length, roughness]      0.457138      0.017027\n",
            "9                          [length, roughness]      0.294584      0.057653\n",
            "2                                     [length]      0.181897      0.013598\n",
            "3                                  [roughness]      0.168987      0.009023\n",
            "Results saved to '/content/drive/My Drive/'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "import itertools\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming combined_df and pipe_df are already defined from previous steps\n",
        "merged_df = pd.merge(combined_df, pipe_df, on='pipe_id', how='inner')\n",
        "print(f\"Merged data size: {len(merged_df)} rows.\")\n",
        "\n",
        "# Compute global flow stats\n",
        "global_flow_mean = merged_df['measurement'].mean()\n",
        "global_flow_std = merged_df['measurement'].std()\n",
        "print(f\"Global flow stats: Mean = {global_flow_mean:.2f}, Std = {global_flow_std:.2f}\")\n",
        "\n",
        "# Compute per-pipe statistics\n",
        "pipe_stats_df = merged_df.groupby('pipe_id')['measurement'].agg(['mean', 'std', 'count']).reset_index()\n",
        "pipe_stats_df.columns = ['pipe_id', 'actual_mean', 'actual_std', 'sample_count']\n",
        "pipe_stats_df = pipe_stats_df[pipe_stats_df['sample_count'] >= 100]\n",
        "\n",
        "# Compute global pipe std for minimum threshold\n",
        "global_pipe_std = pipe_stats_df['actual_std'].mean()\n",
        "\n",
        "# Apply a minimum std to avoid overly narrow ranges (10% of global pipe std)\n",
        "pipe_stats_df['actual_std'] = pipe_stats_df['actual_std'].apply(lambda x: max(x, global_pipe_std * 0.1))\n",
        "\n",
        "# Compute plausible ranges using pipe-level stats with fixed k\n",
        "k = 3  # Fixed number of standard deviations for plausible range\n",
        "pipe_stats_df['min_plausible'] = pipe_stats_df['actual_mean'] - k * pipe_stats_df['actual_std']\n",
        "pipe_stats_df['max_plausible'] = pipe_stats_df['actual_mean'] + k * pipe_stats_df['actual_std']\n",
        "print(\"Per-pipe plausible ranges (fixed k):\\n\", pipe_stats_df[['pipe_id', 'actual_mean', 'actual_std', 'min_plausible', 'max_plausible']])\n",
        "\n",
        "# Merge the plausible ranges into the main DataFrame\n",
        "merged_df = merged_df.merge(pipe_stats_df[['pipe_id', 'min_plausible', 'max_plausible']], on='pipe_id', how='left')\n",
        "\n",
        "# Use full dataset (100% with no subsampling)\n",
        "train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "# Define all possible feature combinations\n",
        "all_features = ['measurement', 'diameter', 'length', 'roughness']\n",
        "feature_combinations = []\n",
        "for r in range(1, len(all_features) + 1):\n",
        "    feature_combinations.extend(list(itertools.combinations(all_features, r)))\n",
        "\n",
        "results_list = []\n",
        "\n",
        "# 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for features in feature_combinations:\n",
        "    features = list(features)\n",
        "    print(f\"Testing feature combination: {features}\")\n",
        "\n",
        "    fold_f1_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
        "        fold_train_df = train_df.iloc[train_idx]\n",
        "        fold_val_df = train_df.iloc[val_idx]\n",
        "\n",
        "        # Scale features\n",
        "        X_cluster_train = fold_train_df[features]\n",
        "        scaler = RobustScaler()\n",
        "        X_scaled_train = scaler.fit_transform(X_cluster_train)\n",
        "\n",
        "        # Determine optimal number of clusters using elbow method (simplified for each fold)\n",
        "        wcss = []\n",
        "        for i in range(2, 12):\n",
        "            kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "            kmeans.fit(X_scaled_train)\n",
        "            wcss.append(kmeans.inertia_)\n",
        "        optimal_clusters = 7  # Fixed based on previous analysis, adjust if needed\n",
        "\n",
        "        kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "        train_cluster_labels = kmeans.fit_predict(X_scaled_train)\n",
        "\n",
        "        X_cluster_val = fold_val_df[features]\n",
        "        X_scaled_val = scaler.transform(X_cluster_val)\n",
        "        val_cluster_labels = kmeans.predict(X_scaled_val)\n",
        "\n",
        "        fold_train_df['cluster'] = train_cluster_labels\n",
        "        fold_val_df['cluster'] = val_cluster_labels\n",
        "\n",
        "        # Compute cluster statistics on training fold\n",
        "        cluster_stats = fold_train_df.groupby('cluster').agg({\n",
        "            'measurement': ['mean', 'std', 'count'],\n",
        "            'pipe_id': lambda x: list(x.unique())\n",
        "        }).reset_index()\n",
        "        cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']\n",
        "        cluster_stats['cluster_mu'] = cluster_stats['cluster_mu'].round(2)\n",
        "        cluster_stats['cluster_sigma'] = cluster_stats['cluster_sigma'].round(2)\n",
        "\n",
        "        cluster_means = fold_train_df.groupby('cluster')['measurement'].mean().to_dict()\n",
        "        cluster_stds = fold_train_df.groupby('cluster')['measurement'].std().to_dict()\n",
        "        fold_val_df['cluster_mu'] = fold_val_df['cluster'].map(cluster_means)\n",
        "        fold_val_df['cluster_sigma'] = fold_val_df['cluster'].map(cluster_stds).fillna(global_pipe_std)\n",
        "\n",
        "        fold_val_df['Z_score'] = (fold_val_df['measurement'] - fold_val_df['cluster_mu']) / fold_val_df['cluster_sigma'].replace(0, np.nan)\n",
        "        fold_val_df['traditional_Z_score'] = (fold_val_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "\n",
        "        fold_val_df['outside_plausible'] = (fold_val_df['measurement'] < fold_val_df['min_plausible']) | (fold_val_df['measurement'] > fold_val_df['max_plausible'])\n",
        "        fold_val_df['is_anomaly'] = np.abs(fold_val_df['Z_score']) > 3\n",
        "        fold_val_df['is_anomaly_traditional'] = np.abs(fold_val_df['traditional_Z_score']) > 3\n",
        "        fold_val_df['is_noise'] = fold_val_df['is_anomaly'] & fold_val_df['outside_plausible']\n",
        "\n",
        "        # Introduce noise for this fold\n",
        "        noisy_val_df = introduce_gaussian_noise(fold_val_df, noise_level=global_flow_std, noise_fraction=0.20)\n",
        "\n",
        "        noisy_val_df['Z_score'] = (noisy_val_df['measurement'] - noisy_val_df['cluster_mu']) / noisy_val_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_val_df['traditional_Z_score'] = (noisy_val_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "        noisy_val_df['outside_plausible'] = (noisy_val_df['measurement'] < noisy_val_df['min_plausible']) | (noisy_val_df['measurement'] > noisy_val_df['max_plausible'])\n",
        "        noisy_val_df['is_anomaly'] = np.abs(noisy_val_df['Z_score']) > 3\n",
        "        noisy_val_df['is_anomaly_traditional'] = np.abs(noisy_val_df['traditional_Z_score']) > 3\n",
        "        noisy_val_df['is_noise'] = noisy_val_df['is_anomaly'] & noisy_val_df['outside_plausible']\n",
        "\n",
        "        y_true = noisy_val_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_val_df['is_noise'].astype(int)\n",
        "\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        fold_f1_scores.append(f1_hybrid)\n",
        "\n",
        "    avg_f1_score = np.mean(fold_f1_scores)\n",
        "    std_f1_score = np.std(fold_f1_scores)\n",
        "    results_list.append({\n",
        "        'Feature Combination': features,\n",
        "        'Avg F1 Score': avg_f1_score,\n",
        "        'Std F1 Score': std_f1_score\n",
        "    })\n",
        "\n",
        "# Sort results by Avg F1 Score\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df = results_df.sort_values(by='Avg F1 Score', ascending=False)\n",
        "print(\"\\nFeature Selection Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Save results\n",
        "try:\n",
        "    results_df.to_csv('/content/drive/My Drive/feature_selection_results_kmeans_flow.csv', index=False)\n",
        "    print(\"Results saved to '/content/drive/My Drive/'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTNTEusc8LQg",
        "outputId": "4c7fb755-4286-4996-92f6-5b9a2279feb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged data size: 5956800 rows.\n",
            "Global flow stats: Mean = 829.88, Std = 1495.64\n",
            "Per-pipe plausible ranges (fixed k):\n",
            "     pipe_id  actual_mean   actual_std  min_plausible  max_plausible\n",
            "0    Pipe_1  5627.037288  1718.679788     470.997924   10783.076651\n",
            "1   Pipe_10   550.314267   195.561336     -36.369742    1136.998276\n",
            "2   Pipe_11   412.248822   162.662078     -75.737413     900.235057\n",
            "3   Pipe_12   253.255767    83.374362       3.132682     503.378853\n",
            "4   Pipe_13   266.276404    99.849458     -33.271971     565.824779\n",
            "5   Pipe_14    88.363952    60.337333     -92.648046     269.375950\n",
            "6   Pipe_15    13.391692    56.662783    -156.596656     183.380040\n",
            "7   Pipe_16   156.229705    78.588859     -79.536872     391.996283\n",
            "8   Pipe_17  -405.530240   134.493077    -809.009471      -2.051009\n",
            "9   Pipe_18  -777.124479   242.406106   -1504.342797     -49.906162\n",
            "10  Pipe_19  -794.791479   247.168817   -1536.297930     -53.285029\n",
            "11   Pipe_2  5386.296760  1649.485821     437.839298   10334.754222\n",
            "12  Pipe_20  2207.171610   750.726857     -45.008962    4459.352181\n",
            "13  Pipe_21   408.035774   252.644128    -349.896611    1165.968159\n",
            "14  Pipe_22   128.769452    41.299230       4.871761     252.667143\n",
            "15  Pipe_23  1447.923884   520.511223    -113.609784    3009.457552\n",
            "16  Pipe_24   896.787986   276.812724      66.349816    1727.226157\n",
            "17  Pipe_25   656.141671   205.627531      39.259077    1273.024265\n",
            "18  Pipe_26  -273.569630   102.998892    -582.566305      35.427044\n",
            "19  Pipe_27   -20.996363    60.255449    -201.762709     159.769983\n",
            "20  Pipe_28    81.056301    67.437810    -121.257130     283.369732\n",
            "21  Pipe_29   258.814212   258.231663    -515.880777    1033.509201\n",
            "22   Pipe_3  2150.557541   648.409214     205.329901    4095.785182\n",
            "23  Pipe_30   107.857110   102.299318    -199.040846     414.755065\n",
            "24  Pipe_31     9.254527    96.846744    -281.285704     299.794759\n",
            "25  Pipe_32   -90.028870   100.600031    -391.828963     211.771223\n",
            "26  Pipe_33   119.069342   103.423985    -191.202612     429.341297\n",
            "27  Pipe_34   336.010973   139.678505     -83.024543     755.046488\n",
            "28   Pipe_4  2115.158836   637.697454     202.066475    4028.251197\n",
            "29   Pipe_5  1907.977644   579.325887     169.999984    3645.955304\n",
            "30   Pipe_6  1632.638527   497.608352     139.813471    3125.463583\n",
            "31   Pipe_7  1274.619699   395.602830      87.811209    2461.428188\n",
            "32   Pipe_8  1118.207281   349.674631      69.183389    2167.231172\n",
            "33   Pipe_9   968.378753   308.317622      43.425887    1893.331620\n",
            "Subsampled data size: 297840\n",
            "Training set size: 238272, Test set size: 59568\n",
            "Testing feature combination: ['measurement']\n",
            "Testing feature combination: ['diameter']\n",
            "  Fold 0: No valid parameters found for ['diameter']. Using default eps=0.010, min_samples=50\n",
            "  Fold 1: No valid parameters found for ['diameter']. Using default eps=0.010, min_samples=50\n",
            "  Fold 2: No valid parameters found for ['diameter']. Using default eps=0.010, min_samples=50\n",
            "  Fold 3: No valid parameters found for ['diameter']. Using default eps=0.010, min_samples=50\n",
            "  Fold 4: No valid parameters found for ['diameter']. Using default eps=0.010, min_samples=50\n",
            "Testing feature combination: ['length']\n",
            "  Fold 0: No valid parameters found for ['length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 1: No valid parameters found for ['length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 2: No valid parameters found for ['length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 3: No valid parameters found for ['length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 4: No valid parameters found for ['length']. Using default eps=0.010, min_samples=50\n",
            "Testing feature combination: ['roughness']\n",
            "  Fold 0: No valid parameters found for ['roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 1: No valid parameters found for ['roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 2: No valid parameters found for ['roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 3: No valid parameters found for ['roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 4: No valid parameters found for ['roughness']. Using default eps=0.010, min_samples=50\n",
            "Testing feature combination: ['measurement', 'diameter']\n",
            "Testing feature combination: ['measurement', 'length']\n",
            "Testing feature combination: ['measurement', 'roughness']\n",
            "Testing feature combination: ['diameter', 'length']\n",
            "  Fold 0: No valid parameters found for ['diameter', 'length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 1: No valid parameters found for ['diameter', 'length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 2: No valid parameters found for ['diameter', 'length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 3: No valid parameters found for ['diameter', 'length']. Using default eps=0.010, min_samples=50\n",
            "  Fold 4: No valid parameters found for ['diameter', 'length']. Using default eps=0.010, min_samples=50\n",
            "Testing feature combination: ['diameter', 'roughness']\n",
            "  Fold 0: No valid parameters found for ['diameter', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 1: No valid parameters found for ['diameter', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 2: No valid parameters found for ['diameter', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 3: No valid parameters found for ['diameter', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 4: No valid parameters found for ['diameter', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "Testing feature combination: ['length', 'roughness']\n",
            "  Fold 0: No valid parameters found for ['length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 1: No valid parameters found for ['length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 2: No valid parameters found for ['length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 3: No valid parameters found for ['length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 4: No valid parameters found for ['length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "Testing feature combination: ['measurement', 'diameter', 'length']\n",
            "Testing feature combination: ['measurement', 'diameter', 'roughness']\n",
            "Testing feature combination: ['measurement', 'length', 'roughness']\n",
            "Testing feature combination: ['diameter', 'length', 'roughness']\n",
            "  Fold 0: No valid parameters found for ['diameter', 'length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 1: No valid parameters found for ['diameter', 'length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 2: No valid parameters found for ['diameter', 'length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 3: No valid parameters found for ['diameter', 'length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "  Fold 4: No valid parameters found for ['diameter', 'length', 'roughness']. Using default eps=0.010, min_samples=50\n",
            "Testing feature combination: ['measurement', 'diameter', 'length', 'roughness']\n",
            "\n",
            "Feature Selection Results:\n",
            "                           Feature Combination  Avg F1 Score  Std F1 Score\n",
            "14  [measurement, diameter, length, roughness]      0.665592      0.014928\n",
            "12            [measurement, length, roughness]      0.658213      0.010984\n",
            "11          [measurement, diameter, roughness]      0.656598      0.013789\n",
            "6                     [measurement, roughness]      0.653255      0.010211\n",
            "10             [measurement, diameter, length]      0.650904      0.010263\n",
            "13               [diameter, length, roughness]      0.647383      0.011232\n",
            "9                          [length, roughness]      0.646838      0.014233\n",
            "7                           [diameter, length]      0.646079      0.012898\n",
            "8                        [diameter, roughness]      0.645976      0.014040\n",
            "2                                     [length]      0.623648      0.006221\n",
            "5                        [measurement, length]      0.622040      0.015416\n",
            "1                                   [diameter]      0.621139      0.016260\n",
            "3                                  [roughness]      0.617942      0.010183\n",
            "4                      [measurement, diameter]      0.603807      0.022081\n",
            "0                                [measurement]      0.108399      0.064943\n",
            "Results saved to '/content/drive/My Drive/'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "import itertools\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming combined_df and pipe_df are already defined from previous steps\n",
        "merged_df = pd.merge(combined_df, pipe_df, on='pipe_id', how='inner')\n",
        "print(f\"Merged data size: {len(merged_df)} rows.\")\n",
        "\n",
        "# Compute global flow stats\n",
        "global_flow_mean = merged_df['measurement'].mean()\n",
        "global_flow_std = merged_df['measurement'].std()\n",
        "print(f\"Global flow stats: Mean = {global_flow_mean:.2f}, Std = {global_flow_std:.2f}\")\n",
        "\n",
        "# Compute per-pipe statistics\n",
        "pipe_stats_df = merged_df.groupby('pipe_id')['measurement'].agg(['mean', 'std', 'count']).reset_index()\n",
        "pipe_stats_df.columns = ['pipe_id', 'actual_mean', 'actual_std', 'sample_count']\n",
        "pipe_stats_df = pipe_stats_df[pipe_stats_df['sample_count'] >= 100]\n",
        "\n",
        "# Compute global pipe std for minimum threshold\n",
        "global_pipe_std = pipe_stats_df['actual_std'].mean()\n",
        "\n",
        "# Apply a minimum std to avoid overly narrow ranges (10% of global pipe std)\n",
        "pipe_stats_df['actual_std'] = pipe_stats_df['actual_std'].apply(lambda x: max(x, global_pipe_std * 0.1))\n",
        "\n",
        "# Compute plausible ranges using pipe-level stats with fixed k\n",
        "k = 3  # Fixed number of standard deviations for plausible range\n",
        "pipe_stats_df['min_plausible'] = pipe_stats_df['actual_mean'] - k * pipe_stats_df['actual_std']\n",
        "pipe_stats_df['max_plausible'] = pipe_stats_df['actual_mean'] + k * pipe_stats_df['actual_std']\n",
        "print(\"Per-pipe plausible ranges (fixed k):\\n\", pipe_stats_df[['pipe_id', 'actual_mean', 'actual_std', 'min_plausible', 'max_plausible']])\n",
        "\n",
        "# Merge the plausible ranges into the main DataFrame\n",
        "merged_df = merged_df.merge(pipe_stats_df[['pipe_id', 'min_plausible', 'max_plausible']], on='pipe_id', how='left')\n",
        "\n",
        "# Subsample the data to 10% as requested\n",
        "subsample_fraction = 0.05  # Use 10% of the data\n",
        "subsampled_df = merged_df.sample(frac=subsample_fraction, random_state=42)\n",
        "print(f\"Subsampled data size: {len(subsampled_df)}\")\n",
        "\n",
        "# Split into training and test sets\n",
        "train_df, test_df = train_test_split(subsampled_df, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "# Define all possible feature combinations\n",
        "all_features = ['measurement', 'diameter', 'length', 'roughness']\n",
        "feature_combinations = []\n",
        "for r in range(1, len(all_features) + 1):\n",
        "    feature_combinations.extend(list(itertools.combinations(all_features, r)))\n",
        "\n",
        "# 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results_list = []\n",
        "\n",
        "def introduce_gaussian_noise(df, noise_level=1495.64, noise_fraction=0.05):\n",
        "    noisy_df = df.copy()\n",
        "    mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "    noise = np.random.normal(0, noise_level, size=len(noisy_df))\n",
        "    noisy_df['measurement'] = noisy_df['measurement'] + noise * mask\n",
        "    noisy_df['is_true_noise'] = mask.astype(int)\n",
        "    return noisy_df\n",
        "\n",
        "for features in feature_combinations:\n",
        "    features = list(features)\n",
        "    print(f\"Testing feature combination: {features}\")\n",
        "\n",
        "    fold_f1_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
        "        fold_train_df = train_df.iloc[train_idx]\n",
        "        fold_val_df = train_df.iloc[val_idx]\n",
        "\n",
        "        # Scale features\n",
        "        X_cluster_train = fold_train_df[features]\n",
        "        scaler = RobustScaler()\n",
        "        X_scaled_train = scaler.fit_transform(X_cluster_train)\n",
        "\n",
        "        # Generate k-distance plot to determine eps (on a smaller sample for efficiency)\n",
        "        min_samples = 50\n",
        "        neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_scaled_train)\n",
        "        distances, indices = neighbors.kneighbors(X_scaled_train)\n",
        "        k_distances = np.sort(distances[:, min_samples - 1])\n",
        "        suggested_eps = np.percentile(k_distances, 95)\n",
        "\n",
        "        # Ensure suggested_eps is positive\n",
        "        suggested_eps = max(suggested_eps, 0.01)  # Minimum eps value\n",
        "\n",
        "        # Parameter tuning using silhouette score\n",
        "        eps_values = np.arange(max(0.005, suggested_eps - 0.005), suggested_eps + 0.005, 0.002)\n",
        "        min_samples_values = [20, 30, 50, 60, 70]\n",
        "        best_score = -1\n",
        "        best_params = {'eps': suggested_eps, 'min_samples': 50}  # Default to suggested_eps and min_samples\n",
        "\n",
        "        for eps in eps_values:\n",
        "            for min_samples in min_samples_values:\n",
        "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "                labels = dbscan.fit_predict(X_scaled_train)\n",
        "                if len(np.unique(labels)) > 1 and -1 in labels:  # Ensure multiple clusters and noise points\n",
        "                    score = silhouette_score(X_scaled_train, labels, sample_size=5000)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "        # If no valid parameters found, log and use default\n",
        "        if best_params['eps'] == suggested_eps and best_params['min_samples'] == 50:\n",
        "            print(f\"  Fold {fold}: No valid parameters found for {features}. Using default eps={suggested_eps:.3f}, min_samples=50\")\n",
        "\n",
        "        # Use best parameters for clustering\n",
        "        dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "        train_cluster_labels = dbscan.fit_predict(X_scaled_train)\n",
        "\n",
        "        X_cluster_val = fold_val_df[features]\n",
        "        X_scaled_val = scaler.transform(X_cluster_val)\n",
        "        val_cluster_labels = dbscan.fit_predict(X_scaled_val)\n",
        "\n",
        "        fold_train_df['cluster'] = train_cluster_labels\n",
        "        fold_val_df['cluster'] = val_cluster_labels\n",
        "\n",
        "        # Compute cluster statistics (non-noise clusters)\n",
        "        cluster_stats = fold_train_df[fold_train_df['cluster'] != -1].groupby('cluster').agg({\n",
        "            'measurement': ['mean', 'std', 'count'],\n",
        "            'pipe_id': lambda x: list(x.unique())\n",
        "        }).reset_index()\n",
        "        cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']\n",
        "        cluster_stats['cluster_mu'] = cluster_stats['cluster_mu'].round(2)\n",
        "        cluster_stats['cluster_sigma'] = cluster_stats['cluster_sigma'].round(2)\n",
        "\n",
        "        # Merge small clusters\n",
        "        min_cluster_size = 100\n",
        "        small_clusters = cluster_stats[cluster_stats['point_count'] < min_cluster_size]['cluster'].tolist()\n",
        "        fold_val_df.loc[fold_val_df['cluster'].isin(small_clusters), 'cluster'] = -1\n",
        "\n",
        "        # Recompute cluster statistics\n",
        "        cluster_stats = fold_train_df[fold_train_df['cluster'] != -1].groupby('cluster').agg({\n",
        "            'measurement': ['mean', 'std', 'count'],\n",
        "            'pipe_id': lambda x: list(x.unique())\n",
        "        }).reset_index()\n",
        "        cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']\n",
        "\n",
        "        cluster_means = fold_train_df[fold_train_df['cluster'] != -1].groupby('cluster')['measurement'].mean().to_dict()\n",
        "        cluster_stds = fold_train_df[fold_train_df['cluster'] != -1].groupby('cluster')['measurement'].std().to_dict()\n",
        "        min_sigma = global_pipe_std * 0.1\n",
        "        cluster_stds = {k: max(v, min_sigma) for k, v in cluster_stds.items()}\n",
        "        fold_val_df['cluster_mu'] = fold_val_df['cluster'].map(cluster_means)\n",
        "        fold_val_df['cluster_sigma'] = fold_val_df['cluster'].map(cluster_stds).fillna(global_pipe_std)\n",
        "        fold_val_df.loc[fold_val_df['cluster'] == -1, 'cluster_mu'] = global_flow_mean\n",
        "        fold_val_df.loc[fold_val_df['cluster'] == -1, 'cluster_sigma'] = global_flow_std\n",
        "\n",
        "        fold_val_df['Z_score'] = (fold_val_df['measurement'] - fold_val_df['cluster_mu']) / fold_val_df['cluster_sigma'].replace(0, np.nan)\n",
        "        fold_val_df['outside_plausible'] = (fold_val_df['measurement'] < fold_val_df['min_plausible']) | (fold_val_df['measurement'] > fold_val_df['max_plausible'])\n",
        "        fold_val_df['is_anomaly'] = np.abs(fold_val_df['Z_score']) > 3\n",
        "        fold_val_df['is_noise'] = fold_val_df['is_anomaly'] & fold_val_df['outside_plausible']\n",
        "\n",
        "        # Introduce noise (β=1, 20% noise fraction)\n",
        "        noisy_val_df = introduce_gaussian_noise(fold_val_df, noise_level=global_flow_std, noise_fraction=0.20)\n",
        "\n",
        "        noisy_val_df['Z_score'] = (noisy_val_df['measurement'] - noisy_val_df['cluster_mu']) / noisy_val_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_val_df['outside_plausible'] = (noisy_val_df['measurement'] < noisy_val_df['min_plausible']) | (noisy_val_df['measurement'] > noisy_val_df['max_plausible'])\n",
        "        noisy_val_df['is_anomaly'] = np.abs(noisy_val_df['Z_score']) > 3\n",
        "        noisy_val_df['is_noise'] = noisy_val_df['is_anomaly'] & noisy_val_df['outside_plausible']\n",
        "\n",
        "        y_true = noisy_val_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_val_df['is_noise'].astype(int)\n",
        "\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        fold_f1_scores.append(f1_hybrid)\n",
        "\n",
        "    avg_f1_score = np.mean(fold_f1_scores)\n",
        "    std_f1_score = np.std(fold_f1_scores)\n",
        "    results_list.append({\n",
        "        'Feature Combination': features,\n",
        "        'Avg F1 Score': avg_f1_score,\n",
        "        'Std F1 Score': std_f1_score\n",
        "    })\n",
        "\n",
        "# Sort results by Avg F1 Score\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df = results_df.sort_values(by='Avg F1 Score', ascending=False)\n",
        "print(\"\\nFeature Selection Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Save results\n",
        "try:\n",
        "    results_df.to_csv('/content/drive/My Drive/feature_selection_results_dbscan_flow.csv', index=False)\n",
        "    print(\"Results saved to '/content/drive/My Drive/'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbaCqLR8w9Mk",
        "outputId": "c6c85621-973a-40b5-8cfd-76f6b8f19405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data size: 5956800 rows.\n",
            "Global flow stats: Mean = 829.88, Std = 1495.64\n",
            "Per-pipe plausible ranges (fixed k):\n",
            "     pipe_id  actual_mean   actual_std  min_plausible  max_plausible\n",
            "0    Pipe_1  5627.037288  1718.679788     470.997924   10783.076651\n",
            "1   Pipe_10   550.314267   195.561336     -36.369742    1136.998276\n",
            "2   Pipe_11   412.248822   162.662078     -75.737413     900.235057\n",
            "3   Pipe_12   253.255767    83.374362       3.132682     503.378853\n",
            "4   Pipe_13   266.276404    99.849458     -33.271971     565.824779\n",
            "5   Pipe_14    88.363952    60.337333     -92.648046     269.375950\n",
            "6   Pipe_15    13.391692    56.662783    -156.596656     183.380040\n",
            "7   Pipe_16   156.229705    78.588859     -79.536872     391.996283\n",
            "8   Pipe_17  -405.530240   134.493077    -809.009471      -2.051009\n",
            "9   Pipe_18  -777.124479   242.406106   -1504.342797     -49.906162\n",
            "10  Pipe_19  -794.791479   247.168817   -1536.297930     -53.285029\n",
            "11   Pipe_2  5386.296760  1649.485821     437.839298   10334.754222\n",
            "12  Pipe_20  2207.171610   750.726857     -45.008962    4459.352181\n",
            "13  Pipe_21   408.035774   252.644128    -349.896611    1165.968159\n",
            "14  Pipe_22   128.769452    41.299230       4.871761     252.667143\n",
            "15  Pipe_23  1447.923884   520.511223    -113.609784    3009.457552\n",
            "16  Pipe_24   896.787986   276.812724      66.349816    1727.226157\n",
            "17  Pipe_25   656.141671   205.627531      39.259077    1273.024265\n",
            "18  Pipe_26  -273.569630   102.998892    -582.566305      35.427044\n",
            "19  Pipe_27   -20.996363    60.255449    -201.762709     159.769983\n",
            "20  Pipe_28    81.056301    67.437810    -121.257130     283.369732\n",
            "21  Pipe_29   258.814212   258.231663    -515.880777    1033.509201\n",
            "22   Pipe_3  2150.557541   648.409214     205.329901    4095.785182\n",
            "23  Pipe_30   107.857110   102.299318    -199.040846     414.755065\n",
            "24  Pipe_31     9.254527    96.846744    -281.285704     299.794759\n",
            "25  Pipe_32   -90.028870   100.600031    -391.828963     211.771223\n",
            "26  Pipe_33   119.069342   103.423985    -191.202612     429.341297\n",
            "27  Pipe_34   336.010973   139.678505     -83.024543     755.046488\n",
            "28   Pipe_4  2115.158836   637.697454     202.066475    4028.251197\n",
            "29   Pipe_5  1907.977644   579.325887     169.999984    3645.955304\n",
            "30   Pipe_6  1632.638527   497.608352     139.813471    3125.463583\n",
            "31   Pipe_7  1274.619699   395.602830      87.811209    2461.428188\n",
            "32   Pipe_8  1118.207281   349.674631      69.183389    2167.231172\n",
            "33   Pipe_9   968.378753   308.317622      43.425887    1893.331620\n",
            "Processing Fold 1\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 1: 7\n",
            "\n",
            "Cluster Statistics (Training Set, Fold {fold}):\n",
            "   cluster  cluster_mu  cluster_sigma  point_count  \\\n",
            "0        0      -41.56         321.27      1120746   \n",
            "1        1     1932.07         740.19       452379   \n",
            "2        2      458.59         353.24       837069   \n",
            "3        3     6196.86        1159.77       220379   \n",
            "4        4     1658.31         688.67       817233   \n",
            "5        5        9.66         478.71       756800   \n",
            "6        6       27.89         275.37       560834   \n",
            "\n",
            "                                               pipes  \n",
            "0  [Pipe_13, Pipe_14, Pipe_15, Pipe_19, Pipe_22, ...  \n",
            "1                 [Pipe_2, Pipe_20, Pipe_23, Pipe_5]  \n",
            "2  [Pipe_10, Pipe_21, Pipe_24, Pipe_25, Pipe_29, ...  \n",
            "3  [Pipe_1, Pipe_2, Pipe_3, Pipe_4, Pipe_20, Pipe...  \n",
            "4  [Pipe_1, Pipe_24, Pipe_3, Pipe_4, Pipe_6, Pipe...  \n",
            "5  [Pipe_11, Pipe_18, Pipe_26, Pipe_33, Pipe_34, ...  \n",
            "6               [Pipe_12, Pipe_16, Pipe_17, Pipe_30]  \n",
            "Processing noise fraction: 2.0% in Fold 1\n",
            "Processing noise fraction: 5.0% in Fold 1\n",
            "Processing noise fraction: 10.0% in Fold 1\n",
            "Processing noise fraction: 20.0% in Fold 1\n",
            "Processing Fold 2\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 2: 7\n",
            "\n",
            "Cluster Statistics (Training Set, Fold {fold}):\n",
            "   cluster  cluster_mu  cluster_sigma  point_count  \\\n",
            "0        0      -40.86         321.31      1121218   \n",
            "1        1     1932.35         738.16       454449   \n",
            "2        2      227.30         367.63       978007   \n",
            "3        3     6199.36        1160.90       220350   \n",
            "4        4      253.30          83.28       140046   \n",
            "5        5     1633.02         688.07       851421   \n",
            "6        6      109.00         491.84       999949   \n",
            "\n",
            "                                               pipes  \n",
            "0  [Pipe_13, Pipe_14, Pipe_15, Pipe_19, Pipe_22, ...  \n",
            "1        [Pipe_2, Pipe_20, Pipe_23, Pipe_5, Pipe_21]  \n",
            "2  [Pipe_10, Pipe_16, Pipe_17, Pipe_21, Pipe_25, ...  \n",
            "3  [Pipe_1, Pipe_2, Pipe_3, Pipe_4, Pipe_20, Pipe...  \n",
            "4                                          [Pipe_12]  \n",
            "5  [Pipe_1, Pipe_10, Pipe_24, Pipe_3, Pipe_4, Pip...  \n",
            "6  [Pipe_11, Pipe_18, Pipe_24, Pipe_26, Pipe_30, ...  \n",
            "Processing noise fraction: 2.0% in Fold 2\n",
            "Processing noise fraction: 5.0% in Fold 2\n",
            "Processing noise fraction: 10.0% in Fold 2\n",
            "Processing noise fraction: 20.0% in Fold 2\n",
            "Processing Fold 3\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 3: 7\n",
            "\n",
            "Cluster Statistics (Training Set, Fold {fold}):\n",
            "   cluster  cluster_mu  cluster_sigma  point_count  \\\n",
            "0        0      -40.84         321.36      1121473   \n",
            "1        1     1934.53         738.41       454452   \n",
            "2        2     6202.19        1158.89       219768   \n",
            "3        3      260.84         521.96      1205733   \n",
            "4        4      253.34          83.36       140236   \n",
            "5        5     1704.03         666.94       785214   \n",
            "6        6       83.69         286.68       838564   \n",
            "\n",
            "                                               pipes  \n",
            "0  [Pipe_13, Pipe_14, Pipe_15, Pipe_19, Pipe_22, ...  \n",
            "1        [Pipe_2, Pipe_20, Pipe_23, Pipe_5, Pipe_21]  \n",
            "2  [Pipe_1, Pipe_2, Pipe_3, Pipe_4, Pipe_20, Pipe...  \n",
            "3  [Pipe_10, Pipe_11, Pipe_18, Pipe_24, Pipe_25, ...  \n",
            "4                                          [Pipe_12]  \n",
            "5  [Pipe_1, Pipe_24, Pipe_3, Pipe_4, Pipe_6, Pipe...  \n",
            "6  [Pipe_16, Pipe_17, Pipe_21, Pipe_29, Pipe_30, ...  \n",
            "Processing noise fraction: 2.0% in Fold 3\n",
            "Processing noise fraction: 5.0% in Fold 3\n",
            "Processing noise fraction: 10.0% in Fold 3\n",
            "Processing noise fraction: 20.0% in Fold 3\n",
            "Processing Fold 4\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 4: 7\n",
            "\n",
            "Cluster Statistics (Training Set, Fold {fold}):\n",
            "   cluster  cluster_mu  cluster_sigma  point_count  \\\n",
            "0        0      -40.94         321.48      1121680   \n",
            "1        1     1933.07         738.36       454617   \n",
            "2        2      253.13          83.43       140347   \n",
            "3        3      262.11         522.32      1208698   \n",
            "4        4     6201.25        1157.27       219817   \n",
            "5        5     1707.97         667.18       782208   \n",
            "6        6       83.44         286.66       838073   \n",
            "\n",
            "                                               pipes  \n",
            "0  [Pipe_13, Pipe_14, Pipe_15, Pipe_19, Pipe_22, ...  \n",
            "1        [Pipe_2, Pipe_20, Pipe_23, Pipe_5, Pipe_21]  \n",
            "2                                          [Pipe_12]  \n",
            "3  [Pipe_10, Pipe_11, Pipe_18, Pipe_24, Pipe_25, ...  \n",
            "4  [Pipe_1, Pipe_2, Pipe_3, Pipe_4, Pipe_20, Pipe...  \n",
            "5  [Pipe_1, Pipe_10, Pipe_24, Pipe_3, Pipe_4, Pip...  \n",
            "6  [Pipe_16, Pipe_17, Pipe_21, Pipe_29, Pipe_30, ...  \n",
            "Processing noise fraction: 2.0% in Fold 4\n",
            "Processing noise fraction: 5.0% in Fold 4\n",
            "Processing noise fraction: 10.0% in Fold 4\n",
            "Processing noise fraction: 20.0% in Fold 4\n",
            "Processing Fold 5\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 5: 9\n",
            "\n",
            "Cluster Statistics (Training Set, Fold {fold}):\n",
            "   cluster  cluster_mu  cluster_sigma  point_count  \\\n",
            "0        0      -55.58          89.82       280430   \n",
            "1        1     1849.89         690.79       420437   \n",
            "2        2       90.17         285.74       862514   \n",
            "3        3     6283.70        1100.06       211564   \n",
            "4        4      851.78         354.18      1029223   \n",
            "5        5      253.30          83.34       140022   \n",
            "6        6      -36.21         367.41       840598   \n",
            "7        7     2362.43         587.73       419160   \n",
            "8        8     -155.40         441.47       561492   \n",
            "\n",
            "                                               pipes  \n",
            "0                                 [Pipe_27, Pipe_32]  \n",
            "1                 [Pipe_2, Pipe_20, Pipe_23, Pipe_5]  \n",
            "2  [Pipe_16, Pipe_17, Pipe_21, Pipe_25, Pipe_29, ...  \n",
            "3  [Pipe_1, Pipe_2, Pipe_20, Pipe_3, Pipe_4, Pipe_5]  \n",
            "4  [Pipe_10, Pipe_11, Pipe_24, Pipe_25, Pipe_3, P...  \n",
            "5                                          [Pipe_12]  \n",
            "6  [Pipe_13, Pipe_14, Pipe_15, Pipe_19, Pipe_22, ...  \n",
            "7  [Pipe_1, Pipe_2, Pipe_3, Pipe_4, Pipe_6, Pipe_...  \n",
            "8      [Pipe_11, Pipe_18, Pipe_26, Pipe_33, Pipe_34]  \n",
            "Processing noise fraction: 2.0% in Fold 5\n",
            "Processing noise fraction: 5.0% in Fold 5\n",
            "Processing noise fraction: 10.0% in Fold 5\n",
            "Processing noise fraction: 20.0% in Fold 5\n",
            "\n",
            "Consolidated Evaluation Results:\n",
            "   Noise Fraction (%)                  Method  Precision    Recall  F1 Score  \\\n",
            "0                 2.0  Hybrid (Cluster-based)   0.833713  0.382386  0.522745   \n",
            "1                 2.0    Traditional (Global)   0.874279  0.009706  0.019199   \n",
            "2                 5.0  Hybrid (Cluster-based)   0.927422  0.382203  0.540460   \n",
            "3                 5.0    Traditional (Global)   0.949075  0.010043  0.019875   \n",
            "4                10.0  Hybrid (Cluster-based)   0.964038  0.381540  0.546069   \n",
            "5                10.0    Traditional (Global)   0.975522  0.010105  0.020004   \n",
            "6                20.0  Hybrid (Cluster-based)   0.983661  0.382066  0.549819   \n",
            "7                20.0    Traditional (Global)   0.988838  0.010037  0.019872   \n",
            "\n",
            "   Noise Detected (%)  \n",
            "0           38.238610  \n",
            "1            0.970611  \n",
            "2           38.220328  \n",
            "3            1.004273  \n",
            "4           38.153990  \n",
            "5            1.010544  \n",
            "6           38.206584  \n",
            "7            1.003697  \n",
            "Results saved to '/content/drive/My Drive/'.\n",
            "\n",
            "Z-score stats (noisy dataset, last noise level of last fold):\n",
            "             Z_score  traditional_Z_score\n",
            "count  5.956800e+06         5.956800e+06\n",
            "mean  -1.227062e-03        -1.963372e-04\n",
            "std    2.994236e+00         1.095214e+00\n",
            "min   -7.846575e+01        -5.542294e+00\n",
            "25%   -7.840125e-01        -5.332023e-01\n",
            "50%    1.414467e-01        -3.406421e-01\n",
            "75%    7.049416e-01         3.357256e-01\n",
            "max    7.420539e+01         8.768866e+00\n",
            "Noise detection summary (hybrid):\n",
            " is_noise\n",
            "False    5428097\n",
            "True      528703\n",
            "Name: count, dtype: int64\n",
            "Noise detection summary (traditional):\n",
            " is_noise_traditional\n",
            "False    5944741\n",
            "True       12059\n",
            "Name: count, dtype: int64\n",
            "Total noise points introduced (last noise level of last fold): 1192276\n",
            "Hybrid method detected (last noise level of last fold): 515126 (43.21%)\n",
            "Traditional method detected (last noise level of last fold): 11916 (1.00%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming combined_df and pipe_df are already defined from previous steps\n",
        "merged_df = pd.merge(combined_df, pipe_df, on='pipe_id', how='inner')\n",
        "print(f\"Merged data size: {len(merged_df)} rows.\")\n",
        "\n",
        "# Compute global flow stats\n",
        "global_flow_mean = merged_df['measurement'].mean()\n",
        "global_flow_std = merged_df['measurement'].std()\n",
        "print(f\"Global flow stats: Mean = {global_flow_mean:.2f}, Std = {global_flow_std:.2f}\")\n",
        "\n",
        "# Compute per-pipe statistics\n",
        "pipe_stats_df = merged_df.groupby('pipe_id')['measurement'].agg(['mean', 'std', 'count']).reset_index()\n",
        "pipe_stats_df.columns = ['pipe_id', 'actual_mean', 'actual_std', 'sample_count']\n",
        "pipe_stats_df = pipe_stats_df[pipe_stats_df['sample_count'] >= 100]\n",
        "\n",
        "# Compute global pipe std for minimum threshold\n",
        "global_pipe_std = pipe_stats_df['actual_std'].mean()\n",
        "\n",
        "# Apply a minimum std to avoid overly narrow ranges (10% of global pipe std)\n",
        "pipe_stats_df['actual_std'] = pipe_stats_df['actual_std'].apply(lambda x: max(x, global_pipe_std * 0.1))\n",
        "\n",
        "# Compute plausible ranges using pipe-level stats with fixed k\n",
        "k = 3  # Fixed number of standard deviations for plausible range\n",
        "pipe_stats_df['min_plausible'] = pipe_stats_df['actual_mean'] - k * pipe_stats_df['actual_std']\n",
        "pipe_stats_df['max_plausible'] = pipe_stats_df['actual_mean'] + k * pipe_stats_df['actual_std']\n",
        "print(\"Per-pipe plausible ranges (fixed k):\\n\", pipe_stats_df[['pipe_id', 'actual_mean', 'actual_std', 'min_plausible', 'max_plausible']])\n",
        "\n",
        "# Merge the plausible ranges into the main DataFrame\n",
        "merged_df = merged_df.merge(pipe_stats_df[['pipe_id', 'min_plausible', 'max_plausible']], on='pipe_id', how='left')\n",
        "\n",
        "# Initialize 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results_list = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(merged_df), 1):\n",
        "    print(f\"Processing Fold {fold}\")\n",
        "    train_df = merged_df.iloc[train_index].copy()\n",
        "    test_df = merged_df.iloc[test_index].copy()\n",
        "    print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "    features = ['measurement', 'diameter', 'length', 'roughness']\n",
        "    X_cluster_train = train_df[features]\n",
        "    scaler = RobustScaler()\n",
        "    X_scaled_train = scaler.fit_transform(X_cluster_train)\n",
        "\n",
        "    # Determine optimal number of clusters using the elbow method on training set\n",
        "    wcss = []\n",
        "    max_clusters = 11\n",
        "    for i in range(2, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "        kmeans.fit(X_scaled_train)\n",
        "        wcss.append(kmeans.inertia_)\n",
        "\n",
        "    # Plot the elbow curve for this fold (optional, can be disabled for efficiency)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(2, max_clusters + 1), wcss, marker='o')\n",
        "    plt.title(f'Elbow Method for Optimal Number of Clusters - Fold {fold}')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('WCSS')\n",
        "    plt.savefig(f'/content/drive/My Drive/elbow_curve_flow_fold_{fold}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Choose the optimal number of clusters (based on previous analysis)\n",
        "    optimal_clusters = 7\n",
        "    for i in range(2, len(wcss) - 1):\n",
        "        if (wcss[i-1] - wcss[i]) / (wcss[i] - wcss[i+1]) < 0.5:\n",
        "            optimal_clusters = i + 2\n",
        "            break\n",
        "    print(f\"Optimal number of clusters for Fold {fold}: {optimal_clusters}\")\n",
        "\n",
        "    # Perform K-means clustering with optimal number of clusters on training set\n",
        "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "    train_cluster_labels = kmeans.fit_predict(X_scaled_train)\n",
        "\n",
        "    # Assign clusters to test set using the trained K-means model\n",
        "    X_cluster_test = test_df[features]\n",
        "    X_scaled_test = scaler.transform(X_cluster_test)\n",
        "    test_cluster_labels = kmeans.predict(X_scaled_test)\n",
        "\n",
        "    # Add cluster labels to train and test dataframes\n",
        "    train_df['cluster'] = train_cluster_labels\n",
        "    test_df['cluster'] = test_cluster_labels\n",
        "\n",
        "    # Combine train and test dataframes for this fold\n",
        "    fold_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "    # Compute cluster statistics (mu, sigma, count, and associated pipes)\n",
        "    cluster_stats = train_df.groupby('cluster').agg({\n",
        "        'measurement': ['mean', 'std', 'count'],\n",
        "        'pipe_id': lambda x: list(x.unique())\n",
        "    }).reset_index()\n",
        "    cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']\n",
        "    cluster_stats['cluster_mu'] = cluster_stats['cluster_mu'].round(2)\n",
        "    cluster_stats['cluster_sigma'] = cluster_stats['cluster_sigma'].round(2)\n",
        "    print(\"\\nCluster Statistics (Training Set, Fold {fold}):\")\n",
        "    print(cluster_stats[['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']])\n",
        "\n",
        "    cluster_means = train_df.groupby('cluster')['measurement'].mean().to_dict()\n",
        "    cluster_stds = train_df.groupby('cluster')['measurement'].std().to_dict()\n",
        "    fold_df['cluster_mu'] = fold_df['cluster'].map(cluster_means)\n",
        "    fold_df['cluster_sigma'] = fold_df['cluster'].map(cluster_stds).fillna(global_pipe_std)\n",
        "\n",
        "    fold_df['Z_score'] = (fold_df['measurement'] - fold_df['cluster_mu']) / fold_df['cluster_sigma'].replace(0, np.nan)\n",
        "    fold_df['traditional_Z_score'] = (fold_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "\n",
        "    threshold = 3\n",
        "    fold_df['outside_plausible'] = (fold_df['measurement'] < fold_df['min_plausible']) | (fold_df['measurement'] > fold_df['max_plausible'])\n",
        "    fold_df['is_anomaly'] = np.abs(fold_df['Z_score']) > threshold\n",
        "    fold_df['is_anomaly_traditional'] = np.abs(fold_df['traditional_Z_score']) > threshold\n",
        "    fold_df['is_noise'] = fold_df['is_anomaly'] & fold_df['outside_plausible']\n",
        "    fold_df['is_noise_traditional'] = fold_df['is_anomaly_traditional'] & fold_df['outside_plausible']\n",
        "\n",
        "    numerical_cols = fold_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    fold_df[numerical_cols] = fold_df[numerical_cols].round(2)\n",
        "\n",
        "    def introduce_gaussian_noise(df, noise_level=1495.64, noise_fraction=0.05):\n",
        "        noisy_df = df.copy()\n",
        "        mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "        noise = np.random.normal(0, noise_level, size=len(noisy_df))\n",
        "        noisy_df['measurement'] = noisy_df['measurement'] + noise * mask\n",
        "        noisy_df['is_true_noise'] = mask.astype(int)\n",
        "        return noisy_df\n",
        "\n",
        "    noise_fractions = [0.02, 0.05, 0.10, 0.20]\n",
        "\n",
        "    for noise_fraction in noise_fractions:\n",
        "        print(f\"Processing noise fraction: {noise_fraction*100}% in Fold {fold}\")\n",
        "        noisy_df = introduce_gaussian_noise(fold_df, noise_level=global_flow_std, noise_fraction=noise_fraction)\n",
        "\n",
        "        noisy_df['Z_score'] = (noisy_df['measurement'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_df['traditional_Z_score'] = (noisy_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "        noisy_df['outside_plausible'] = (noisy_df['measurement'] < noisy_df['min_plausible']) | (noisy_df['measurement'] > noisy_df['max_plausible'])\n",
        "        noisy_df['is_anomaly'] = np.abs(noisy_df['Z_score']) > threshold\n",
        "        noisy_df['is_anomaly_traditional'] = np.abs(noisy_df['traditional_Z_score']) > threshold\n",
        "        noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "        noisy_df['is_noise_traditional'] = noisy_df['is_anomaly_traditional'] & noisy_df['outside_plausible']\n",
        "\n",
        "        y_true = noisy_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "        y_pred_traditional = noisy_df['is_noise_traditional'].astype(int)\n",
        "\n",
        "        precision_hybrid = precision_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        recall_hybrid = recall_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        precision_traditional = precision_score(y_true, y_pred_traditional, zero_division=0)\n",
        "        recall_traditional = recall_score(y_true, y_pred_traditional, zero_division=0)\n",
        "        f1_traditional = f1_score(y_true, y_pred_traditional, zero_division=0)\n",
        "\n",
        "        total_noise_points = y_true.sum()\n",
        "        hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "        traditional_detected = y_pred_traditional[y_true == 1].sum()\n",
        "        hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "        traditional_noise_percentage = (traditional_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "        results_list.append({\n",
        "            'Fold': fold,\n",
        "            'Algorithm': 'K-means',\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Hybrid (Cluster-based)',\n",
        "            'Precision': precision_hybrid,\n",
        "            'Recall': recall_hybrid,\n",
        "            'F1 Score': f1_hybrid,\n",
        "            'Noise Detected (%)': hybrid_noise_percentage\n",
        "        })\n",
        "        results_list.append({\n",
        "            'Fold': fold,\n",
        "            'Algorithm': 'K-means',\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Traditional (Global)',\n",
        "            'Precision': precision_traditional,\n",
        "            'Recall': recall_traditional,\n",
        "            'F1 Score': f1_traditional,\n",
        "            'Noise Detected (%)': traditional_noise_percentage\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(\"\\nConsolidated Evaluation Results:\")\n",
        "print(results_df.groupby(['Noise Fraction (%)', 'Method']).agg({\n",
        "    'Precision': 'mean',\n",
        "    'Recall': 'mean',\n",
        "    'F1 Score': 'mean',\n",
        "    'Noise Detected (%)': 'mean'\n",
        "}).reset_index())\n",
        "\n",
        "try:\n",
        "    noisy_df.to_csv('/content/drive/My Drive/noisy_results_kmeans_flow.csv', index=False)\n",
        "    results_df.to_csv('/content/drive/My Drive/evaluation_results_kmeans_flow.csv', index=False)\n",
        "    print(\"Results saved to '/content/drive/My Drive/'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")\n",
        "\n",
        "print(\"\\nZ-score stats (noisy dataset, last noise level of last fold):\\n\", noisy_df[['Z_score', 'traditional_Z_score']].describe())\n",
        "print(\"Noise detection summary (hybrid):\\n\", noisy_df['is_noise'].value_counts())\n",
        "print(\"Noise detection summary (traditional):\\n\", noisy_df['is_noise_traditional'].value_counts())\n",
        "print(f\"Total noise points introduced (last noise level of last fold): {total_noise_points}\")\n",
        "print(f\"Hybrid method detected (last noise level of last fold): {hybrid_detected} ({hybrid_noise_percentage:.2f}%)\")\n",
        "print(f\"Traditional method detected (last noise level of last fold): {traditional_detected} ({traditional_noise_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "from scipy.stats import gennorm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# === Data Loading and Merging ===\n",
        "# (Assumes combined_df and pipe_df are defined elsewhere in your workflow)\n",
        "merged_df = pd.merge(combined_df, pipe_df, on='pipe_id', how='inner')\n",
        "print(f\"Merged data size: {len(merged_df)} rows.\")\n",
        "\n",
        "# Compute global statistics\n",
        "global_flow_mean = merged_df['measurement'].mean()\n",
        "global_flow_std = merged_df['measurement'].std()\n",
        "print(f\"Global flow stats: Mean = {global_flow_mean:.2f}, Std = {global_flow_std:.2f}\")\n",
        "\n",
        "# Per-pipe statistics and plausible ranges\n",
        "tube_stats = merged_df.groupby('pipe_id')['measurement'].agg(['mean','std','count']).reset_index()\n",
        "tube_stats.columns = ['pipe_id','actual_mean','actual_std','sample_count']\n",
        "tube_stats = tube_stats[tube_stats['sample_count'] >= 100]\n",
        "\n",
        "global_pipe_std = tube_stats['actual_std'].mean()\n",
        "tube_stats['actual_std'] = tube_stats['actual_std'].apply(lambda x: max(x, global_pipe_std * 0.1))\n",
        "k = 3\n",
        "tube_stats['min_plausible'] = tube_stats['actual_mean'] - k * tube_stats['actual_std']\n",
        "tube_stats['max_plausible'] = tube_stats['actual_mean'] + k * tube_stats['actual_std']\n",
        "\n",
        "merged_df = merged_df.merge(\n",
        "    tube_stats[['pipe_id','min_plausible','max_plausible']],\n",
        "    on='pipe_id', how='left'\n",
        ")\n",
        "\n",
        "# Initialize 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(merged_df), 1):\n",
        "    print(f\"Processing Fold {fold}\")\n",
        "    train_df = merged_df.iloc[train_index].copy()\n",
        "    test_df = merged_df.iloc[test_index].copy()\n",
        "    print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "    # Features and scaling\n",
        "    features = ['measurement','diameter','length','roughness']\n",
        "    scaler = RobustScaler()\n",
        "    X_train = scaler.fit_transform(train_df[features])\n",
        "\n",
        "    # Elbow plot for this fold\n",
        "    wcss = []\n",
        "    for i in range(2, 12):\n",
        "        km = KMeans(n_clusters=i, random_state=42)\n",
        "        km.fit(X_train)\n",
        "        wcss.append(km.inertia_)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(range(2, 12), wcss, 'o-')\n",
        "    plt.xlabel('Clusters'); plt.ylabel('WCSS'); plt.title(f'Elbow Method - Fold {fold}')\n",
        "    plt.savefig(f'elbow_kmeans_fold_{fold}.png'); plt.close()\n",
        "\n",
        "    # Fit KMeans with chosen clusters\n",
        "    optimal_clusters = 7\n",
        "    for i in range(2, len(wcss) - 1):\n",
        "        if (wcss[i-1] - wcss[i]) / (wcss[i] - wcss[i+1]) < 0.5:\n",
        "            optimal_clusters = i + 2\n",
        "            break\n",
        "    print(f\"Optimal number of clusters for Fold {fold}: {optimal_clusters}\")\n",
        "\n",
        "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "    train_df['cluster'] = kmeans.fit_predict(scaler.transform(train_df[features]))\n",
        "    test_df['cluster'] = kmeans.predict(scaler.transform(test_df[features]))\n",
        "    fold_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "    # Cluster statistics for Z-score\n",
        "    stats = train_df.groupby('cluster')['measurement'].agg(['mean','std']).rename(columns={'mean':'mu','std':'sigma'})\n",
        "    fold_df['cluster_mu'] = fold_df['cluster'].map(stats['mu'])\n",
        "    fold_df['cluster_sigma'] = fold_df['cluster'].map(stats['sigma']).fillna(global_pipe_std)\n",
        "\n",
        "    # Z-scores and traditional scores\n",
        "    fold_df['Z_score'] = (fold_df['measurement'] - fold_df['cluster_mu']) / fold_df['cluster_sigma']\n",
        "    fold_df['traditional_Z_score'] = (fold_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "\n",
        "    # Plausibility and anomaly labels\n",
        "    threshold = 3\n",
        "    fold_df['outside_plausible'] = (\n",
        "        (fold_df['measurement'] < fold_df['min_plausible']) |\n",
        "        (fold_df['measurement'] > fold_df['max_plausible'])\n",
        "    )\n",
        "    fold_df['is_anomaly'] = fold_df['Z_score'].abs() > threshold\n",
        "    fold_df['is_anomaly_traditional'] = fold_df['traditional_Z_score'].abs() > threshold\n",
        "    fold_df['is_noise'] = fold_df['is_anomaly'] & fold_df['outside_plausible']\n",
        "    fold_df['is_noise_traditional'] = fold_df['is_anomaly_traditional'] & fold_df['outside_plausible']\n",
        "\n",
        "    # Function to introduce Generalized Normal Noise (GNN)\n",
        "    def introduce_gnn_noise(df, beta=1.5, scale=global_flow_std, noise_fraction=0.05):\n",
        "        noisy = df.copy()\n",
        "        mask = np.random.rand(len(noisy)) < noise_fraction\n",
        "        noise = gennorm.rvs(beta, loc=0, scale=scale, size=len(noisy))\n",
        "        noisy['measurement'] += noise * mask\n",
        "        noisy['is_true_noise'] = mask.astype(int)\n",
        "        return noisy\n",
        "\n",
        "    # Evaluate across noise fractions\n",
        "    noise_fractions = [0.02, 0.05, 0.1, 0.2]\n",
        "    for frac in noise_fractions:\n",
        "        print(f\"GNN noise fraction: {frac*100}% in Fold {fold}\")\n",
        "        df_noisy = introduce_gnn_noise(fold_df, beta=1.5, scale=global_flow_std, noise_fraction=frac)\n",
        "        df_noisy['Z_score'] = (df_noisy['measurement'] - df_noisy['cluster_mu']) / df_noisy['cluster_sigma']\n",
        "        df_noisy['is_anomaly'] = df_noisy['Z_score'].abs() > threshold\n",
        "        df_noisy['outside_plausible'] = (\n",
        "            (df_noisy['measurement'] < df_noisy['min_plausible']) |\n",
        "            (df_noisy['measurement'] > df_noisy['max_plausible'])\n",
        "        )\n",
        "        df_noisy['is_noise'] = df_noisy['is_anomaly'] & df_noisy['outside_plausible']\n",
        "\n",
        "        df_noisy['traditional_Z_score'] = (df_noisy['measurement'] - global_flow_mean) / global_flow_std\n",
        "        df_noisy['is_anomaly_traditional'] = df_noisy['traditional_Z_score'].abs() > threshold\n",
        "        df_noisy['is_noise_traditional'] = df_noisy['is_anomaly_traditional'] & df_noisy['outside_plausible']\n",
        "\n",
        "        y_true = df_noisy['is_true_noise']\n",
        "        for method, label in [('Hybrid', 'is_noise'), ('Traditional', 'is_noise_traditional')]:\n",
        "            y_pred = df_noisy[label].astype(int)\n",
        "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "            rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "            detected = y_pred[y_true==1].sum()\n",
        "            total = y_true.sum()\n",
        "            perc = (detected/total*100) if total>0 else 0\n",
        "            results.append({\n",
        "                'Fold': fold,\n",
        "                'Noise (%)': frac*100,\n",
        "                'Method': method,\n",
        "                'Precision': prec,\n",
        "                'Recall': rec,\n",
        "                'F1': f1,\n",
        "                'Detected (%)': perc\n",
        "            })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nResults:\\n\", results_df.groupby(['Noise (%)', 'Method']).agg({\n",
        "    'Precision': 'mean',\n",
        "    'Recall': 'mean',\n",
        "    'F1': 'mean',\n",
        "    'Detected (%)': 'mean'\n",
        "}).reset_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRnaNNEjSWlg",
        "outputId": "aa157462-52aa-4c81-9ad4-950eac796557"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data size: 5956800 rows.\n",
            "Global flow stats: Mean = 829.88, Std = 1495.64\n",
            "Processing Fold 1\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 1: 7\n",
            "GNN noise fraction: 2.0% in Fold 1\n",
            "GNN noise fraction: 5.0% in Fold 1\n",
            "GNN noise fraction: 10.0% in Fold 1\n",
            "GNN noise fraction: 20.0% in Fold 1\n",
            "Processing Fold 2\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 2: 7\n",
            "GNN noise fraction: 2.0% in Fold 2\n",
            "GNN noise fraction: 5.0% in Fold 2\n",
            "GNN noise fraction: 10.0% in Fold 2\n",
            "GNN noise fraction: 20.0% in Fold 2\n",
            "Processing Fold 3\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 3: 7\n",
            "GNN noise fraction: 2.0% in Fold 3\n",
            "GNN noise fraction: 5.0% in Fold 3\n",
            "GNN noise fraction: 10.0% in Fold 3\n",
            "GNN noise fraction: 20.0% in Fold 3\n",
            "Processing Fold 4\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 4: 7\n",
            "GNN noise fraction: 2.0% in Fold 4\n",
            "GNN noise fraction: 5.0% in Fold 4\n",
            "GNN noise fraction: 10.0% in Fold 4\n",
            "GNN noise fraction: 20.0% in Fold 4\n",
            "Processing Fold 5\n",
            "Training set size: 4765440, Test set size: 1191360\n",
            "Optimal number of clusters for Fold 5: 9\n",
            "GNN noise fraction: 2.0% in Fold 5\n",
            "GNN noise fraction: 5.0% in Fold 5\n",
            "GNN noise fraction: 10.0% in Fold 5\n",
            "GNN noise fraction: 20.0% in Fold 5\n",
            "\n",
            "Results:\n",
            "    Noise (%)       Method  Precision    Recall        F1  Detected (%)\n",
            "0        2.0       Hybrid   0.800249  0.303160  0.438057     30.315989\n",
            "1        2.0  Traditional   0.828325  0.006705  0.013303      0.670534\n",
            "2        5.0       Hybrid   0.910668  0.302943  0.453750     30.294340\n",
            "3        5.0  Traditional   0.926173  0.006709  0.013322      0.670939\n",
            "4       10.0       Hybrid   0.955189  0.302351  0.458629     30.235136\n",
            "5       10.0  Traditional   0.962391  0.006645  0.013199      0.664518\n",
            "6       20.0       Hybrid   0.979578  0.302795  0.462004     30.279464\n",
            "7       20.0  Traditional   0.983508  0.006730  0.013368      0.672980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8fVM7xh4yiT",
        "outputId": "49cff045-5628-4630-9192-32112eb8700f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data size: 5956800 rows.\n",
            "Global flow stats: Mean = 829.88, Std = 1495.64\n",
            "Per-pipe plausible ranges (fixed k):\n",
            "     pipe_id  actual_mean   actual_std  min_plausible  max_plausible\n",
            "0    Pipe_1  5627.037288  1718.679788     470.997924   10783.076651\n",
            "1   Pipe_10   550.314267   195.561336     -36.369742    1136.998276\n",
            "2   Pipe_11   412.248822   162.662078     -75.737413     900.235057\n",
            "3   Pipe_12   253.255767    83.374362       3.132682     503.378853\n",
            "4   Pipe_13   266.276404    99.849458     -33.271971     565.824779\n",
            "5   Pipe_14    88.363952    60.337333     -92.648046     269.375950\n",
            "6   Pipe_15    13.391692    56.662783    -156.596656     183.380040\n",
            "7   Pipe_16   156.229705    78.588859     -79.536872     391.996283\n",
            "8   Pipe_17  -405.530240   134.493077    -809.009471      -2.051009\n",
            "9   Pipe_18  -777.124479   242.406106   -1504.342797     -49.906162\n",
            "10  Pipe_19  -794.791479   247.168817   -1536.297930     -53.285029\n",
            "11   Pipe_2  5386.296760  1649.485821     437.839298   10334.754222\n",
            "12  Pipe_20  2207.171610   750.726857     -45.008962    4459.352181\n",
            "13  Pipe_21   408.035774   252.644128    -349.896611    1165.968159\n",
            "14  Pipe_22   128.769452    41.299230       4.871761     252.667143\n",
            "15  Pipe_23  1447.923884   520.511223    -113.609784    3009.457552\n",
            "16  Pipe_24   896.787986   276.812724      66.349816    1727.226157\n",
            "17  Pipe_25   656.141671   205.627531      39.259077    1273.024265\n",
            "18  Pipe_26  -273.569630   102.998892    -582.566305      35.427044\n",
            "19  Pipe_27   -20.996363    60.255449    -201.762709     159.769983\n",
            "20  Pipe_28    81.056301    67.437810    -121.257130     283.369732\n",
            "21  Pipe_29   258.814212   258.231663    -515.880777    1033.509201\n",
            "22   Pipe_3  2150.557541   648.409214     205.329901    4095.785182\n",
            "23  Pipe_30   107.857110   102.299318    -199.040846     414.755065\n",
            "24  Pipe_31     9.254527    96.846744    -281.285704     299.794759\n",
            "25  Pipe_32   -90.028870   100.600031    -391.828963     211.771223\n",
            "26  Pipe_33   119.069342   103.423985    -191.202612     429.341297\n",
            "27  Pipe_34   336.010973   139.678505     -83.024543     755.046488\n",
            "28   Pipe_4  2115.158836   637.697454     202.066475    4028.251197\n",
            "29   Pipe_5  1907.977644   579.325887     169.999984    3645.955304\n",
            "30   Pipe_6  1632.638527   497.608352     139.813471    3125.463583\n",
            "31   Pipe_7  1274.619699   395.602830      87.811209    2461.428188\n",
            "32   Pipe_8  1118.207281   349.674631      69.183389    2167.231172\n",
            "33   Pipe_9   968.378753   308.317622      43.425887    1893.331620\n",
            "Subsampled data size: 1191360\n",
            "Processing Fold 1\n",
            "Training set size: 953088, Test set size: 238272\n",
            "K-distance plot saved to '/content/drive/My Drive/k_distance_plot_flow_fold_1.png'.\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 1: 0.003\n",
            "Best parameters for Fold 1: eps=0.011, min_samples=20, Best Silhouette Score=0.610\n",
            "\n",
            "Number of noise points (cluster -1) in training set (Fold 1): 3308 (0.35%)\n",
            "\n",
            "Cluster Statistics (Training Set, Non-Noise Clusters, Fold 1):\n",
            "    cluster  cluster_mu  cluster_sigma  point_count      pipes\n",
            "0         0     1392.74         429.98        27026  [Pipe_23]\n",
            "1         1     5333.44        1578.68        27402   [Pipe_2]\n",
            "2         2     -793.27         244.98        27797  [Pipe_19]\n",
            "3         3     1110.55         335.48        27742   [Pipe_8]\n",
            "4         4      253.25          82.75        27828  [Pipe_12]\n",
            "5         5      335.11         137.82        28210  [Pipe_34]\n",
            "6         6     2106.41         623.78        28055   [Pipe_4]\n",
            "7         7      -69.84          31.18        26917  [Pipe_32]\n",
            "8         8      893.41         276.74        27846  [Pipe_24]\n",
            "9         9     -776.18         243.01        27938  [Pipe_18]\n",
            "10       10      127.20          42.95        27069  [Pipe_30]\n",
            "11       11      408.52         128.75        28051  [Pipe_11]\n",
            "12       12      206.31          64.26        26564  [Pipe_29]\n",
            "13       13       15.36          46.78        27878  [Pipe_15]\n",
            "14       14      265.11          90.12        27929  [Pipe_13]\n",
            "15       15      961.13         294.39        27754   [Pipe_9]\n",
            "16       16       90.62          51.25        28133  [Pipe_14]\n",
            "17       17     -404.95         133.69        28048  [Pipe_17]\n",
            "18       18       81.46          65.92        27892  [Pipe_28]\n",
            "19       19      156.87          76.96        27807  [Pipe_16]\n",
            "20       20      655.07         204.68        28043  [Pipe_25]\n",
            "21       21       98.93          38.54        26636  [Pipe_33]\n",
            "22       22     5567.76        1649.39        27183   [Pipe_1]\n",
            "23       23      546.64         168.71        28062  [Pipe_10]\n",
            "24       24     1623.59         484.72        27950   [Pipe_6]\n",
            "25       25      379.51         117.62        27701  [Pipe_21]\n",
            "26       26     1269.32         383.86        27983   [Pipe_7]\n",
            "27       27       29.01          24.62        27053  [Pipe_31]\n",
            "28       28      -30.04          38.03        26717  [Pipe_27]\n",
            "29       29     2188.58         730.61        27958  [Pipe_20]\n",
            "30       30     1898.37         562.57        27809   [Pipe_5]\n",
            "31       31      129.30          41.16        28304  [Pipe_22]\n",
            "32       32     2145.34         637.69        28051   [Pipe_3]\n",
            "33       33     -273.28         101.80        28099  [Pipe_26]\n",
            "34       34      576.32          22.31         1247  [Pipe_33]\n",
            "35       35     -436.85          21.83         1189  [Pipe_31]\n",
            "36       36     -333.12          55.68         1198  [Pipe_30]\n",
            "37       37     3073.82         126.62          699  [Pipe_23]\n",
            "38       38     1448.25          20.46         1176  [Pipe_29]\n",
            "39       39      193.48          12.32         1203  [Pipe_27]\n",
            "40       40     -545.66          12.90         1205  [Pipe_32]\n",
            "41       41     2133.93          15.02          428  [Pipe_21]\n",
            "Processing noise fraction: 2.0% in Fold 1\n",
            "Processing noise fraction: 5.0% in Fold 1\n",
            "Processing noise fraction: 10.0% in Fold 1\n",
            "Processing noise fraction: 20.0% in Fold 1\n",
            "Processing Fold 2\n",
            "Training set size: 953088, Test set size: 238272\n",
            "K-distance plot saved to '/content/drive/My Drive/k_distance_plot_flow_fold_2.png'.\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 2: 0.003\n",
            "Best parameters for Fold 2: eps=0.011, min_samples=20, Best Silhouette Score=0.607\n",
            "\n",
            "Number of noise points (cluster -1) in training set (Fold 2): 3546 (0.37%)\n",
            "\n",
            "Cluster Statistics (Training Set, Non-Noise Clusters, Fold 2):\n",
            "    cluster  cluster_mu  cluster_sigma  point_count      pipes\n",
            "0         0       90.55          51.18        28060  [Pipe_14]\n",
            "1         1     1113.16         333.93        27837   [Pipe_8]\n",
            "2         2     2187.74         726.93        27847  [Pipe_20]\n",
            "3         3      253.20          82.84        27969  [Pipe_12]\n",
            "4         4     -793.33         245.51        27759  [Pipe_19]\n",
            "5         5      335.30         138.47        28339  [Pipe_34]\n",
            "6         6      -70.01          31.24        27022  [Pipe_32]\n",
            "7         7       81.01          65.43        28079  [Pipe_28]\n",
            "8         8     -775.29         242.50        27928  [Pipe_18]\n",
            "9         9       29.04          24.55        27041  [Pipe_31]\n",
            "10       10      893.91         275.79        27949  [Pipe_24]\n",
            "11       11      127.01          43.03        26767  [Pipe_30]\n",
            "12       12     2103.66         626.10        28101   [Pipe_4]\n",
            "13       13      206.28          64.51        26672  [Pipe_29]\n",
            "14       14       15.14          46.83        27881  [Pipe_15]\n",
            "15       15      265.50          90.14        27854  [Pipe_13]\n",
            "16       16      961.76         293.54        27547   [Pipe_9]\n",
            "17       17     1393.99         429.15        27110  [Pipe_23]\n",
            "18       18      655.71         203.72        27922  [Pipe_25]\n",
            "19       19     -405.02         133.81        27943  [Pipe_17]\n",
            "20       20      157.11          77.39        27849  [Pipe_16]\n",
            "21       21       98.86          38.44        26597  [Pipe_33]\n",
            "22       22      547.73         168.14        27945  [Pipe_10]\n",
            "23       23     1622.22         488.80        28001   [Pipe_6]\n",
            "24       24     5580.96        1645.92        27269   [Pipe_1]\n",
            "25       25      377.82         118.34        27629  [Pipe_21]\n",
            "26       26     1268.56         382.64        27902   [Pipe_7]\n",
            "27       27      -29.82          37.95        26698  [Pipe_27]\n",
            "28       28     1899.70         566.01        27935   [Pipe_5]\n",
            "29       29      129.36          41.03        28338  [Pipe_22]\n",
            "30       30     2143.02         639.32        28115   [Pipe_3]\n",
            "31       31     5324.70        1573.01        27298   [Pipe_2]\n",
            "32       32     -273.72         102.23        28042  [Pipe_26]\n",
            "33       33      408.95         128.74        28122  [Pipe_11]\n",
            "34       34      576.26          22.33         1229  [Pipe_33]\n",
            "35       35     -436.49          21.93         1172  [Pipe_31]\n",
            "36       36     -332.82          54.96         1158  [Pipe_30]\n",
            "37       37     3097.81         108.29          610  [Pipe_23]\n",
            "38       38     1448.32          20.48         1166  [Pipe_29]\n",
            "39       40      193.70          12.34         1186  [Pipe_27]\n",
            "40       42     -545.58          12.90         1199  [Pipe_32]\n",
            "41       43     2133.80          14.82          455  [Pipe_21]\n",
            "Processing noise fraction: 2.0% in Fold 2\n",
            "Processing noise fraction: 5.0% in Fold 2\n",
            "Processing noise fraction: 10.0% in Fold 2\n",
            "Processing noise fraction: 20.0% in Fold 2\n",
            "Processing Fold 3\n",
            "Training set size: 953088, Test set size: 238272\n",
            "K-distance plot saved to '/content/drive/My Drive/k_distance_plot_flow_fold_3.png'.\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 3: 0.003\n",
            "Best parameters for Fold 3: eps=0.013, min_samples=30, Best Silhouette Score=0.604\n",
            "\n",
            "Number of noise points (cluster -1) in training set (Fold 3): 3591 (0.38%)\n",
            "\n",
            "Cluster Statistics (Training Set, Non-Noise Clusters, Fold 3):\n",
            "    cluster  cluster_mu  cluster_sigma  point_count      pipes\n",
            "0         0     1389.83         427.27        26892  [Pipe_23]\n",
            "1         1     5337.00        1573.36        27461   [Pipe_2]\n",
            "2         2       90.60          51.17        28098  [Pipe_14]\n",
            "3         3     -792.06         244.41        27756  [Pipe_19]\n",
            "4         4     1113.93         334.97        27780   [Pipe_8]\n",
            "5         5     2129.34         654.46        27003  [Pipe_20]\n",
            "6         6      335.57         138.48        28223  [Pipe_34]\n",
            "7         7     2104.73         625.12        28144   [Pipe_4]\n",
            "8         8       81.19          65.77        28101  [Pipe_28]\n",
            "9         9     -775.04         242.29        27936  [Pipe_18]\n",
            "10       10       29.12          24.64        27094  [Pipe_31]\n",
            "11       11      893.42         275.62        27983  [Pipe_24]\n",
            "12       12      127.06          43.12        26850  [Pipe_30]\n",
            "13       13      253.63          82.85        28136  [Pipe_12]\n",
            "14       14      408.35         128.40        27981  [Pipe_11]\n",
            "15       15      206.47          64.38        26755  [Pipe_29]\n",
            "16       16       15.26          46.77        28090  [Pipe_15]\n",
            "17       17      657.12         203.53        28012  [Pipe_25]\n",
            "18       18      265.61          89.95        27868  [Pipe_13]\n",
            "19       19      156.91          77.14        27726  [Pipe_16]\n",
            "20       20     5557.68        1642.02        27134   [Pipe_1]\n",
            "21       21      547.60         167.79        27959  [Pipe_10]\n",
            "22       22     1623.18         486.71        27819   [Pipe_6]\n",
            "23       23      -69.87          31.13        26855  [Pipe_32]\n",
            "24       24      378.63         117.98        27642  [Pipe_21]\n",
            "25       25     1270.43         384.67        27755   [Pipe_7]\n",
            "26       26       98.91          38.37        26549  [Pipe_33]\n",
            "27       27     -405.00         134.21        28043  [Pipe_17]\n",
            "28       28      -29.89          38.01        26827  [Pipe_27]\n",
            "29       29      962.22         294.16        27646   [Pipe_9]\n",
            "30       30      129.29          41.18        28282  [Pipe_22]\n",
            "31       31     2140.32         637.84        27890   [Pipe_3]\n",
            "32       32     -273.77         102.66        28067  [Pipe_26]\n",
            "33       33     1900.25         560.81        27909   [Pipe_5]\n",
            "34       34     -332.97          56.05         1199  [Pipe_30]\n",
            "35       35     3081.16         121.26          684  [Pipe_23]\n",
            "36       36     1448.78          20.45         1159  [Pipe_29]\n",
            "37       37     -436.49          21.57         1161  [Pipe_31]\n",
            "38       38      576.73          22.31         1247  [Pipe_33]\n",
            "39       40      193.56          12.45         1193  [Pipe_27]\n",
            "40       42     -545.99          12.92         1227  [Pipe_32]\n",
            "41       43     4132.53         149.02          747  [Pipe_20]\n",
            "42       44     2133.84          14.66          432  [Pipe_21]\n",
            "43       45     3773.35          43.71          182  [Pipe_20]\n",
            "Processing noise fraction: 2.0% in Fold 3\n",
            "Processing noise fraction: 5.0% in Fold 3\n",
            "Processing noise fraction: 10.0% in Fold 3\n",
            "Processing noise fraction: 20.0% in Fold 3\n",
            "Processing Fold 4\n",
            "Training set size: 953088, Test set size: 238272\n",
            "K-distance plot saved to '/content/drive/My Drive/k_distance_plot_flow_fold_4.png'.\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 4: 0.003\n",
            "Best parameters for Fold 4: eps=0.013, min_samples=20, Best Silhouette Score=0.610\n",
            "\n",
            "Number of noise points (cluster -1) in training set (Fold 4): 2395 (0.25%)\n",
            "\n",
            "Cluster Statistics (Training Set, Non-Noise Clusters, Fold 4):\n",
            "    cluster  cluster_mu  cluster_sigma  point_count      pipes\n",
            "0         0     1396.80         433.95        27172  [Pipe_23]\n",
            "1         1     5354.42        1589.38        27539   [Pipe_2]\n",
            "2         2       90.75          51.30        28000  [Pipe_14]\n",
            "3         3     -792.99         246.98        27658  [Pipe_19]\n",
            "4         4     2189.01         732.00        27880  [Pipe_20]\n",
            "5         5      253.46          82.83        28161  [Pipe_12]\n",
            "6         6      336.08         139.29        28332  [Pipe_34]\n",
            "7         7     2106.59         626.08        27916   [Pipe_4]\n",
            "8         8      -69.90          31.21        26831  [Pipe_32]\n",
            "9         9       80.92          65.61        28160  [Pipe_28]\n",
            "10       10     -776.64         242.46        27809  [Pipe_18]\n",
            "11       11       29.02          24.55        27149  [Pipe_31]\n",
            "12       12      127.24          42.97        26972  [Pipe_30]\n",
            "13       13      409.09         128.78        27929  [Pipe_11]\n",
            "14       14       15.19          46.85        27771  [Pipe_15]\n",
            "15       15      264.92          89.98        27803  [Pipe_13]\n",
            "16       16      962.67         295.16        27690   [Pipe_9]\n",
            "17       17      654.67         204.62        28062  [Pipe_25]\n",
            "18       18      894.71         276.96        27988  [Pipe_24]\n",
            "19       19     -404.55         134.51        27843  [Pipe_17]\n",
            "20       20      156.23          78.68        27842  [Pipe_16]\n",
            "21       21       99.12          38.49        26516  [Pipe_33]\n",
            "22       22     5562.74        1646.65        27331   [Pipe_1]\n",
            "23       23      546.78         167.96        28244  [Pipe_10]\n",
            "24       24     1622.54         489.87        27810   [Pipe_6]\n",
            "25       25      206.47          64.34        26673  [Pipe_29]\n",
            "26       26     1269.21         385.08        27941   [Pipe_7]\n",
            "27       27     1110.55         336.98        27841   [Pipe_8]\n",
            "28       28      379.14         118.33        27732  [Pipe_21]\n",
            "29       29      -29.96          38.04        26807  [Pipe_27]\n",
            "30       30     1905.83         567.03        28097   [Pipe_5]\n",
            "31       31      129.07          41.11        28374  [Pipe_22]\n",
            "32       32     2145.09         640.44        28046   [Pipe_3]\n",
            "33       33      576.17          22.53         1217  [Pipe_33]\n",
            "34       34     -273.36         102.32        28113  [Pipe_26]\n",
            "35       35     -436.49          22.14         1182  [Pipe_31]\n",
            "36       36     3089.17         138.06          741  [Pipe_23]\n",
            "37       37     1448.13          21.01         1177  [Pipe_29]\n",
            "38       38     -332.53          55.67         1199  [Pipe_30]\n",
            "39       40      193.43          12.20         1172  [Pipe_27]\n",
            "40       42     -545.63          12.99         1219  [Pipe_32]\n",
            "41       43     2134.07          15.03          456  [Pipe_21]\n",
            "42       45     9079.12          67.31          179   [Pipe_1]\n",
            "43       46     8811.17          45.02          119   [Pipe_2]\n",
            "Processing noise fraction: 2.0% in Fold 4\n",
            "Processing noise fraction: 5.0% in Fold 4\n",
            "Processing noise fraction: 10.0% in Fold 4\n",
            "Processing noise fraction: 20.0% in Fold 4\n",
            "Processing Fold 5\n",
            "Training set size: 953088, Test set size: 238272\n",
            "K-distance plot saved to '/content/drive/My Drive/k_distance_plot_flow_fold_5.png'.\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 5: 0.003\n",
            "Best parameters for Fold 5: eps=0.011, min_samples=30, Best Silhouette Score=0.612\n",
            "\n",
            "Number of noise points (cluster -1) in training set (Fold 5): 5299 (0.56%)\n",
            "\n",
            "Cluster Statistics (Training Set, Non-Noise Clusters, Fold 5):\n",
            "    cluster  cluster_mu  cluster_sigma  point_count      pipes\n",
            "0         0     1387.60         425.99        26849  [Pipe_23]\n",
            "1         1     5313.80        1549.17        26951   [Pipe_2]\n",
            "2         2       90.62          51.10        28060  [Pipe_14]\n",
            "3         3     -792.63         245.79        27818  [Pipe_19]\n",
            "4         4     1112.03         334.38        27623   [Pipe_8]\n",
            "5         5     2114.21         643.72        26671  [Pipe_20]\n",
            "6         6      253.30          82.92        28052  [Pipe_12]\n",
            "7         7     2105.00         624.14        28042   [Pipe_4]\n",
            "8         8      -69.95          31.26        26815  [Pipe_32]\n",
            "9         9       81.12          66.08        28086  [Pipe_28]\n",
            "10       10     -776.32         241.98        27743  [Pipe_18]\n",
            "11       11       28.88          24.67        27243  [Pipe_31]\n",
            "12       12      892.92         274.71        27768  [Pipe_24]\n",
            "13       13      127.23          42.97        26762  [Pipe_30]\n",
            "14       14      408.34         128.57        27881  [Pipe_11]\n",
            "15       15      205.97          64.23        26679  [Pipe_29]\n",
            "16       16      336.19         139.31        28187  [Pipe_34]\n",
            "17       17      265.07          89.87        27881  [Pipe_13]\n",
            "18       18      961.57         293.68        27805   [Pipe_9]\n",
            "19       19      654.64         203.01        27921  [Pipe_25]\n",
            "20       20     -404.55         133.42        27774  [Pipe_17]\n",
            "21       21      156.47          75.76        27789  [Pipe_16]\n",
            "22       22       98.56          38.12        26605  [Pipe_33]\n",
            "23       23     5555.62        1605.94        26992   [Pipe_1]\n",
            "24       24     1624.16         485.21        28024   [Pipe_6]\n",
            "25       25       15.15          46.77        27856  [Pipe_15]\n",
            "26       26      379.57         117.91        27702  [Pipe_21]\n",
            "27       27     1266.42         382.10        27821   [Pipe_7]\n",
            "28       28      546.58         168.04        28212  [Pipe_10]\n",
            "29       29      -29.93          37.93        26896  [Pipe_27]\n",
            "30       30     1898.32         562.50        27854   [Pipe_5]\n",
            "31       31      129.11          41.01        28334  [Pipe_22]\n",
            "32       32     2137.38         630.54        27944   [Pipe_3]\n",
            "33       33     -272.82         101.72        28152  [Pipe_26]\n",
            "34       34      576.69          22.41         1232  [Pipe_33]\n",
            "35       35     -436.65          21.40         1166  [Pipe_31]\n",
            "36       36     -333.64          55.51         1214  [Pipe_30]\n",
            "37       37     1448.50          20.68         1203  [Pipe_29]\n",
            "38       41      193.69          12.15         1213  [Pipe_27]\n",
            "39       43     -545.69          12.81         1222  [Pipe_32]\n",
            "40       44     2133.53          14.89          429  [Pipe_21]\n",
            "41       46     4215.92          64.98          356  [Pipe_20]\n",
            "42       47     8363.25          30.45          135   [Pipe_2]\n",
            "43       49     4017.63          45.08          232  [Pipe_20]\n",
            "44       50     3102.32         100.78          595  [Pipe_23]\n",
            "Processing noise fraction: 2.0% in Fold 5\n",
            "Processing noise fraction: 5.0% in Fold 5\n",
            "Processing noise fraction: 10.0% in Fold 5\n",
            "Processing noise fraction: 20.0% in Fold 5\n",
            "\n",
            "Consolidated Evaluation Results:\n",
            "   Noise Fraction (%)                  Method  Precision    Recall  F1 Score  \\\n",
            "0                 2.0  Hybrid (Cluster-based)   0.833699  0.599431  0.697322   \n",
            "1                 2.0    Traditional (Global)   0.876269  0.009402  0.018604   \n",
            "2                 5.0  Hybrid (Cluster-based)   0.927960  0.597231  0.726708   \n",
            "3                 5.0    Traditional (Global)   0.952331  0.009983  0.019759   \n",
            "4                10.0  Hybrid (Cluster-based)   0.964531  0.598585  0.738714   \n",
            "5                10.0    Traditional (Global)   0.977189  0.010346  0.020475   \n",
            "6                20.0  Hybrid (Cluster-based)   0.983792  0.597501  0.743461   \n",
            "7                20.0    Traditional (Global)   0.989249  0.010030  0.019859   \n",
            "\n",
            "   Noise Detected (%)  \n",
            "0           59.943102  \n",
            "1            0.940222  \n",
            "2           59.723102  \n",
            "3            0.998325  \n",
            "4           59.858493  \n",
            "5            1.034581  \n",
            "6           59.750130  \n",
            "7            1.003005  \n",
            "Results saved to '/content/drive/My Drive/'.\n",
            "\n",
            "Z-score stats (noisy dataset, last noise level of last fold):\n",
            "             Z_score  traditional_Z_score\n",
            "count  1.189771e+06         1.191360e+06\n",
            "mean   7.982442e-01        -1.105610e-04\n",
            "std    1.215057e+01         1.095725e+00\n",
            "min   -2.163638e+02        -5.604120e+00\n",
            "25%   -1.131990e+00        -5.324401e-01\n",
            "50%    7.299366e-02        -3.406421e-01\n",
            "75%    1.028165e+00         3.381326e-01\n",
            "max    2.544406e+02         8.751785e+00\n",
            "Noise detection summary (hybrid):\n",
            " is_noise\n",
            "False    1046404\n",
            "True      144956\n",
            "Name: count, dtype: int64\n",
            "Noise detection summary (traditional):\n",
            " is_noise_traditional\n",
            "False    1188880\n",
            "True        2480\n",
            "Name: count, dtype: int64\n",
            "Total noise points introduced (last noise level of last fold): 238663\n",
            "Hybrid method detected (last noise level of last fold): 142908 (59.88%)\n",
            "Traditional method detected (last noise level of last fold): 2454 (1.03%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming combined_df and pipe_df are already defined from previous steps\n",
        "merged_df = pd.merge(combined_df, pipe_df, on='pipe_id', how='inner')\n",
        "print(f\"Merged data size: {len(merged_df)} rows.\")\n",
        "\n",
        "# Compute global flow stats\n",
        "global_flow_mean = merged_df['measurement'].mean()\n",
        "global_flow_std = merged_df['measurement'].std()\n",
        "print(f\"Global flow stats: Mean = {global_flow_mean:.2f}, Std = {global_flow_std:.2f}\")\n",
        "\n",
        "# Compute per-pipe statistics\n",
        "pipe_stats_df = merged_df.groupby('pipe_id')['measurement'].agg(['mean', 'std', 'count']).reset_index()\n",
        "pipe_stats_df.columns = ['pipe_id', 'actual_mean', 'actual_std', 'sample_count']\n",
        "pipe_stats_df = pipe_stats_df[pipe_stats_df['sample_count'] >= 100]\n",
        "\n",
        "# Compute global pipe std for minimum threshold\n",
        "global_pipe_std = pipe_stats_df['actual_std'].mean()\n",
        "\n",
        "# Apply a minimum std to avoid overly narrow ranges (10% of global pipe std)\n",
        "pipe_stats_df['actual_std'] = pipe_stats_df['actual_std'].apply(lambda x: max(x, global_pipe_std * 0.1))\n",
        "\n",
        "# Compute plausible ranges using pipe-level stats with fixed k\n",
        "k = 3  # Fixed number of standard deviations for plausible range\n",
        "pipe_stats_df['min_plausible'] = pipe_stats_df['actual_mean'] - k * pipe_stats_df['actual_std']\n",
        "pipe_stats_df['max_plausible'] = pipe_stats_df['actual_mean'] + k * pipe_stats_df['actual_std']\n",
        "print(\"Per-pipe plausible ranges (fixed k):\\n\", pipe_stats_df[['pipe_id', 'actual_mean', 'actual_std', 'min_plausible', 'max_plausible']])\n",
        "\n",
        "# Merge the plausible ranges into the main DataFrame\n",
        "merged_df = merged_df.merge(pipe_stats_df[['pipe_id', 'min_plausible', 'max_plausible']], on='pipe_id', how='left')\n",
        "\n",
        "# Subsample the data to reduce memory usage (scale up to 50% after tuning)\n",
        "subsample_fraction = 0.2  # Use 20% of the data\n",
        "subsampled_df = merged_df.sample(frac=subsample_fraction, random_state=42)\n",
        "print(f\"Subsampled data size: {len(subsampled_df)}\")\n",
        "\n",
        "# Initialize 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results_list = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(subsampled_df), 1):\n",
        "    print(f\"Processing Fold {fold}\")\n",
        "    train_df = subsampled_df.iloc[train_index].copy()\n",
        "    test_df = subsampled_df.iloc[test_index].copy()\n",
        "    print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "    # Prepare data for clustering on training set\n",
        "    features = ['measurement', 'diameter', 'length', 'roughness']\n",
        "    X_cluster_train = train_df[features]\n",
        "    scaler = RobustScaler()\n",
        "    X_scaled_train = scaler.fit_transform(X_cluster_train)\n",
        "\n",
        "    # Prepare data for test set\n",
        "    X_cluster_test = test_df[features]\n",
        "    X_scaled_test = scaler.transform(X_cluster_test)\n",
        "\n",
        "    # Generate k-distance plot to determine initial eps\n",
        "    min_samples = 50  # Default min_samples for k-distance plot\n",
        "    neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_scaled_train)\n",
        "    distances, indices = neighbors.kneighbors(X_scaled_train)\n",
        "    k_distances = np.sort(distances[:, min_samples - 1])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_distances)\n",
        "    plt.title(f'K-Distance Plot (Distance to {min_samples}th Nearest Neighbor) - Fold {fold}')\n",
        "    plt.xlabel('Points Sorted by Distance')\n",
        "    plt.ylabel('Distance')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'/content/drive/My Drive/k_distance_plot_flow_fold_{fold}.png')\n",
        "    plt.close()\n",
        "    print(f\"K-distance plot saved to '/content/drive/My Drive/k_distance_plot_flow_fold_{fold}.png'.\")\n",
        "\n",
        "    # Suggest eps based on the 95th percentile of k-distances\n",
        "    suggested_eps = np.percentile(k_distances, 95)\n",
        "    print(f\"Suggested eps based on 95th percentile of k-distances for Fold {fold}: {suggested_eps:.3f}\")\n",
        "\n",
        "    # Parameter tuning using silhouette score\n",
        "    eps_values = np.arange(0.005, 0.015, 0.002)  # Adjusted range based on suggested eps\n",
        "    min_samples_values = [20, 30, 50, 60, 70]\n",
        "    best_score = -1\n",
        "    best_params = {'eps': 0, 'min_samples': 0}\n",
        "\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = dbscan.fit_predict(X_scaled_train)\n",
        "            if len(np.unique(labels)) > 1 and -1 in labels:  # Ensure multiple clusters and noise points\n",
        "                score = silhouette_score(X_scaled_train, labels, sample_size=5000)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "    print(f\"Best parameters for Fold {fold}: eps={best_params['eps']:.3f}, min_samples={best_params['min_samples']}, Best Silhouette Score={best_score:.3f}\")\n",
        "\n",
        "    # Use best parameters for clustering\n",
        "    dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "    train_cluster_labels = dbscan.fit_predict(X_scaled_train)\n",
        "    test_cluster_labels = dbscan.fit_predict(X_scaled_test)\n",
        "\n",
        "    # Add cluster labels to train and test dataframes\n",
        "    train_df['cluster'] = train_cluster_labels\n",
        "    test_df['cluster'] = test_cluster_labels\n",
        "    fold_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "    # Compute cluster statistics (mu, sigma, count, and associated pipes) for non-noise clusters\n",
        "    cluster_stats = train_df[train_df['cluster'] != -1].groupby('cluster').agg({\n",
        "        'measurement': ['mean', 'std', 'count'],\n",
        "        'pipe_id': lambda x: list(x.unique())  # Get unique pipes in each cluster\n",
        "    }).reset_index()\n",
        "    cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']\n",
        "    cluster_stats['cluster_mu'] = cluster_stats['cluster_mu'].round(2)\n",
        "    cluster_stats['cluster_sigma'] = cluster_stats['cluster_sigma'].round(2)\n",
        "\n",
        "    # Merge small clusters (point_count < 100) into noise (-1)\n",
        "    min_cluster_size = 100\n",
        "    small_clusters = cluster_stats[cluster_stats['point_count'] < min_cluster_size]['cluster'].tolist()\n",
        "    fold_df.loc[fold_df['cluster'].isin(small_clusters), 'cluster'] = -1\n",
        "    train_df.loc[train_df['cluster'].isin(small_clusters), 'cluster'] = -1\n",
        "    test_df.loc[test_df['cluster'].isin(small_clusters), 'cluster'] = -1\n",
        "\n",
        "    # Recompute cluster statistics after merging small clusters\n",
        "    cluster_stats = train_df[train_df['cluster'] != -1].groupby('cluster').agg({\n",
        "        'measurement': ['mean', 'std', 'count'],\n",
        "        'pipe_id': lambda x: list(x.unique())\n",
        "    }).reset_index()\n",
        "    cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']\n",
        "    cluster_stats['cluster_mu'] = cluster_stats['cluster_mu'].round(2)\n",
        "    cluster_stats['cluster_sigma'] = cluster_stats['cluster_sigma'].round(2)\n",
        "\n",
        "    # Report noise points\n",
        "    noise_count = len(train_df[train_df['cluster'] == -1])\n",
        "    print(f\"\\nNumber of noise points (cluster -1) in training set (Fold {fold}): {noise_count} ({noise_count/len(train_df)*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nCluster Statistics (Training Set, Non-Noise Clusters, Fold {fold}):\")\n",
        "    print(cluster_stats[['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']])\n",
        "\n",
        "    # Assign cluster mean and std for Z-score calculation\n",
        "    cluster_means = train_df[train_df['cluster'] != -1].groupby('cluster')['measurement'].mean().to_dict()\n",
        "    cluster_stds = train_df[train_df['cluster'] != -1].groupby('cluster')['measurement'].std().to_dict()\n",
        "    # Apply minimum cluster_sigma to avoid extreme Z-scores\n",
        "    min_sigma = global_pipe_std * 0.1  # 10% of global pipe std\n",
        "    cluster_stds = {k: max(v, min_sigma) for k, v in cluster_stds.items()}\n",
        "    fold_df['cluster_mu'] = fold_df['cluster'].map(cluster_means)\n",
        "    fold_df['cluster_sigma'] = fold_df['cluster'].map(cluster_stds).fillna(global_pipe_std)\n",
        "    # For noise points (cluster -1), use global statistics\n",
        "    fold_df.loc[fold_df['cluster'] == -1, 'cluster_mu'] = global_flow_mean\n",
        "    fold_df.loc[fold_df['cluster'] == -1, 'cluster_sigma'] = global_flow_std\n",
        "\n",
        "    fold_df['Z_score'] = (fold_df['measurement'] - fold_df['cluster_mu']) / fold_df['cluster_sigma'].replace(0, np.nan)\n",
        "    fold_df['traditional_Z_score'] = (fold_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "\n",
        "    threshold = 3\n",
        "    fold_df['outside_plausible'] = (fold_df['measurement'] < fold_df['min_plausible']) | (fold_df['measurement'] > fold_df['max_plausible'])\n",
        "    fold_df['is_anomaly'] = np.abs(fold_df['Z_score']) > threshold\n",
        "    fold_df['is_anomaly_traditional'] = np.abs(fold_df['traditional_Z_score']) > threshold\n",
        "    fold_df['is_noise'] = fold_df['is_anomaly'] & fold_df['outside_plausible']\n",
        "    fold_df['is_noise_traditional'] = fold_df['is_anomaly_traditional'] & fold_df['outside_plausible']\n",
        "\n",
        "    numerical_cols = fold_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    fold_df[numerical_cols] = fold_df[numerical_cols].round(2)\n",
        "\n",
        "    def introduce_gaussian_noise(df, noise_level=global_flow_std, noise_fraction=0.05):\n",
        "        noisy_df = df.copy()\n",
        "        mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "        noise = np.random.normal(0, noise_level, size=len(noisy_df))\n",
        "        noisy_df['measurement'] = noisy_df['measurement'] + noise * mask\n",
        "        noisy_df['is_true_noise'] = mask.astype(int)\n",
        "        return noisy_df\n",
        "\n",
        "    # Evaluate across noise fractions\n",
        "    noise_fractions = [0.02, 0.05, 0.10, 0.20]\n",
        "    for noise_fraction in noise_fractions:\n",
        "        print(f\"Processing noise fraction: {noise_fraction*100}% in Fold {fold}\")\n",
        "        noisy_df = introduce_gaussian_noise(fold_df, noise_level=global_flow_std, noise_fraction=noise_fraction)\n",
        "\n",
        "        noisy_df['Z_score'] = (noisy_df['measurement'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_df['traditional_Z_score'] = (noisy_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "        noisy_df['outside_plausible'] = (noisy_df['measurement'] < noisy_df['min_plausible']) | (noisy_df['measurement'] > noisy_df['max_plausible'])\n",
        "        noisy_df['is_anomaly'] = np.abs(noisy_df['Z_score']) > threshold\n",
        "        noisy_df['is_anomaly_traditional'] = np.abs(noisy_df['traditional_Z_score']) > threshold\n",
        "        noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "        noisy_df['is_noise_traditional'] = noisy_df['is_anomaly_traditional'] & noisy_df['outside_plausible']\n",
        "\n",
        "        y_true = noisy_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "        y_pred_traditional = noisy_df['is_noise_traditional'].astype(int)\n",
        "\n",
        "        precision_hybrid = precision_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        recall_hybrid = recall_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        precision_traditional = precision_score(y_true, y_pred_traditional, zero_division=0)\n",
        "        recall_traditional = recall_score(y_true, y_pred_traditional, zero_division=0)\n",
        "        f1_traditional = f1_score(y_true, y_pred_traditional, zero_division=0)\n",
        "\n",
        "        total_noise_points = y_true.sum()\n",
        "        hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "        traditional_detected = y_pred_traditional[y_true == 1].sum()\n",
        "        hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "        traditional_noise_percentage = (traditional_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Hybrid (Cluster-based)',\n",
        "            'Precision': precision_hybrid,\n",
        "            'Recall': recall_hybrid,\n",
        "            'F1 Score': f1_hybrid,\n",
        "            'Noise Detected (%)': hybrid_noise_percentage\n",
        "        })\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Traditional (Global)',\n",
        "            'Precision': precision_traditional,\n",
        "            'Recall': recall_traditional,\n",
        "            'F1 Score': f1_traditional,\n",
        "            'Noise Detected (%)': traditional_noise_percentage\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(\"\\nConsolidated Evaluation Results:\")\n",
        "print(results_df.groupby(['Noise Fraction (%)', 'Method']).agg({\n",
        "    'Precision': 'mean',\n",
        "    'Recall': 'mean',\n",
        "    'F1 Score': 'mean',\n",
        "    'Noise Detected (%)': 'mean'\n",
        "}).reset_index())\n",
        "\n",
        "try:\n",
        "    noisy_df.to_csv(f'/content/drive/My Drive/noisy_results_dbscan_flow_fold_{fold}.csv', index=False)\n",
        "    results_df.to_csv('/content/drive/My Drive/evaluation_results_dbscan_flow.csv', index=False)\n",
        "    print(\"Results saved to '/content/drive/My Drive/'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")\n",
        "\n",
        "print(\"\\nZ-score stats (noisy dataset, last noise level of last fold):\\n\", noisy_df[['Z_score', 'traditional_Z_score']].describe())\n",
        "print(\"Noise detection summary (hybrid):\\n\", noisy_df['is_noise'].value_counts())\n",
        "print(\"Noise detection summary (traditional):\\n\", noisy_df['is_noise_traditional'].value_counts())\n",
        "print(f\"Total noise points introduced (last noise level of last fold): {total_noise_points}\")\n",
        "print(f\"Hybrid method detected (last noise level of last fold): {hybrid_detected} ({hybrid_noise_percentage:.2f}%)\")\n",
        "print(f\"Traditional method detected (last noise level of last fold): {traditional_detected} ({traditional_noise_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import warnings\n",
        "from scipy.stats import gennorm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming combined_df and pipe_df are already defined\n",
        "merged_df = pd.merge(combined_df, pipe_df, on='pipe_id', how='inner')\n",
        "print(f\"Merged data size: {len(merged_df)} rows.\")\n",
        "\n",
        "# Compute global flow stats\n",
        "global_flow_mean = merged_df['measurement'].mean()\n",
        "global_flow_std = merged_df['measurement'].std()\n",
        "print(f\"Global flow stats: Mean = {global_flow_mean:.2f}, Std = {global_flow_std:.2f}\")\n",
        "\n",
        "# Compute per-pipe statistics\n",
        "pipe_stats_df = merged_df.groupby('pipe_id')['measurement'].agg(['mean', 'std', 'count']).reset_index()\n",
        "pipe_stats_df.columns = ['pipe_id', 'actual_mean', 'actual_std', 'sample_count']\n",
        "pipe_stats_df = pipe_stats_df[pipe_stats_df['sample_count'] >= 100]\n",
        "\n",
        "global_pipe_std = pipe_stats_df['actual_std'].mean()\n",
        "pipe_stats_df['actual_std'] = pipe_stats_df['actual_std'].apply(lambda x: max(x, global_pipe_std * 0.1))\n",
        "\n",
        "# Compute plausible ranges\n",
        "k = 3\n",
        "pipe_stats_df['min_plausible'] = pipe_stats_df['actual_mean'] - k * pipe_stats_df['actual_std']\n",
        "pipe_stats_df['max_plausible'] = pipe_stats_df['actual_mean'] + k * pipe_stats_df['actual_std']\n",
        "merged_df = merged_df.merge(pipe_stats_df[['pipe_id', 'min_plausible', 'max_plausible']], on='pipe_id', how='left')\n",
        "\n",
        "# Subsample\n",
        "subsample_fraction = 0.2\n",
        "subsampled_df = merged_df.sample(frac=subsample_fraction, random_state=42)\n",
        "\n",
        "# Initialize 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results_list = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(subsampled_df), 1):\n",
        "    print(f\"Processing Fold {fold}\")\n",
        "    train_df = subsampled_df.iloc[train_index].copy()\n",
        "    test_df = subsampled_df.iloc[test_index].copy()\n",
        "\n",
        "    # Scale features\n",
        "    features = ['measurement', 'diameter', 'length', 'roughness']\n",
        "    scaler = RobustScaler()\n",
        "    X_train = scaler.fit_transform(train_df[features])\n",
        "    X_test = scaler.transform(test_df[features])\n",
        "\n",
        "    # K-distance plot\n",
        "    min_samples = 50\n",
        "    neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_train)\n",
        "    dists, _ = neighbors.kneighbors(X_train)\n",
        "    k_distances = np.sort(dists[:, min_samples - 1])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_distances)\n",
        "    plt.title(f'K-Distance Plot (Distance to {min_samples}th NN) - Fold {fold}')\n",
        "    plt.xlabel('Points Sorted')\n",
        "    plt.ylabel('Distance')\n",
        "    plt.savefig(f'k_distance_plot_fold_{fold}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Parameter tuning\n",
        "    suggested_eps = np.percentile(k_distances, 95)\n",
        "    eps_values = np.arange(suggested_eps * 0.5, suggested_eps * 1.5, suggested_eps * 0.1)\n",
        "    min_samples_values = [20, 30, 50]\n",
        "    best_score = -1\n",
        "    best_params = {'eps': None, 'min_samples': None}\n",
        "    for eps in eps_values:\n",
        "        for ms in min_samples_values:\n",
        "            labels = DBSCAN(eps=eps, min_samples=ms).fit_predict(X_train)\n",
        "            if len(set(labels)) > 1 and -1 in labels:\n",
        "                score = silhouette_score(X_train, labels, sample_size=2000)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'eps': eps, 'min_samples': ms}\n",
        "    print(f\"Best DBSCAN params for Fold {fold}: {best_params}, silhouette={best_score:.3f}\")\n",
        "\n",
        "    # Final clustering\n",
        "    db = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "    train_df['cluster'] = db.fit_predict(X_train)\n",
        "    test_df['cluster'] = db.fit_predict(X_test)\n",
        "\n",
        "    # Combine data for this fold\n",
        "    merged_all = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "    # Compute cluster stats\n",
        "    cluster_stats = train_df[train_df['cluster'] != -1].groupby('cluster').agg({\n",
        "        'measurement': ['mean', 'std', 'count'],\n",
        "        'pipe_id': lambda x: list(x.unique())\n",
        "    }).reset_index()\n",
        "    cluster_stats.columns = ['cluster', 'cluster_mu', 'cluster_sigma', 'point_count', 'pipes']\n",
        "    # Merge small clusters into noise\n",
        "    min_cluster_size = 100\n",
        "    small_clusters = cluster_stats[cluster_stats['point_count'] < min_cluster_size]['cluster']\n",
        "    merged_all.loc[merged_all['cluster'].isin(small_clusters), 'cluster'] = -1\n",
        "\n",
        "    # Map cluster stats\n",
        "    dict_mu = cluster_stats.set_index('cluster')['cluster_mu'].to_dict()\n",
        "    dict_sigma = cluster_stats.set_index('cluster')['cluster_sigma'].to_dict()\n",
        "    merged_all['cluster_mu'] = merged_all['cluster'].map(dict_mu).fillna(global_flow_mean)\n",
        "    merged_all['cluster_sigma'] = merged_all['cluster'].map(dict_sigma).fillna(global_flow_std)\n",
        "\n",
        "    # Z-scores and flags\n",
        "    threshold = 3\n",
        "    merged_all['Z_score'] = (merged_all['measurement'] - merged_all['cluster_mu']) / merged_all['cluster_sigma']\n",
        "    merged_all['Z_trad'] = (merged_all['measurement'] - global_flow_mean) / global_flow_std\n",
        "    merged_all['outside_plausible'] = (\n",
        "        merged_all['measurement'] < merged_all['min_plausible']\n",
        "    ) | (\n",
        "        merged_all['measurement'] > merged_all['max_plausible']\n",
        "    )\n",
        "    merged_all['is_anomaly'] = merged_all['Z_score'].abs() > threshold\n",
        "    merged_all['is_anomaly_traditional'] = merged_all['Z_trad'].abs() > threshold\n",
        "    merged_all['is_noise'] = merged_all['is_anomaly'] & merged_all['outside_plausible']\n",
        "    merged_all['is_noise_traditional'] = merged_all['is_anomaly_traditional'] & merged_all['outside_plausible']\n",
        "\n",
        "    # --- Noise Introduction using GNN ---\n",
        "    def introduce_gnn_noise(df, beta=1.5, scale=global_flow_std, noise_fraction=0.05):\n",
        "        noisy_df = df.copy()\n",
        "        mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "        noise = gennorm.rvs(beta, loc=0, scale=scale, size=len(noisy_df))\n",
        "        noisy_df['measurement'] += noise * mask\n",
        "        noisy_df['is_true_noise'] = mask.astype(int)\n",
        "        return noisy_df\n",
        "\n",
        "    # Evaluation loop\n",
        "    noise_fractions = [0.02, 0.05, 0.10, 0.20]\n",
        "    for noise_fraction in noise_fractions:\n",
        "        print(f\"Processing GNN noise fraction: {noise_fraction*100}% in Fold {fold}\")\n",
        "        noisy_df = introduce_gnn_noise(\n",
        "            merged_all,\n",
        "            beta=1.5,\n",
        "            scale=global_flow_std,\n",
        "            noise_fraction=noise_fraction\n",
        "        )\n",
        "\n",
        "        # Recompute detection flags\n",
        "        noisy_df['Z_score'] = (noisy_df['measurement'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma']\n",
        "        noisy_df['Z_trad'] = (noisy_df['measurement'] - global_flow_mean) / global_flow_std\n",
        "        noisy_df['outside_plausible'] = (\n",
        "            noisy_df['measurement'] < noisy_df['min_plausible']\n",
        "        ) | (\n",
        "            noisy_df['measurement'] > noisy_df['max_plausible']\n",
        "        )\n",
        "        noisy_df['is_anomaly'] = noisy_df['Z_score'].abs() > threshold\n",
        "        noisy_df['is_anomaly_traditional'] = noisy_df['Z_trad'].abs() > threshold\n",
        "        noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "        noisy_df['is_noise_traditional'] = noisy_df['is_anomaly_traditional'] & noisy_df['outside_plausible']\n",
        "\n",
        "        # Metrics\n",
        "        y_true = noisy_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "        y_pred_traditional = noisy_df['is_noise_traditional'].astype(int)\n",
        "\n",
        "        precision_hybrid = precision_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        recall_hybrid = recall_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        precision_traditional = precision_score(y_true, y_pred_traditional, zero_division=0)\n",
        "        recall_traditional = recall_score(y_true, y_pred_traditional, zero_division=0)\n",
        "        f1_traditional = f1_score(y_true, y_pred_traditional, zero_division=0)\n",
        "\n",
        "        total_noise_points = y_true.sum()\n",
        "        hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "        traditional_detected = y_pred_traditional[y_true == 1].sum()\n",
        "        hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "        traditional_noise_percentage = (traditional_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Hybrid (Cluster-based)',\n",
        "            'Precision': precision_hybrid,\n",
        "            'Recall': recall_hybrid,\n",
        "            'F1 Score': f1_hybrid,\n",
        "            'Noise Detected (%)': hybrid_noise_percentage\n",
        "        })\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Traditional (Global)',\n",
        "            'Precision': precision_traditional,\n",
        "            'Recall': recall_traditional,\n",
        "            'F1 Score': f1_traditional,\n",
        "            'Noise Detected (%)': traditional_noise_percentage\n",
        "        })\n",
        "\n",
        "# Consolidate and save\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(\"\\nConsolidated Evaluation Results:\")\n",
        "print(results_df.groupby(['Noise Fraction (%)', 'Method']).agg({\n",
        "    'Precision': 'mean',\n",
        "    'Recall': 'mean',\n",
        "    'F1 Score': 'mean',\n",
        "    'Noise Detected (%)': 'mean'\n",
        "}).reset_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvFTk8UVVjEF",
        "outputId": "85fe84ab-6b33-4e9a-f140-78d0e45b67cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data size: 5956800 rows.\n",
            "Global flow stats: Mean = 829.88, Std = 1495.64\n",
            "Processing Fold 1\n",
            "Best DBSCAN params for Fold 1: {'eps': np.float64(0.003174603174604051), 'min_samples': 20}, silhouette=0.582\n",
            "Processing GNN noise fraction: 2.0% in Fold 1\n",
            "Processing GNN noise fraction: 5.0% in Fold 1\n",
            "Processing GNN noise fraction: 10.0% in Fold 1\n",
            "Processing GNN noise fraction: 20.0% in Fold 1\n",
            "Processing Fold 2\n",
            "Best DBSCAN params for Fold 2: {'eps': np.float64(0.004126984126985266), 'min_samples': 20}, silhouette=0.598\n",
            "Processing GNN noise fraction: 2.0% in Fold 2\n",
            "Processing GNN noise fraction: 5.0% in Fold 2\n",
            "Processing GNN noise fraction: 10.0% in Fold 2\n",
            "Processing GNN noise fraction: 20.0% in Fold 2\n",
            "Processing Fold 3\n",
            "Best DBSCAN params for Fold 3: {'eps': np.float64(0.004444444444445672), 'min_samples': 20}, silhouette=0.585\n",
            "Processing GNN noise fraction: 2.0% in Fold 3\n",
            "Processing GNN noise fraction: 5.0% in Fold 3\n",
            "Processing GNN noise fraction: 10.0% in Fold 3\n",
            "Processing GNN noise fraction: 20.0% in Fold 3\n",
            "Processing Fold 4\n",
            "Best DBSCAN params for Fold 4: {'eps': np.float64(0.003809523809524861), 'min_samples': 20}, silhouette=0.599\n",
            "Processing GNN noise fraction: 2.0% in Fold 4\n",
            "Processing GNN noise fraction: 5.0% in Fold 4\n",
            "Processing GNN noise fraction: 10.0% in Fold 4\n",
            "Processing GNN noise fraction: 20.0% in Fold 4\n",
            "Processing Fold 5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyM1EG3se5h21aOW/oyNwVxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}