{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonanew/noise_detection/blob/main/DBSCAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yPXzjZaUbsA",
        "outputId": "45c991ec-63a5-410d-e00c-08e0c5a0d646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib) (3.2.3)\n",
            "Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/565.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m563.2/565.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdflib\n",
            "Successfully installed rdflib-7.1.4\n",
            "Collecting noise\n",
            "  Downloading noise-1.2.2.zip (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: noise\n",
            "  Building wheel for noise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for noise: filename=noise-1.2.2-cp311-cp311-linux_x86_64.whl size=56278 sha256=4209298aec21c8325fe2a1f06933da245aab07966596065aa19852fb0983794c\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/25/2e/af6d1bcc91a8f99af0f651f8718b9ab999720a21c6d4149091\n",
            "Successfully built noise\n",
            "Installing collected packages: noise\n",
            "Successfully installed noise-1.2.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib\n",
        "\n",
        "# Install noise package for pink noise generation\n",
        "!pip install noise\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssp8oKr_Lm8M",
        "outputId": "4b68d93b-3cb7-4e0f-a397-0d7f511cdd6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        timestamp                        sensor_id  \\\n",
            "0        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "1        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "2        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "3        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "4        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "...                           ...                              ...   \n",
            "4605955  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605956  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605957  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605958  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605959  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "\n",
            "         measurement           property  \n",
            "0           1.528132       AttitudeRoll  \n",
            "1          -0.733896      AttitudePitch  \n",
            "2           0.696372        AttitudeYaw  \n",
            "3           0.294894  UserAccelerationX  \n",
            "4          -0.184493  UserAccelerationY  \n",
            "...              ...                ...  \n",
            "4605955    -1.435057      AttitudePitch  \n",
            "4605956     0.377682        AttitudeYaw  \n",
            "4605957     0.043053  UserAccelerationX  \n",
            "4605958     3.977308  UserAccelerationY  \n",
            "4605959     0.174186  UserAccelerationZ  \n",
            "\n",
            "[4605960 rows x 4 columns]\n",
            "Global magnitude stats: Mean = 0.753240, Std = 0.638734\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Subsampled data size: 153532\n",
            "Processing Fold 1\n",
            "Training set size: 122825, Test set size: 30707\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 1: 0.292\n",
            "Best parameters for Fold 1: eps=0.080, min_samples=20, Best Silhouette Score=0.255\n",
            "Processing noise fraction: 2.0% in Fold 1\n",
            "Processing noise fraction: 5.0% in Fold 1\n",
            "Processing noise fraction: 10.0% in Fold 1\n",
            "Processing noise fraction: 20.0% in Fold 1\n",
            "Processing Fold 2\n",
            "Training set size: 122825, Test set size: 30707\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 2: 0.294\n",
            "Best parameters for Fold 2: eps=0.080, min_samples=20, Best Silhouette Score=0.252\n",
            "Processing noise fraction: 2.0% in Fold 2\n",
            "Processing noise fraction: 5.0% in Fold 2\n",
            "Processing noise fraction: 10.0% in Fold 2\n",
            "Processing noise fraction: 20.0% in Fold 2\n",
            "Processing Fold 3\n",
            "Training set size: 122826, Test set size: 30706\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 3: 0.290\n",
            "Best parameters for Fold 3: eps=0.080, min_samples=20, Best Silhouette Score=0.240\n",
            "Processing noise fraction: 2.0% in Fold 3\n",
            "Processing noise fraction: 5.0% in Fold 3\n",
            "Processing noise fraction: 10.0% in Fold 3\n",
            "Processing noise fraction: 20.0% in Fold 3\n",
            "Processing Fold 4\n",
            "Training set size: 122826, Test set size: 30706\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 4: 0.290\n",
            "Best parameters for Fold 4: eps=0.070, min_samples=20, Best Silhouette Score=0.248\n",
            "Processing noise fraction: 2.0% in Fold 4\n",
            "Processing noise fraction: 5.0% in Fold 4\n",
            "Processing noise fraction: 10.0% in Fold 4\n",
            "Processing noise fraction: 20.0% in Fold 4\n",
            "Processing Fold 5\n",
            "Training set size: 122826, Test set size: 30706\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 5: 0.291\n",
            "Best parameters for Fold 5: eps=0.080, min_samples=20, Best Silhouette Score=0.240\n",
            "Processing noise fraction: 2.0% in Fold 5\n",
            "Processing noise fraction: 5.0% in Fold 5\n",
            "Processing noise fraction: 10.0% in Fold 5\n",
            "Processing noise fraction: 20.0% in Fold 5\n",
            "\n",
            "Consolidated Evaluation Results:\n",
            "   Noise Fraction (%)                  Method  Precision    Recall  F1 Score  \\\n",
            "0                 2.0  Hybrid (Cluster-based)   0.370806  0.398589  0.384189   \n",
            "1                 2.0    Traditional (Global)   0.289558  0.283336  0.286407   \n",
            "2                 5.0  Hybrid (Cluster-based)   0.608099  0.406970  0.487604   \n",
            "3                 5.0    Traditional (Global)   0.523538  0.296311  0.378432   \n",
            "4                10.0  Hybrid (Cluster-based)   0.767259  0.408272  0.532945   \n",
            "5                10.0    Traditional (Global)   0.699657  0.296792  0.416778   \n",
            "6                20.0  Hybrid (Cluster-based)   0.880408  0.406548  0.556233   \n",
            "7                20.0    Traditional (Global)   0.838584  0.294727  0.436151   \n",
            "\n",
            "   Noise Detected (%)  \n",
            "0           39.858947  \n",
            "1           28.333628  \n",
            "2           40.696962  \n",
            "3           29.631140  \n",
            "4           40.827191  \n",
            "5           29.679201  \n",
            "6           40.654784  \n",
            "7           29.472717  \n",
            "All results saved to '/content/drive/My Drive/ColabNotebooks/all_results_clustered_optimized_dbscan.csv'.\n",
            "Noisy dataset saved to '/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan.csv'.\n",
            "Evaluation results saved to '/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan.csv'.\n",
            "\n",
            "Z-score stats (noisy dataset, last noise level of last fold):\n",
            "              Z_score  traditional_Z_score\n",
            "count  153532.000000        153532.000000\n",
            "mean        0.073296            -0.007574\n",
            "std         3.426426             1.330775\n",
            "min      -143.559924            -9.863654\n",
            "25%        -0.880000            -0.678279\n",
            "50%        -0.034483            -0.271224\n",
            "75%         1.000000             0.433294\n",
            "max       157.424676            15.283588\n",
            "Noise detection summary (hybrid, noisy dataset):\n",
            " is_noise\n",
            "False    139411\n",
            "True      14121\n",
            "Name: count, dtype: int64\n",
            "Noise detection summary (traditional, noisy dataset):\n",
            " is_noise_traditional\n",
            "False    142757\n",
            "True      10775\n",
            "Name: count, dtype: int64\n",
            "Total noise points introduced (last noise level of last fold): 30522\n",
            "Hybrid method detected: 12453 (40.80%)\n",
            "Traditional method detected: 9051 (29.65%)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdflib import Graph\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths to sensor reading csv file data and knowledge graph\n",
        "data_path = \"/content/drive/My Drive/ColabNotebooks/motion_sensor_readings.csv\"\n",
        "kg_path = \"/content/drive/My Drive/ColabNotebooks/motion_sense_ssn_kg.ttl\"\n",
        "\n",
        "# Load knowledge graph\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(kg_path, format='turtle')\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing knowledge graph: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for participant characteristics\n",
        "query = \"\"\"\n",
        "PREFIX ms: <http://example.org/motion-sense#>\n",
        "SELECT ?p ?code ?age ?gender ?height ?weight\n",
        "WHERE {\n",
        "    ?p a ms:Participant ;\n",
        "       ms:hasCode ?code ;\n",
        "       ms:hasAge ?age ;\n",
        "       ms:hasGender ?gender ;\n",
        "       ms:hasHeightCm ?height ;\n",
        "       ms:hasWeightKg ?weight .\n",
        "}\n",
        "\"\"\"\n",
        "results = g.query(query)\n",
        "participants = []\n",
        "for row in results:\n",
        "    participants.append({\n",
        "        'participant': str(row.code).strip().upper(),\n",
        "        'age': int(row.age),\n",
        "        'gender': int(row.gender),\n",
        "        'height': float(row.height),\n",
        "        'weight': float(row.weight)\n",
        "    })\n",
        "participants_df = pd.DataFrame(participants)\n",
        "\n",
        "if participants_df.empty:\n",
        "    print(\"Error: No participants found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for activity plausible ranges\n",
        "query_ranges = \"\"\"\n",
        "PREFIX activity: <http://example.org/activity-recognition#>\n",
        "SELECT ?activityCode ?min ?max\n",
        "WHERE {\n",
        "    ?act a activity:Activity ;\n",
        "         activity:hasActivityCode ?activityCode ;\n",
        "         activity:hasMinAcceleration ?min ;\n",
        "         activity:hasMaxAcceleration ?max .\n",
        "}\n",
        "\"\"\"\n",
        "results_ranges = g.query(query_ranges)\n",
        "activity_ranges = {}\n",
        "for row in results_ranges:\n",
        "    activity_code = str(row.activityCode).strip().lower()\n",
        "    activity_ranges[activity_code] = {\n",
        "        'min': float(row.min),\n",
        "        'max': float(row.max)\n",
        "    }\n",
        "\n",
        "if not activity_ranges:\n",
        "    print(\"Error: No activity ranges found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Load sensor readings csv file data\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(df)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sensor data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Parse sensor_id with validation\n",
        "def parse_sensor_id(sid):\n",
        "    parts = sid.split('_')\n",
        "    if len(parts) >= 5:\n",
        "        return parts[0].strip().upper(), parts[3], parts[4]\n",
        "    return None, None, None\n",
        "\n",
        "df['participant'], df['activity'], df['sensor_type'] = zip(*df['sensor_id'].apply(parse_sensor_id))\n",
        "df = df.dropna(subset=['participant'])\n",
        "\n",
        "# Filter for accelerometer data\n",
        "df = df[(df['sensor_type'] == 'Accelerometer') &\n",
        "        (df['property'].isin(['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']))]\n",
        "\n",
        "# Convert measurement to numeric, drop invalid entries\n",
        "df['measurement'] = pd.to_numeric(df['measurement'], errors='coerce')\n",
        "df = df.dropna(subset=['measurement'])\n",
        "\n",
        "if df.empty:\n",
        "    print(\"Error: No valid accelerometer data after cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Pivot to get X, Y, Z\n",
        "acc_df = df.pivot_table(index=['timestamp', 'sensor_id', 'participant', 'activity'],\n",
        "                        columns='property', values='measurement', aggfunc='first').reset_index()\n",
        "\n",
        "# Drop rows with missing or invalid acceleration components\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "for col in ['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']:\n",
        "    acc_df[col] = pd.to_numeric(acc_df[col], errors='coerce')\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "\n",
        "if acc_df.empty:\n",
        "    print(\"Error: No valid data after pivot and cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Compute magnitude\n",
        "acc_df['magnitude'] = np.sqrt(acc_df['UserAccelerationX']**2 +\n",
        "                              acc_df['UserAccelerationY']**2 +\n",
        "                              acc_df['UserAccelerationZ']**2)\n",
        "\n",
        "# Compute global mean and std for magnitude (for traditional Z-score)\n",
        "global_magnitude_mean = acc_df['magnitude'].mean()\n",
        "global_magnitude_std = acc_df['magnitude'].std()\n",
        "print(f\"Global magnitude stats: Mean = {global_magnitude_mean:.6f}, Std = {global_magnitude_std:.6f}\")\n",
        "\n",
        "# Compute per-activity stats for reference on cleaned data\n",
        "activity_stats_df = acc_df.groupby('activity')['magnitude'].agg(['mean', 'std', 'count']).reset_index()\n",
        "activity_stats_df.columns = ['activity', 'actual_mean', 'actual_std', 'sample_count']\n",
        "\n",
        "# Ensure sufficient samples and smooth std\n",
        "activity_stats_df = activity_stats_df[activity_stats_df['sample_count'] >= 100]\n",
        "global_activity_std = activity_stats_df['actual_std'].mean()\n",
        "activity_stats_df['actual_std'] = activity_stats_df['actual_std'].apply(lambda x: max(x, global_activity_std * 0.1))\n",
        "\n",
        "# Debug: Print per-activity stats\n",
        "print(\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "\n",
        "# Merge with participant characteristics for clustering\n",
        "merged_df = pd.merge(acc_df, participants_df, on='participant', how='inner')\n",
        "\n",
        "if merged_df.empty:\n",
        "    print(\"Error: No matching participants found after merging.\")\n",
        "    exit(1)\n",
        "\n",
        "# Subsample the data to reduce memory usage\n",
        "subsample_fraction = 0.2  # Use 10% of the data\n",
        "subsampled_df = merged_df.sample(frac=subsample_fraction, random_state=42)\n",
        "print(f\"Subsampled data size: {len(subsampled_df)}\")\n",
        "\n",
        "# Initialize 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results_list = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(subsampled_df), 1):\n",
        "    print(f\"Processing Fold {fold}\")\n",
        "    train_df = subsampled_df.iloc[train_index].copy()\n",
        "    test_df = subsampled_df.iloc[test_index].copy()\n",
        "    print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "    # Prepare data for clustering on training set, including magnitude\n",
        "    X_cluster_train = train_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "    scaler = RobustScaler()\n",
        "    X_numeric_train = scaler.fit_transform(X_cluster_train[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "    X_activity_train = pd.get_dummies(X_cluster_train['activity'], prefix='activity')\n",
        "    X_scaled_train = np.hstack([X_numeric_train, X_activity_train])\n",
        "\n",
        "    # Prepare data for test set\n",
        "    X_cluster_test = test_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "    X_numeric_test = scaler.transform(X_cluster_test[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "    X_activity_test = pd.get_dummies(X_cluster_test['activity'], prefix='activity')\n",
        "    X_activity_test = X_activity_test.reindex(columns=X_activity_train.columns, fill_value=0)\n",
        "    X_scaled_test = np.hstack([X_numeric_test, X_activity_test])\n",
        "\n",
        "    # Generate k-distance plot to determine initial eps\n",
        "    min_samples = 50  # Default min_samples\n",
        "    neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_scaled_train)\n",
        "    distances, indices = neighbors.kneighbors(X_scaled_train)\n",
        "    k_distances = np.sort(distances[:, min_samples - 1])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_distances)\n",
        "    plt.title(f'K-Distance Plot (Distance to {min_samples}th Nearest Neighbor) - Fold {fold}')\n",
        "    plt.xlabel('Points Sorted by Distance')\n",
        "    plt.ylabel('Distance')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'/content/drive/My Drive/ColabNotebooks/k_distance_plot_fold_{fold}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Suggest eps based on the knee point (e.g., 95th percentile of k-distances)\n",
        "    suggested_eps = np.percentile(k_distances, 95)\n",
        "    print(f\"Suggested eps based on 95th percentile of k-distances for Fold {fold}: {suggested_eps:.3f}\")\n",
        "\n",
        "    # Parameter tuning using silhouette score\n",
        "    eps_values = np.arange(0.06, 0.08, 0.01)  # Adjusted range based on knee\n",
        "    min_samples_values = [20, 30, 50, 60, 70]  # Adjusted range\n",
        "    best_score = -1\n",
        "    best_params = {'eps': 0, 'min_samples': 0}\n",
        "\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = dbscan.fit_predict(X_scaled_train)\n",
        "            if len(np.unique(labels)) > 1 and -1 in labels:\n",
        "                score = silhouette_score(X_scaled_train, labels, sample_size=5000)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "    print(f\"Best parameters for Fold {fold}: eps={best_params['eps']:.3f}, min_samples={best_params['min_samples']}, Best Silhouette Score={best_score:.3f}\")\n",
        "\n",
        "    # Use best parameters for clustering\n",
        "    dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "    train_cluster_labels = dbscan.fit_predict(X_scaled_train)\n",
        "    test_cluster_labels = dbscan.fit_predict(X_scaled_test)\n",
        "\n",
        "    # Add cluster labels to train and test dataframes\n",
        "    train_df['cluster'] = train_cluster_labels\n",
        "    test_df['cluster'] = test_cluster_labels\n",
        "\n",
        "    # Combine train and test dataframes\n",
        "    acc_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "    # Assign cluster mean and std for Z-score calculation using DBSCAN clusters\n",
        "    cluster_means = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].mean().to_dict()\n",
        "    cluster_stds = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].std().to_dict()\n",
        "    acc_df['cluster_mu'] = acc_df['cluster'].map(cluster_means)\n",
        "    acc_df['cluster_sigma'] = acc_df['cluster'].map(cluster_stds).fillna(global_activity_std)\n",
        "    acc_df.loc[acc_df['cluster'] == -1, 'cluster_mu'] = global_magnitude_mean\n",
        "    acc_df.loc[acc_df['cluster'] == -1, 'cluster_sigma'] = global_magnitude_std\n",
        "\n",
        "    # Compute Z-scores using cluster-based statistics\n",
        "    acc_df['Z_score'] = (acc_df['magnitude'] - acc_df['cluster_mu']) / acc_df['cluster_sigma'].replace(0, np.nan)\n",
        "    acc_df['traditional_Z_score'] = (acc_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "\n",
        "    # Add global mean and std to output\n",
        "    acc_df['global_magnitude_mean'] = global_magnitude_mean\n",
        "    acc_df['global_magnitude_std'] = global_magnitude_std\n",
        "\n",
        "    # Apply activity-specific plausible ranges\n",
        "    acc_df['activity'] = acc_df['activity'].str.lower()\n",
        "    default_min_plausible = 0\n",
        "    default_max_plausible = 10\n",
        "    acc_df['min_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'min': default_min_plausible})['min'])\n",
        "    acc_df['max_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'max': default_max_plausible})['max'])\n",
        "\n",
        "    # Determine if observations are outside plausible range\n",
        "    acc_df['outside_plausible'] = (acc_df['magnitude'] < acc_df['min_plausible']) | (acc_df['magnitude'] > acc_df['max_plausible'])\n",
        "\n",
        "    # Determine anomaly\n",
        "    threshold = 2\n",
        "    acc_df['is_anomaly'] = np.abs(acc_df['Z_score']) > threshold\n",
        "    acc_df['is_anomaly_traditional'] = np.abs(acc_df['traditional_Z_score']) > threshold\n",
        "\n",
        "    # Label noise\n",
        "    acc_df['is_noise'] = acc_df['is_anomaly'] & acc_df['outside_plausible']\n",
        "    acc_df['is_noise_traditional'] = acc_df['is_anomaly_traditional'] & acc_df['outside_plausible']\n",
        "\n",
        "    # Round numerical columns in acc_df to 2 decimal places\n",
        "    numerical_cols = acc_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    acc_df[numerical_cols] = acc_df[numerical_cols].round(2)\n",
        "\n",
        "    # Function to introduce Gaussian noise\n",
        "    def introduce_noise(df, noise_level=0.1, noise_fraction=0.05):\n",
        "        noisy_df = df.copy()\n",
        "        mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "        noise = np.random.normal(0, noise_level, size=len(noisy_df))\n",
        "        noisy_df['magnitude'] = noisy_df['magnitude'] + noise * mask\n",
        "        noisy_df['is_true_noise'] = mask.astype(int)  # Ground truth: 1 for noisy, 0 for normal\n",
        "        return noisy_df\n",
        "\n",
        "    # List of noise fractions to test\n",
        "    noise_fractions = [0.02, 0.05, 0.10, 0.20]\n",
        "    for noise_fraction in noise_fractions:\n",
        "        print(f\"Processing noise fraction: {noise_fraction*100}% in Fold {fold}\")\n",
        "        # Introduce noise to the dataset\n",
        "        noisy_df = introduce_noise(acc_df, noise_level=2 * global_magnitude_std, noise_fraction=noise_fraction)\n",
        "\n",
        "        # Apply anomaly detection - Hybrid method\n",
        "        noisy_df['Z_score'] = (noisy_df['magnitude'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_df['is_anomaly'] = np.abs(noisy_df['Z_score']) > threshold\n",
        "        noisy_df['outside_plausible'] = (noisy_df['magnitude'] < noisy_df['min_plausible']) | (noisy_df['magnitude'] > noisy_df['max_plausible'])\n",
        "        noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "\n",
        "        # Apply anomaly detection - Traditional method\n",
        "        noisy_df['traditional_Z_score'] = (noisy_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "        noisy_df['is_anomaly_traditional'] = np.abs(noisy_df['traditional_Z_score']) > threshold\n",
        "        noisy_df['is_noise_traditional'] = noisy_df['is_anomaly_traditional'] & noisy_df['outside_plausible']\n",
        "\n",
        "        # Evaluate performance\n",
        "        y_true = noisy_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "        y_pred_traditional = noisy_df['is_noise_traditional'].astype(int)\n",
        "\n",
        "        precision_hybrid = precision_score(y_true, y_pred_hybrid)\n",
        "        recall_hybrid = recall_score(y_true, y_pred_hybrid)\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid)\n",
        "\n",
        "        precision_traditional = precision_score(y_true, y_pred_traditional)\n",
        "        recall_traditional = recall_score(y_true, y_pred_traditional)\n",
        "        f1_traditional = f1_score(y_true, y_pred_traditional)\n",
        "\n",
        "        # Calculate percentage of noise detected\n",
        "        total_noise_points = y_true.sum()\n",
        "        hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "        traditional_detected = y_pred_traditional[y_true == 1].sum()\n",
        "        hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "        traditional_noise_percentage = (traditional_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "        # Store results for this noise level\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Hybrid (Cluster-based)',\n",
        "            'Precision': precision_hybrid,\n",
        "            'Recall': recall_hybrid,\n",
        "            'F1 Score': f1_hybrid,\n",
        "            'Noise Detected (%)': hybrid_noise_percentage\n",
        "        })\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Traditional (Global)',\n",
        "            'Precision': precision_traditional,\n",
        "            'Recall': recall_traditional,\n",
        "            'F1 Score': f1_traditional,\n",
        "            'Noise Detected (%)': traditional_noise_percentage\n",
        "        })\n",
        "\n",
        "# Create consolidated results table\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(\"\\nConsolidated Evaluation Results:\")\n",
        "print(results_df.groupby(['Noise Fraction (%)', 'Method']).agg({\n",
        "    'Precision': 'mean',\n",
        "    'Recall': 'mean',\n",
        "    'F1 Score': 'mean',\n",
        "    'Noise Detected (%)': 'mean'\n",
        "}).reset_index())\n",
        "\n",
        "# Save all results to first CSV\n",
        "all_results_path = '/content/drive/My Drive/ColabNotebooks/all_results_clustered_optimized_dbscan.csv'\n",
        "try:\n",
        "    acc_df.to_csv(all_results_path, index=False)\n",
        "    print(f\"All results saved to '{all_results_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving all results: {e}\")\n",
        "\n",
        "# Save noisy dataset and results for the last fold\n",
        "try:\n",
        "    noisy_df.to_csv('/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan.csv', index=False)\n",
        "    print(f\"Noisy dataset saved to '/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan.csv'.\")\n",
        "    results_df.to_csv('/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan.csv', index=False)\n",
        "    print(f\"Evaluation results saved to '/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan.csv'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")\n",
        "\n",
        "# Debug: Print summary for the last noise level of the last fold\n",
        "print(\"\\nZ-score stats (noisy dataset, last noise level of last fold):\\n\", noisy_df[['Z_score', 'traditional_Z_score']].describe())\n",
        "print(\"Noise detection summary (hybrid, noisy dataset):\\n\", noisy_df['is_noise'].value_counts())\n",
        "print(\"Noise detection summary (traditional, noisy dataset):\\n\", noisy_df['is_noise_traditional'].value_counts())\n",
        "print(f\"Total noise points introduced (last noise level of last fold): {total_noise_points}\")\n",
        "print(f\"Hybrid method detected: {hybrid_detected} ({hybrid_noise_percentage:.2f}%)\")\n",
        "print(f\"Traditional method detected: {traditional_detected} ({traditional_noise_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txv-lU7oma9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1POxnp9NaZC",
        "outputId": "e69f598a-dd22-4ad9-c871-7b563bea7990"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        timestamp                        sensor_id  \\\n",
            "0        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "1        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "2        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "3        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "4        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "...                           ...                              ...   \n",
            "4605955  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605956  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605957  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605958  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605959  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "\n",
            "         measurement           property  \n",
            "0           1.528132       AttitudeRoll  \n",
            "1          -0.733896      AttitudePitch  \n",
            "2           0.696372        AttitudeYaw  \n",
            "3           0.294894  UserAccelerationX  \n",
            "4          -0.184493  UserAccelerationY  \n",
            "...              ...                ...  \n",
            "4605955    -1.435057      AttitudePitch  \n",
            "4605956     0.377682        AttitudeYaw  \n",
            "4605957     0.043053  UserAccelerationX  \n",
            "4605958     3.977308  UserAccelerationY  \n",
            "4605959     0.174186  UserAccelerationZ  \n",
            "\n",
            "[4605960 rows x 4 columns]\n",
            "Global magnitude stats: Mean = 0.753240, Std = 0.638734\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Subsampled data size: 307064\n",
            "Processing Fold 1\n",
            "Training set size: 245651, Test set size: 61413\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 1: 0.156\n",
            "Best parameters for Fold 1: eps=0.6, min_samples=30, Best Silhouette Score=0.339\n",
            "Processing noise fraction: 2.0% in Fold 1\n",
            "Processing noise fraction: 5.0% in Fold 1\n",
            "Processing noise fraction: 10.0% in Fold 1\n",
            "Processing noise fraction: 20.0% in Fold 1\n",
            "Processing Fold 2\n",
            "Training set size: 245651, Test set size: 61413\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 2: 0.155\n",
            "Best parameters for Fold 2: eps=0.6, min_samples=40, Best Silhouette Score=0.346\n",
            "Processing noise fraction: 2.0% in Fold 2\n",
            "Processing noise fraction: 5.0% in Fold 2\n",
            "Processing noise fraction: 10.0% in Fold 2\n",
            "Processing noise fraction: 20.0% in Fold 2\n",
            "Processing Fold 3\n",
            "Training set size: 245651, Test set size: 61413\n",
            "Suggested eps based on 95th percentile of k-distances for Fold 3: 0.155\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import gennorm\n",
        "from rdflib import Graph\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths to sensor reading csv file data and knowledge graph\n",
        "data_path = \"/content/drive/My Drive/ColabNotebooks/motion_sensor_readings.csv\"\n",
        "kg_path = \"/content/drive/My Drive/ColabNotebooks/motion_sense_ssn_kg.ttl\"\n",
        "\n",
        "# Load knowledge graph\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(kg_path, format='turtle')\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing knowledge graph: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for participant characteristics\n",
        "query = \"\"\"\n",
        "PREFIX ms: <http://example.org/motion-sense#>\n",
        "SELECT ?p ?code ?age ?gender ?height ?weight\n",
        "WHERE {\n",
        "    ?p a ms:Participant ;\n",
        "       ms:hasCode ?code ;\n",
        "       ms:hasAge ?age ;\n",
        "       ms:hasGender ?gender ;\n",
        "       ms:hasHeightCm ?height ;\n",
        "       ms:hasWeightKg ?weight .\n",
        "}\n",
        "\"\"\"\n",
        "results = g.query(query)\n",
        "participants = []\n",
        "for row in results:\n",
        "    participants.append({\n",
        "        'participant': str(row.code).strip().upper(),\n",
        "        'age': int(row.age),\n",
        "        'gender': int(row.gender),\n",
        "        'height': float(row.height),\n",
        "        'weight': float(row.weight)\n",
        "    })\n",
        "participants_df = pd.DataFrame(participants)\n",
        "\n",
        "if participants_df.empty:\n",
        "    print(\"Error: No participants found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for activity plausible ranges\n",
        "query_ranges = \"\"\"\n",
        "PREFIX activity: <http://example.org/activity-recognition#>\n",
        "SELECT ?activityCode ?min ?max\n",
        "WHERE {\n",
        "    ?act a activity:Activity ;\n",
        "         activity:hasActivityCode ?activityCode ;\n",
        "         activity:hasMinAcceleration ?min ;\n",
        "         activity:hasMaxAcceleration ?max .\n",
        "}\n",
        "\"\"\"\n",
        "results_ranges = g.query(query_ranges)\n",
        "activity_ranges = {}\n",
        "for row in results_ranges:\n",
        "    activity_code = str(row.activityCode).strip().lower()\n",
        "    activity_ranges[activity_code] = {\n",
        "        'min': float(row.min),\n",
        "        'max': float(row.max)\n",
        "    }\n",
        "\n",
        "if not activity_ranges:\n",
        "    print(\"Error: No activity ranges found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Load sensor readings csv file data\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(df)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sensor data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Parse sensor_id with validation\n",
        "def parse_sensor_id(sid):\n",
        "    parts = sid.split('_')\n",
        "    if len(parts) >= 5:\n",
        "        return parts[0].strip().upper(), parts[3], parts[4]\n",
        "    return None, None, None\n",
        "\n",
        "df['participant'], df['activity'], df['sensor_type'] = zip(*df['sensor_id'].apply(parse_sensor_id))\n",
        "df = df.dropna(subset=['participant'])\n",
        "\n",
        "# Filter for accelerometer data\n",
        "df = df[(df['sensor_type'] == 'Accelerometer') &\n",
        "        (df['property'].isin(['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']))]\n",
        "\n",
        "# Convert measurement to numeric, drop invalid entries\n",
        "df['measurement'] = pd.to_numeric(df['measurement'], errors='coerce')\n",
        "df = df.dropna(subset=['measurement'])\n",
        "\n",
        "if df.empty:\n",
        "    print(\"Error: No valid accelerometer data after cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Pivot to get X, Y, Z\n",
        "acc_df = df.pivot_table(index=['timestamp', 'sensor_id', 'participant', 'activity'],\n",
        "                        columns='property', values='measurement', aggfunc='first').reset_index()\n",
        "\n",
        "# Drop rows with missing or invalid acceleration components\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "for col in ['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']:\n",
        "    acc_df[col] = pd.to_numeric(acc_df[col], errors='coerce')\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "\n",
        "if acc_df.empty:\n",
        "    print(\"Error: No valid data after pivot and cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Compute magnitude\n",
        "acc_df['magnitude'] = np.sqrt(acc_df['UserAccelerationX']**2 +\n",
        "                              acc_df['UserAccelerationY']**2 +\n",
        "                              acc_df['UserAccelerationZ']**2)\n",
        "\n",
        "# Compute global mean and std for magnitude (for traditional Z-score)\n",
        "global_magnitude_mean = acc_df['magnitude'].mean()\n",
        "global_magnitude_std = acc_df['magnitude'].std()\n",
        "print(f\"Global magnitude stats: Mean = {global_magnitude_mean:.6f}, Std = {global_magnitude_std:.6f}\")\n",
        "\n",
        "# Compute per-activity stats for reference on cleaned data\n",
        "activity_stats_df = acc_df.groupby('activity')['magnitude'].agg(['mean', 'std', 'count']).reset_index()\n",
        "activity_stats_df.columns = ['activity', 'actual_mean', 'actual_std', 'sample_count']\n",
        "\n",
        "# Ensure sufficient samples and smooth std\n",
        "activity_stats_df = activity_stats_df[activity_stats_df['sample_count'] >= 100]\n",
        "global_activity_std = activity_stats_df['actual_std'].mean()\n",
        "activity_stats_df['actual_std'] = activity_stats_df['actual_std'].apply(lambda x: max(x, global_activity_std * 0.1))\n",
        "\n",
        "# Debug: Print per-activity stats\n",
        "print(\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "\n",
        "# Merge with participant characteristics for clustering\n",
        "merged_df = pd.merge(acc_df, participants_df, on='participant', how='inner')\n",
        "\n",
        "if merged_df.empty:\n",
        "    print(\"Error: No matching participants found after merging.\")\n",
        "    exit(1)\n",
        "\n",
        "# Subsample the data to reduce memory usage\n",
        "subsample_fraction = 0.4  # Use 40% of the data\n",
        "subsampled_df = merged_df.sample(frac=subsample_fraction, random_state=42)\n",
        "print(f\"Subsampled data size: {len(subsampled_df)}\")\n",
        "\n",
        "# Initialize 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results_list = []\n",
        "\n",
        "# Iterate over folds\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(subsampled_df), 1):\n",
        "    print(f\"Processing Fold {fold}\")\n",
        "    train_df = subsampled_df.iloc[train_index].copy()\n",
        "    test_df = subsampled_df.iloc[test_index].copy()\n",
        "    print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "    # Prepare data for clustering on training set, including magnitude\n",
        "    X_cluster_train = train_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "    scaler = RobustScaler()\n",
        "    X_numeric_train = scaler.fit_transform(X_cluster_train[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "    X_activity_train = pd.get_dummies(X_cluster_train['activity'], prefix='activity')\n",
        "    X_scaled_train = np.hstack([X_numeric_train, X_activity_train])\n",
        "\n",
        "    # Prepare data for test set\n",
        "    X_cluster_test = test_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "    X_numeric_test = scaler.transform(X_cluster_test[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "    X_activity_test = pd.get_dummies(X_cluster_test['activity'], prefix='activity')\n",
        "    X_activity_test = X_activity_test.reindex(columns=X_activity_train.columns, fill_value=0)\n",
        "    X_scaled_test = np.hstack([X_numeric_test, X_activity_test])\n",
        "\n",
        "    # Generate k-distance plot to determine initial eps\n",
        "    min_samples = 50  # Default min_samples\n",
        "    neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_scaled_train)\n",
        "    distances, indices = neighbors.kneighbors(X_scaled_train)\n",
        "    k_distances = np.sort(distances[:, min_samples - 1])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_distances)\n",
        "    plt.title(f'K-Distance Plot (Distance to {min_samples}th Nearest Neighbor) - Fold {fold}')\n",
        "    plt.xlabel('Points Sorted by Distance')\n",
        "    plt.ylabel('Distance')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f'/content/drive/My Drive/ColabNotebooks/k_distance_plot_fold_{fold}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Suggest eps based on the knee point (e.g., 95th percentile of k-distances)\n",
        "    suggested_eps = np.percentile(k_distances, 95)\n",
        "    print(f\"Suggested eps based on 95th percentile of k-distances for Fold {fold}: {suggested_eps:.3f}\")\n",
        "\n",
        "    # Parameter tuning using silhouette score\n",
        "    eps_values = np.arange(0.1, 1.0, 0.1)  # Wider range to capture more noise\n",
        "    min_samples_values = [10, 20, 30, 40, 50]  # Adjusted range\n",
        "    best_score = -1\n",
        "    best_params = {'eps': 0, 'min_samples': 0}\n",
        "\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "            labels = dbscan.fit_predict(X_scaled_train)\n",
        "            if len(np.unique(labels)) > 1 and -1 in labels:\n",
        "                score = silhouette_score(X_scaled_train, labels, sample_size=5000)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "    print(f\"Best parameters for Fold {fold}: eps={best_params['eps']:.1f}, min_samples={best_params['min_samples']}, Best Silhouette Score={best_score:.3f}\")\n",
        "\n",
        "    # Use best parameters for clustering\n",
        "    dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "    train_cluster_labels = dbscan.fit_predict(X_scaled_train)\n",
        "    test_cluster_labels = dbscan.fit_predict(X_scaled_test)\n",
        "\n",
        "    # Add cluster labels to train and test dataframes\n",
        "    train_df['cluster'] = train_cluster_labels\n",
        "    test_df['cluster'] = test_cluster_labels\n",
        "\n",
        "    # Combine train and test dataframes\n",
        "    acc_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "    # Assign cluster mean and std for Z-score calculation using DBSCAN clusters\n",
        "    cluster_means = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].mean().to_dict()\n",
        "    cluster_stds = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].std().to_dict()\n",
        "    acc_df['cluster_mu'] = acc_df['cluster'].map(cluster_means)\n",
        "    acc_df['cluster_sigma'] = acc_df['cluster'].map(cluster_stds).fillna(global_activity_std)\n",
        "    acc_df.loc[acc_df['cluster'] == -1, 'cluster_mu'] = global_magnitude_mean\n",
        "    acc_df.loc[acc_df['cluster'] == -1, 'cluster_sigma'] = global_magnitude_std\n",
        "\n",
        "    # Compute Z-scores using cluster-based statistics\n",
        "    acc_df['Z_score'] = (acc_df['magnitude'] - acc_df['cluster_mu']) / acc_df['cluster_sigma'].replace(0, np.nan)\n",
        "    acc_df['traditional_Z_score'] = (acc_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "\n",
        "    # Add global mean and std to output\n",
        "    acc_df['global_magnitude_mean'] = global_magnitude_mean\n",
        "    acc_df['global_magnitude_std'] = global_magnitude_std\n",
        "\n",
        "    # Apply activity-specific plausible ranges\n",
        "    acc_df['activity'] = acc_df['activity'].str.lower()\n",
        "    default_min_plausible = 0\n",
        "    default_max_plausible = 10\n",
        "    acc_df['min_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'min': default_min_plausible})['min'])\n",
        "    acc_df['max_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'max': default_max_plausible})['max'])\n",
        "\n",
        "    # Determine if observations are outside plausible range\n",
        "    acc_df['outside_plausible'] = (acc_df['magnitude'] < acc_df['min_plausible']) | (acc_df['magnitude'] > acc_df['max_plausible'])\n",
        "\n",
        "    # Determine anomaly\n",
        "    threshold = 2\n",
        "    acc_df['is_anomaly'] = np.abs(acc_df['Z_score']) > threshold\n",
        "    acc_df['is_anomaly_traditional'] = np.abs(acc_df['traditional_Z_score']) > threshold\n",
        "\n",
        "    # Label noise\n",
        "    acc_df['is_noise'] = acc_df['is_anomaly'] & acc_df['outside_plausible']\n",
        "    acc_df['is_noise_traditional'] = acc_df['is_anomaly_traditional'] & acc_df['outside_plausible']\n",
        "\n",
        "    # Round numerical columns in acc_df to 2 decimal places\n",
        "    numerical_cols = acc_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    acc_df[numerical_cols] = acc_df[numerical_cols].round(2)\n",
        "\n",
        "    # Function to introduce Generalized Normal noise\n",
        "    def introduce_noise(df, noise_level=0.1, noise_fraction=0.05):\n",
        "        noisy_df = df.copy()\n",
        "        mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "        noise = gennorm.rvs(beta=1.5, loc=0, scale=noise_level, size=len(noisy_df))\n",
        "        noisy_df['magnitude'] = noisy_df['magnitude'] + noise * mask\n",
        "        noisy_df['is_true_noise'] = mask.astype(int)  # Ground truth: 1 for noisy, 0 for normal\n",
        "        return noisy_df\n",
        "\n",
        "    # List of noise fractions to test\n",
        "    noise_fractions = [0.02, 0.05, 0.10, 0.20]\n",
        "    for noise_fraction in noise_fractions:\n",
        "        print(f\"Processing noise fraction: {noise_fraction*100}% in Fold {fold}\")\n",
        "        # Introduce noise to the dataset\n",
        "        noisy_df = introduce_noise(acc_df, noise_level=2 * global_magnitude_std, noise_fraction=noise_fraction)\n",
        "\n",
        "        # Apply anomaly detection - Hybrid method\n",
        "        noisy_df['Z_score'] = (noisy_df['magnitude'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_df['is_anomaly'] = np.abs(noisy_df['Z_score']) > threshold\n",
        "        noisy_df['outside_plausible'] = (noisy_df['magnitude'] < noisy_df['min_plausible']) | (noisy_df['magnitude'] > noisy_df['max_plausible'])\n",
        "        noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "\n",
        "        # Apply anomaly detection - Traditional method\n",
        "        noisy_df['traditional_Z_score'] = (noisy_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "        noisy_df['is_anomaly_traditional'] = np.abs(noisy_df['traditional_Z_score']) > threshold\n",
        "        noisy_df['is_noise_traditional'] = noisy_df['is_anomaly_traditional'] & noisy_df['outside_plausible']\n",
        "\n",
        "        # Evaluate performance\n",
        "        y_true = noisy_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "        y_pred_traditional = noisy_df['is_noise_traditional'].astype(int)\n",
        "\n",
        "        precision_hybrid = precision_score(y_true, y_pred_hybrid)\n",
        "        recall_hybrid = recall_score(y_true, y_pred_hybrid)\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid)\n",
        "\n",
        "        precision_traditional = precision_score(y_true, y_pred_traditional)\n",
        "        recall_traditional = recall_score(y_true, y_pred_traditional)\n",
        "        f1_traditional = f1_score(y_true, y_pred_traditional)\n",
        "\n",
        "        # Calculate percentage of noise detected\n",
        "        total_noise_points = y_true.sum()\n",
        "        hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "        traditional_detected = y_pred_traditional[y_true == 1].sum()\n",
        "        hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "        traditional_noise_percentage = (traditional_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "        # Store results for this noise level\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Hybrid (Cluster-based)',\n",
        "            'Precision': precision_hybrid,\n",
        "            'Recall': recall_hybrid,\n",
        "            'F1 Score': f1_hybrid,\n",
        "            'Noise Detected (%)': hybrid_noise_percentage\n",
        "        })\n",
        "        results_list.append({\n",
        "            'Algorithm': 'DBSCAN',\n",
        "            'Fold': fold,\n",
        "            'Noise Fraction (%)': noise_fraction * 100,\n",
        "            'Method': 'Traditional (Global)',\n",
        "            'Precision': precision_traditional,\n",
        "            'Recall': recall_traditional,\n",
        "            'F1 Score': f1_traditional,\n",
        "            'Noise Detected (%)': traditional_noise_percentage\n",
        "        })\n",
        "\n",
        "# Create consolidated results table\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(\"\\nConsolidated Evaluation Results:\")\n",
        "print(results_df.groupby(['Noise Fraction (%)', 'Method']).agg({\n",
        "    'Precision': 'mean',\n",
        "    'Recall': 'mean',\n",
        "    'F1 Score': 'mean',\n",
        "    'Noise Detected (%)': 'mean'\n",
        "}).reset_index())\n",
        "\n",
        "# Save all results to first CSV\n",
        "all_results_path = '/content/drive/My Drive/ColabNotebooks/all_results_clustered_optimized_dbscan_gn.csv'\n",
        "try:\n",
        "    acc_df.to_csv(all_results_path, index=False)\n",
        "    print(f\"All results saved to '{all_results_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving all results: {e}\")\n",
        "\n",
        "# Save noisy dataset and results for the last fold\n",
        "try:\n",
        "    noisy_df.to_csv('/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv', index=False)\n",
        "    print(f\"Noisy dataset saved to '/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv'.\")\n",
        "    results_df.to_csv('/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv', index=False)\n",
        "    print(f\"Evaluation results saved to '/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")\n",
        "\n",
        "# Debug: Print summary for the last noise level of the last fold\n",
        "print(\"\\nZ-score stats (noisy dataset, last noise level of last fold):\\n\", noisy_df[['Z_score', 'traditional_Z_score']].describe())\n",
        "print(\"Noise detection summary (hybrid, noisy dataset):\\n\", noisy_df['is_noise'].value_counts())\n",
        "print(\"Noise detection summary (traditional, noisy dataset):\\n\", noisy_df['is_noise_traditional'].value_counts())\n",
        "print(f\"Total noise points introduced (last noise level of last fold): {total_noise_points}\")\n",
        "print(f\"Hybrid method detected: {hybrid_detected} ({hybrid_noise_percentage:.2f}%)\")\n",
        "print(f\"Traditional method detected: {traditional_detected} ({traditional_noise_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4nPSeoqy2ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y89FGc7EwzUA",
        "outputId": "1880e496-367f-49af-86b6-624269ceef9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        timestamp                        sensor_id  \\\n",
            "0        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "1        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "2        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "3        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "4        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "...                           ...                              ...   \n",
            "4605955  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605956  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605957  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605958  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605959  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "\n",
            "         measurement           property  \n",
            "0           1.528132       AttitudeRoll  \n",
            "1          -0.733896      AttitudePitch  \n",
            "2           0.696372        AttitudeYaw  \n",
            "3           0.294894  UserAccelerationX  \n",
            "4          -0.184493  UserAccelerationY  \n",
            "...              ...                ...  \n",
            "4605955    -1.435057      AttitudePitch  \n",
            "4605956     0.377682        AttitudeYaw  \n",
            "4605957     0.043053  UserAccelerationX  \n",
            "4605958     3.977308  UserAccelerationY  \n",
            "4605959     0.174186  UserAccelerationZ  \n",
            "\n",
            "[4605960 rows x 4 columns]\n",
            "Global magnitude stats: Mean = 0.753240, Std = 0.638734\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Subsampled data size: 537362\n",
            "Training set size: 429889, Test set size: 107473\n",
            "Global magnitude stats: Mean = 0.753240, Std = 0.638734\n",
            "Suggested eps based on 95th percentile of k-distances: 0.094\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Best parameters: eps=0.4, min_samples=20, Best Silhouette Score=0.325\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Best parameters: eps=0.4, min_samples=20, Best Silhouette Score=0.325\n",
            "Sample of plausible ranges applied:\n",
            "       activity  min_plausible  max_plausible\n",
            "659589      wlk            0.0            2.0\n",
            "381703      jog            0.0            4.5\n",
            "166480      dws            0.0            2.0\n",
            "144633      ups            0.0            2.5\n",
            "All results saved to '/content/drive/My Drive/ColabNotebooks/all_results_clustered_optimized_dbscan_gn.csv'.\n",
            "\n",
            "Consolidated Evaluation Results:\n",
            "  Algorithm  Noise Fraction (%)                  Method  Precision    Recall  \\\n",
            "0    DBSCAN                 2.0  Hybrid (Cluster-based)   0.257079  0.234385   \n",
            "1    DBSCAN                 2.0    Traditional (Global)   0.166003  0.142685   \n",
            "2    DBSCAN                 5.0  Hybrid (Cluster-based)   0.465585  0.230298   \n",
            "3    DBSCAN                 5.0    Traditional (Global)   0.333956  0.140217   \n",
            "4    DBSCAN                10.0  Hybrid (Cluster-based)   0.645282  0.229013   \n",
            "5    DBSCAN                10.0    Traditional (Global)   0.513512  0.140428   \n",
            "6    DBSCAN                20.0  Hybrid (Cluster-based)   0.805593  0.229515   \n",
            "7    DBSCAN                20.0    Traditional (Global)   0.705690  0.140211   \n",
            "\n",
            "   F1 Score  Noise Detected (%)  \n",
            "0  0.245208           23.438512  \n",
            "1  0.153463           14.268530  \n",
            "2  0.308165           23.029828  \n",
            "3  0.197507           14.021727  \n",
            "4  0.338051           22.901349  \n",
            "5  0.220544           14.042768  \n",
            "6  0.357249           22.951504  \n",
            "7  0.233941           14.021127  \n",
            "Noisy dataset saved to '/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv'.\n",
            "Evaluation results saved to '/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv'.\n",
            "\n",
            "Z-score stats (noisy dataset):\n",
            "              Z_score  traditional_Z_score\n",
            "count  537362.000000        537362.000000\n",
            "mean        0.046056            -0.000690\n",
            "std         1.658664             1.156523\n",
            "min       -21.619067            -6.973136\n",
            "25%        -0.795455            -0.662623\n",
            "50%        -0.192308            -0.261448\n",
            "75%         0.611111             0.401982\n",
            "max        25.127896            14.648906\n",
            "Noise detection summary (hybrid, noisy dataset):\n",
            " is_noise\n",
            "False    506643\n",
            "True      30719\n",
            "Name: count, dtype: int64\n",
            "Noise detection summary (traditional, noisy dataset):\n",
            " is_noise_traditional\n",
            "False    515939\n",
            "True      21423\n",
            "Name: count, dtype: int64\n",
            "Total noise points introduced: 107823\n",
            "Hybrid method detected: 24747 (22.95%)\n",
            "Traditional method detected: 15118 (14.02%)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import gennorm\n",
        "from rdflib import Graph\n",
        "\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths to sensor reading csv file data and knowledge graph\n",
        "data_path = \"/content/drive/My Drive/ColabNotebooks/motion_sensor_readings.csv\"\n",
        "kg_path = \"/content/drive/My Drive/ColabNotebooks/motion_sense_ssn_kg.ttl\"\n",
        "\n",
        "# Load knowledge graph\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(kg_path, format='turtle')\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing knowledge graph: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for participant characteristics\n",
        "query = \"\"\"\n",
        "PREFIX ms: <http://example.org/motion-sense#>\n",
        "SELECT ?p ?code ?age ?gender ?height ?weight\n",
        "WHERE {\n",
        "    ?p a ms:Participant ;\n",
        "       ms:hasCode ?code ;\n",
        "       ms:hasAge ?age ;\n",
        "       ms:hasGender ?gender ;\n",
        "       ms:hasHeightCm ?height ;\n",
        "       ms:hasWeightKg ?weight .\n",
        "}\n",
        "\"\"\"\n",
        "results = g.query(query)\n",
        "participants = []\n",
        "for row in results:\n",
        "    participants.append({\n",
        "        'participant': str(row.code).strip().upper(),\n",
        "        'age': int(row.age),\n",
        "        'gender': int(row.gender),\n",
        "        'height': float(row.height),\n",
        "        'weight': float(row.weight)\n",
        "    })\n",
        "participants_df = pd.DataFrame(participants)\n",
        "\n",
        "if participants_df.empty:\n",
        "    print(\"Error: No participants found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for activity plausible ranges\n",
        "query_ranges = \"\"\"\n",
        "PREFIX activity: <http://example.org/activity-recognition#>\n",
        "SELECT ?activityCode ?min ?max\n",
        "WHERE {\n",
        "    ?act a activity:Activity ;\n",
        "         activity:hasActivityCode ?activityCode ;\n",
        "         activity:hasMinAcceleration ?min ;\n",
        "         activity:hasMaxAcceleration ?max .\n",
        "}\n",
        "\"\"\"\n",
        "results_ranges = g.query(query_ranges)\n",
        "activity_ranges = {}\n",
        "for row in results_ranges:\n",
        "    activity_code = str(row.activityCode).strip().lower()\n",
        "    activity_ranges[activity_code] = {\n",
        "        'min': float(row.min),\n",
        "        'max': float(row.max)\n",
        "    }\n",
        "\n",
        "if not activity_ranges:\n",
        "    print(\"Error: No activity ranges found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Load sensor readings csv file data\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(df)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sensor data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Parse sensor_id with validation\n",
        "def parse_sensor_id(sid):\n",
        "    parts = sid.split('_')\n",
        "    if len(parts) >= 5:\n",
        "        return parts[0].strip().upper(), parts[3], parts[4]\n",
        "    return None, None, None\n",
        "\n",
        "df['participant'], df['activity'], df['sensor_type'] = zip(*df['sensor_id'].apply(parse_sensor_id))\n",
        "df = df.dropna(subset=['participant'])\n",
        "\n",
        "# Filter for accelerometer data\n",
        "df = df[(df['sensor_type'] == 'Accelerometer') &\n",
        "        (df['property'].isin(['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']))]\n",
        "\n",
        "# Convert measurement to numeric, drop invalid entries\n",
        "df['measurement'] = pd.to_numeric(df['measurement'], errors='coerce')\n",
        "df = df.dropna(subset=['measurement'])\n",
        "\n",
        "if df.empty:\n",
        "    print(\"Error: No valid accelerometer data after cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Pivot to get X, Y, Z\n",
        "acc_df = df.pivot_table(index=['timestamp', 'sensor_id', 'participant', 'activity'],\n",
        "                        columns='property', values='measurement', aggfunc='first').reset_index()\n",
        "\n",
        "# Drop rows with missing or invalid acceleration components\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "for col in ['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']:\n",
        "    acc_df[col] = pd.to_numeric(acc_df[col], errors='coerce')\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "\n",
        "if acc_df.empty:\n",
        "    print(\"Error: No valid data after pivot and cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Compute magnitude\n",
        "acc_df['magnitude'] = np.sqrt(acc_df['UserAccelerationX']**2 +\n",
        "                              acc_df['UserAccelerationY']**2 +\n",
        "                              acc_df['UserAccelerationZ']**2)\n",
        "\n",
        "# Compute global mean and std for magnitude (for traditional Z-score)\n",
        "global_magnitude_mean = acc_df['magnitude'].mean()\n",
        "global_magnitude_std = acc_df['magnitude'].std()\n",
        "print(f\"Global magnitude stats: Mean = {global_magnitude_mean:.6f}, Std = {global_magnitude_std:.6f}\")\n",
        "\n",
        "# Compute per-activity stats for reference on cleaned data\n",
        "activity_stats_df = acc_df.groupby('activity')['magnitude'].agg(['mean', 'std', 'count']).reset_index()\n",
        "activity_stats_df.columns = ['activity', 'actual_mean', 'actual_std', 'sample_count']\n",
        "\n",
        "# Ensure sufficient samples and smooth std\n",
        "activity_stats_df = activity_stats_df[activity_stats_df['sample_count'] >= 100]\n",
        "global_activity_std = activity_stats_df['actual_std'].mean()\n",
        "activity_stats_df['actual_std'] = activity_stats_df['actual_std'].apply(lambda x: max(x, global_activity_std * 0.1))\n",
        "\n",
        "# Debug: Print per-activity stats\n",
        "print(\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "\n",
        "# Merge with participant characteristics for clustering\n",
        "merged_df = pd.merge(acc_df, participants_df, on='participant', how='inner')\n",
        "\n",
        "if merged_df.empty:\n",
        "    print(\"Error: No matching participants found after merging.\")\n",
        "    exit(1)\n",
        "\n",
        "# Subsample the data to reduce memory usage\n",
        "subsample_fraction = 0.7  # Use 70% of the data\n",
        "subsampled_df = merged_df.sample(frac=subsample_fraction, random_state=42)\n",
        "print(f\"Subsampled data size: {len(subsampled_df)}\")\n",
        "\n",
        "# Split into training and test sets\n",
        "train_df, test_df = train_test_split(subsampled_df, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "# Prepare data for clustering on training set, including magnitude\n",
        "X_cluster_train = train_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "scaler = RobustScaler()\n",
        "X_numeric_train = scaler.fit_transform(X_cluster_train[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "X_activity_train = pd.get_dummies(X_cluster_train['activity'], prefix='activity')\n",
        "X_scaled_train = np.hstack([X_numeric_train, X_activity_train])\n",
        "\n",
        "# Prepare data for test set\n",
        "X_cluster_test = test_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "X_numeric_test = scaler.transform(X_cluster_test[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "X_activity_test = pd.get_dummies(X_cluster_test['activity'], prefix='activity')\n",
        "X_activity_test = X_activity_test.reindex(columns=X_activity_train.columns, fill_value=0)\n",
        "X_scaled_test = np.hstack([X_numeric_test, X_activity_test])\n",
        "\n",
        "# Generate k-distance plot to determine initial eps\n",
        "min_samples = 50  # Default min_samples\n",
        "neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_scaled_train)\n",
        "distances, indices = neighbors.kneighbors(X_scaled_train)\n",
        "k_distances = np.sort(distances[:, min_samples - 1])\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_distances)\n",
        "plt.title('K-Distance Plot (Distance to {}th Nearest Neighbor)'.format(min_samples))\n",
        "plt.xlabel('Points Sorted by Distance')\n",
        "plt.ylabel('Distance')\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/My Drive/ColabNotebooks/k_distance_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Suggest eps based on the knee point (e.g., 95th percentile of k-distances)\n",
        "suggested_eps = np.percentile(k_distances, 95)\n",
        "print(f\"Global magnitude stats: Mean = {global_magnitude_mean:.6f}, Std = {global_magnitude_std:.6f}\")\n",
        "print(f\"Suggested eps based on 95th percentile of k-distances: {suggested_eps:.3f}\")\n",
        "\n",
        "# Parameter tuning using silhouette score\n",
        "eps_values = np.arange(0.1, 0.5, 0.05)  # Narrower range around suggested eps and prior best\n",
        "min_samples_values = [5, 10, 15, 20]  # Lower range to capture smaller noise clusters\n",
        "best_score = -1\n",
        "best_params = {'eps': 0, 'min_samples': 0}\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X_scaled_train)\n",
        "        if len(np.unique(labels)) > 1 and -1 in labels:\n",
        "            score = silhouette_score(X_scaled_train, labels, sample_size=5000)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "print(f\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "print(f\"Best parameters: eps={best_params['eps']:.1f}, min_samples={best_params['min_samples']}, Best Silhouette Score={best_score:.3f}\")\n",
        "print(f\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "print(f\"Best parameters: eps={best_params['eps']:.1f}, min_samples={best_params['min_samples']}, Best Silhouette Score={best_score:.3f}\")\n",
        "\n",
        "# Use best parameters for clustering\n",
        "dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "train_cluster_labels = dbscan.fit_predict(X_scaled_train)\n",
        "test_cluster_labels = dbscan.fit_predict(X_scaled_test)\n",
        "\n",
        "# Add cluster labels to train and test dataframes\n",
        "train_df['cluster'] = train_cluster_labels\n",
        "test_df['cluster'] = test_cluster_labels\n",
        "\n",
        "# Combine train and test dataframes\n",
        "acc_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "# Assign cluster mean and std for Z-score calculation using DBSCAN clusters\n",
        "cluster_means = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].mean().to_dict()\n",
        "cluster_stds = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].std().to_dict()\n",
        "acc_df['cluster_mu'] = acc_df['cluster'].map(cluster_means)\n",
        "acc_df['cluster_sigma'] = acc_df['cluster'].map(cluster_stds).fillna(global_activity_std)\n",
        "acc_df.loc[acc_df['cluster'] == -1, 'cluster_mu'] = global_magnitude_mean\n",
        "acc_df.loc[acc_df['cluster'] == -1, 'cluster_sigma'] = global_magnitude_std\n",
        "\n",
        "# Compute Z-scores using cluster-based statistics\n",
        "acc_df['Z_score'] = (acc_df['magnitude'] - acc_df['cluster_mu']) / acc_df['cluster_sigma'].replace(0, np.nan)\n",
        "acc_df['traditional_Z_score'] = (acc_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "\n",
        "# Add global mean and std to output\n",
        "acc_df['global_magnitude_mean'] = global_magnitude_mean\n",
        "acc_df['global_magnitude_std'] = global_magnitude_std\n",
        "\n",
        "# Apply activity-specific plausible ranges\n",
        "acc_df['activity'] = acc_df['activity'].str.lower()\n",
        "default_min_plausible = 0\n",
        "default_max_plausible = 10\n",
        "acc_df['min_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'min': default_min_plausible})['min'])\n",
        "acc_df['max_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'max': default_max_plausible})['max'])\n",
        "\n",
        "# Debug: Print a sample of plausible ranges applied\n",
        "print(\"Sample of plausible ranges applied:\")\n",
        "print(acc_df[['activity', 'min_plausible', 'max_plausible']].drop_duplicates())\n",
        "\n",
        "# Determine if observations are outside plausible range\n",
        "acc_df['outside_plausible'] = (acc_df['magnitude'] < acc_df['min_plausible']) | (acc_df['magnitude'] > acc_df['max_plausible'])\n",
        "\n",
        "# Determine anomaly\n",
        "threshold = 2\n",
        "acc_df['is_anomaly'] = np.abs(acc_df['Z_score']) > threshold\n",
        "acc_df['is_anomaly_traditional'] = np.abs(acc_df['traditional_Z_score']) > threshold\n",
        "\n",
        "# Label noise\n",
        "acc_df['is_noise'] = acc_df['is_anomaly'] & acc_df['outside_plausible']\n",
        "acc_df['is_noise_traditional'] = acc_df['is_anomaly_traditional'] & acc_df['outside_plausible']\n",
        "\n",
        "# Round numerical columns in acc_df to 2 decimal places\n",
        "numerical_cols = acc_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "acc_df[numerical_cols] = acc_df[numerical_cols].round(2)\n",
        "\n",
        "# Save all results to first CSV\n",
        "all_results_path = '/content/drive/My Drive/ColabNotebooks/all_results_clustered_optimized_dbscan_gn.csv'\n",
        "try:\n",
        "    acc_df.to_csv(all_results_path, index=False)\n",
        "    print(f\"All results saved to '{all_results_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving all results: {e}\")\n",
        "\n",
        "# Function to introduce Generalized Normal noise\n",
        "def introduce_noise(df, noise_level=0.1, noise_fraction=0.05):\n",
        "    noisy_df = df.copy()\n",
        "    mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "    noise = gennorm.rvs(beta=1.5, loc=0, scale=noise_level, size=len(noisy_df))\n",
        "    noisy_df['magnitude'] = noisy_df['magnitude'] + noise * mask\n",
        "    noisy_df['is_true_noise'] = mask.astype(int)  # Ground truth: 1 for noisy, 0 for normal\n",
        "    return noisy_df\n",
        "\n",
        "# List of noise fractions to test\n",
        "noise_fractions = [0.02, 0.05, 0.10, 0.20]\n",
        "results_list = []\n",
        "\n",
        "# Iterate over noise levels\n",
        "for noise_fraction in noise_fractions:\n",
        "    # Introduce noise to the dataset\n",
        "    noisy_df = introduce_noise(acc_df, noise_level=1.5 * global_magnitude_std, noise_fraction=noise_fraction)\n",
        "\n",
        "    # Apply anomaly detection - Hybrid method\n",
        "    noisy_df['Z_score'] = (noisy_df['magnitude'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma'].replace(0, np.nan)\n",
        "    noisy_df['is_anomaly'] = np.abs(noisy_df['Z_score']) > threshold\n",
        "    noisy_df['outside_plausible'] = (noisy_df['magnitude'] < noisy_df['min_plausible']) | (noisy_df['magnitude'] > noisy_df['max_plausible'])\n",
        "    noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "\n",
        "    # Apply anomaly detection - Traditional method\n",
        "    noisy_df['traditional_Z_score'] = (noisy_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "    noisy_df['is_anomaly_traditional'] = np.abs(noisy_df['traditional_Z_score']) > threshold\n",
        "    noisy_df['is_noise_traditional'] = noisy_df['is_anomaly_traditional'] & noisy_df['outside_plausible']\n",
        "\n",
        "    # Evaluate performance\n",
        "    y_true = noisy_df['is_true_noise']\n",
        "    y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "    y_pred_traditional = noisy_df['is_noise_traditional'].astype(int)\n",
        "\n",
        "    precision_hybrid = precision_score(y_true, y_pred_hybrid)\n",
        "    recall_hybrid = recall_score(y_true, y_pred_hybrid)\n",
        "    f1_hybrid = f1_score(y_true, y_pred_hybrid)\n",
        "\n",
        "    precision_traditional = precision_score(y_true, y_pred_traditional)\n",
        "    recall_traditional = recall_score(y_true, y_pred_traditional)\n",
        "    f1_traditional = f1_score(y_true, y_pred_traditional)\n",
        "\n",
        "    # Calculate percentage of noise detected\n",
        "    total_noise_points = y_true.sum()\n",
        "    hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "    traditional_detected = y_pred_traditional[y_true == 1].sum()\n",
        "    hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "    traditional_noise_percentage = (traditional_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "    # Store results for this noise level\n",
        "    results_list.append({\n",
        "        'Algorithm': 'DBSCAN',\n",
        "        'Noise Fraction (%)': noise_fraction * 100,\n",
        "        'Method': 'Hybrid (Cluster-based)',\n",
        "        'Precision': precision_hybrid,\n",
        "        'Recall': recall_hybrid,\n",
        "        'F1 Score': f1_hybrid,\n",
        "        'Noise Detected (%)': hybrid_noise_percentage\n",
        "    })\n",
        "    results_list.append({\n",
        "        'Algorithm': 'DBSCAN',\n",
        "        'Noise Fraction (%)': noise_fraction * 100,\n",
        "        'Method': 'Traditional (Global)',\n",
        "        'Precision': precision_traditional,\n",
        "        'Recall': recall_traditional,\n",
        "        'F1 Score': f1_traditional,\n",
        "        'Noise Detected (%)': traditional_noise_percentage\n",
        "    })\n",
        "\n",
        "# Create consolidated results table\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(\"\\nConsolidated Evaluation Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Save noisy dataset and results\n",
        "try:\n",
        "    noisy_df.to_csv('/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv', index=False)\n",
        "    print(f\"Noisy dataset saved to '/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv'.\")\n",
        "    results_df.to_csv('/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv', index=False)\n",
        "    print(f\"Evaluation results saved to '/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")\n",
        "\n",
        "# Debug: Print summary for the last noise level\n",
        "print(\"\\nZ-score stats (noisy dataset):\\n\", noisy_df[['Z_score', 'traditional_Z_score']].describe())\n",
        "print(\"Noise detection summary (hybrid, noisy dataset):\\n\", noisy_df['is_noise'].value_counts())\n",
        "print(\"Noise detection summary (traditional, noisy dataset):\\n\", noisy_df['is_noise_traditional'].value_counts())\n",
        "print(f\"Total noise points introduced: {total_noise_points}\")\n",
        "print(f\"Hybrid method detected: {hybrid_detected} ({hybrid_noise_percentage:.2f}%)\")\n",
        "print(f\"Traditional method detected: {traditional_detected} ({traditional_noise_percentage:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import gennorm\n",
        "from rdflib import Graph\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths to sensor reading csv file data and knowledge graph\n",
        "data_path = \"/content/drive/My Drive/ColabNotebooks/motion_sensor_readings.csv\"\n",
        "kg_path = \"/content/drive/My Drive/ColabNotebooks/motion_sense_ssn_kg.ttl\"\n",
        "\n",
        "# Load knowledge graph\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(kg_path, format='turtle')\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing knowledge graph: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for participant characteristics\n",
        "query = \"\"\"\n",
        "PREFIX ms: <http://example.org/motion-sense#>\n",
        "SELECT ?p ?code ?age ?gender ?height ?weight\n",
        "WHERE {\n",
        "    ?p a ms:Participant ;\n",
        "       ms:hasCode ?code ;\n",
        "       ms:hasAge ?age ;\n",
        "       ms:hasGender ?gender ;\n",
        "       ms:hasHeightCm ?height ;\n",
        "       ms:hasWeightKg ?weight .\n",
        "}\n",
        "\"\"\"\n",
        "results = g.query(query)\n",
        "participants = []\n",
        "for row in results:\n",
        "    participants.append({\n",
        "        'participant': str(row.code).strip().upper(),\n",
        "        'age': int(row.age),\n",
        "        'gender': int(row.gender),\n",
        "        'height': float(row.height),\n",
        "        'weight': float(row.weight)\n",
        "    })\n",
        "participants_df = pd.DataFrame(participants)\n",
        "\n",
        "if participants_df.empty:\n",
        "    print(\"Error: No participants found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for activity plausible ranges\n",
        "query_ranges = \"\"\"\n",
        "PREFIX activity: <http://example.org/activity-recognition#>\n",
        "SELECT ?activityCode ?min ?max\n",
        "WHERE {\n",
        "    ?act a activity:Activity ;\n",
        "         activity:hasActivityCode ?activityCode ;\n",
        "         activity:hasMinAcceleration ?min ;\n",
        "         activity:hasMaxAcceleration ?max .\n",
        "}\n",
        "\"\"\"\n",
        "results_ranges = g.query(query_ranges)\n",
        "activity_ranges = {}\n",
        "for row in results_ranges:\n",
        "    activity_code = str(row.activityCode).strip().lower()\n",
        "    activity_ranges[activity_code] = {\n",
        "        'min': float(row.min),\n",
        "        'max': float(row.max)\n",
        "    }\n",
        "\n",
        "if not activity_ranges:\n",
        "    print(\"Error: No activity ranges found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Load sensor readings csv file data\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(df)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sensor data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Parse sensor_id with validation\n",
        "def parse_sensor_id(sid):\n",
        "    parts = sid.split('_')\n",
        "    if len(parts) >= 5:\n",
        "        return parts[0].strip().upper(), parts[3], parts[4]\n",
        "    return None, None, None\n",
        "\n",
        "df['participant'], df['activity'], df['sensor_type'] = zip(*df['sensor_id'].apply(parse_sensor_id))\n",
        "df = df.dropna(subset=['participant'])\n",
        "\n",
        "# Filter for accelerometer data\n",
        "df = df[(df['sensor_type'] == 'Accelerometer') &\n",
        "        (df['property'].isin(['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']))]\n",
        "\n",
        "# Convert measurement to numeric, drop invalid entries\n",
        "df['measurement'] = pd.to_numeric(df['measurement'], errors='coerce')\n",
        "df = df.dropna(subset=['measurement'])\n",
        "\n",
        "if df.empty:\n",
        "    print(\"Error: No valid accelerometer data after cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Pivot to get X, Y, Z\n",
        "acc_df = df.pivot_table(index=['timestamp', 'sensor_id', 'participant', 'activity'],\n",
        "                        columns='property', values='measurement', aggfunc='first').reset_index()\n",
        "\n",
        "# Drop rows with missing or invalid acceleration components\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "for col in ['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']:\n",
        "    acc_df[col] = pd.to_numeric(acc_df[col], errors='coerce')\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "\n",
        "if acc_df.empty:\n",
        "    print(\"Error: No valid data after pivot and cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Compute magnitude\n",
        "acc_df['magnitude'] = np.sqrt(acc_df['UserAccelerationX']**2 +\n",
        "                              acc_df['UserAccelerationY']**2 +\n",
        "                              acc_df['UserAccelerationZ']**2)\n",
        "\n",
        "# Compute global mean and std for magnitude (for traditional Z-score)\n",
        "global_magnitude_mean = acc_df['magnitude'].mean()\n",
        "global_magnitude_std = acc_df['magnitude'].std()\n",
        "print(f\"Global magnitude stats: Mean = {global_magnitude_mean:.6f}, Std = {global_magnitude_std:.6f}\")\n",
        "\n",
        "# Compute per-activity stats for reference on cleaned data\n",
        "activity_stats_df = acc_df.groupby('activity')['magnitude'].agg(['mean', 'std', 'count']).reset_index()\n",
        "activity_stats_df.columns = ['activity', 'actual_mean', 'actual_std', 'sample_count']\n",
        "\n",
        "# Ensure sufficient samples and smooth std\n",
        "activity_stats_df = activity_stats_df[activity_stats_df['sample_count'] >= 100]\n",
        "global_activity_std = activity_stats_df['actual_std'].mean()\n",
        "activity_stats_df['actual_std'] = activity_stats_df['actual_std'].apply(lambda x: max(x, global_activity_std * 0.1))\n",
        "\n",
        "# Debug: Print per-activity stats\n",
        "print(\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "\n",
        "# Merge with participant characteristics for clustering\n",
        "merged_df = pd.merge(acc_df, participants_df, on='participant', how='inner')\n",
        "\n",
        "if merged_df.empty:\n",
        "    print(\"Error: No matching participants found after merging.\")\n",
        "    exit(1)\n",
        "\n",
        "# Use full dataset (no subsampling)\n",
        "subsampled_df = merged_df  # Full dataset\n",
        "print(f\"Full dataset size: {len(subsampled_df)}\")\n",
        "\n",
        "# Split into training and test sets\n",
        "train_df, test_df = train_test_split(subsampled_df, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")\n",
        "\n",
        "# Prepare data for clustering on training set, including magnitude\n",
        "X_cluster_train = train_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "scaler = RobustScaler()\n",
        "X_numeric_train = scaler.fit_transform(X_cluster_train[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "X_activity_train = pd.get_dummies(X_cluster_train['activity'], prefix='activity')\n",
        "X_scaled_train = np.hstack([X_numeric_train, X_activity_train])\n",
        "\n",
        "# Prepare data for test set\n",
        "X_cluster_test = test_df[['age', 'gender', 'height', 'weight', 'magnitude', 'activity']]\n",
        "X_numeric_test = scaler.transform(X_cluster_test[['age', 'gender', 'height', 'weight', 'magnitude']])\n",
        "X_activity_test = pd.get_dummies(X_cluster_test['activity'], prefix='activity')\n",
        "X_activity_test = X_activity_test.reindex(columns=X_activity_train.columns, fill_value=0)\n",
        "X_scaled_test = np.hstack([X_numeric_test, X_activity_test])\n",
        "\n",
        "# Generate k-distance plot to determine initial eps\n",
        "min_samples = 50  # Default min_samples\n",
        "neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_scaled_train)\n",
        "distances, indices = neighbors.kneighbors(X_scaled_train)\n",
        "k_distances = np.sort(distances[:, min_samples - 1])\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_distances)\n",
        "plt.title('K-Distance Plot (Distance to {}th Nearest Neighbor)'.format(min_samples))\n",
        "plt.xlabel('Points Sorted by Distance')\n",
        "plt.ylabel('Distance')\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/My Drive/ColabNotebooks/k_distance_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Suggest eps based on the knee point (e.g., 95th percentile of k-distances)\n",
        "suggested_eps = np.percentile(k_distances, 95)\n",
        "print(f\"Global magnitude stats: Mean = {global_magnitude_mean:.6f}, Std = {global_magnitude_std:.6f}\")\n",
        "print(f\"Suggested eps based on 95th percentile of k-distances: {suggested_eps:.3f}\")\n",
        "\n",
        "# Parameter tuning using silhouette score\n",
        "eps_values = np.arange(0.05, 0.2, 0.01)  # Tighter range around suggested eps\n",
        "min_samples_values = [3, 5, 10, 15]  # Lower range for smaller clusters\n",
        "best_score = -1\n",
        "best_params = {'eps': 0, 'min_samples': 0}\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X_scaled_train)\n",
        "        if len(np.unique(labels)) > 1 and -1 in labels:\n",
        "            score = silhouette_score(X_scaled_train, labels, sample_size=5000)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "print(f\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "print(f\"Best parameters: eps={best_params['eps']:.2f}, min_samples={best_params['min_samples']}, Best Silhouette Score={best_score:.3f}\")\n",
        "print(f\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "print(f\"Best parameters: eps={best_params['eps']:.2f}, min_samples={best_params['min_samples']}, Best Silhouette Score={best_score:.3f}\")\n",
        "\n",
        "# Use best parameters for clustering\n",
        "dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "train_cluster_labels = dbscan.fit_predict(X_scaled_train)\n",
        "test_cluster_labels = dbscan.fit_predict(X_scaled_test)\n",
        "\n",
        "# Add cluster labels to train and test dataframes\n",
        "train_df['cluster'] = train_cluster_labels\n",
        "test_df['cluster'] = test_cluster_labels\n",
        "\n",
        "# Combine train and test dataframes\n",
        "acc_df = pd.concat([train_df, test_df], axis=0)\n",
        "\n",
        "# Assign cluster mean and std for Z-score calculation using DBSCAN clusters\n",
        "cluster_means = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].mean().to_dict()\n",
        "cluster_stds = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].std().to_dict()\n",
        "acc_df['cluster_mu'] = acc_df['cluster'].map(cluster_means)\n",
        "acc_df['cluster_sigma'] = acc_df['cluster'].map(cluster_stds).fillna(global_activity_std)\n",
        "acc_df.loc[acc_df['cluster'] == -1, 'cluster_mu'] = global_magnitude_mean\n",
        "acc_df.loc[acc_df['cluster'] == -1, 'cluster_sigma'] = global_magnitude_std\n",
        "\n",
        "# Compute Z-scores using cluster-based statistics\n",
        "acc_df['Z_score'] = (acc_df['magnitude'] - acc_df['cluster_mu']) / acc_df['cluster_sigma'].replace(0, np.nan)\n",
        "acc_df['traditional_Z_score'] = (acc_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "\n",
        "# Add global mean and std to output\n",
        "acc_df['global_magnitude_mean'] = global_magnitude_mean\n",
        "acc_df['global_magnitude_std'] = global_magnitude_std\n",
        "\n",
        "# Apply activity-specific plausible ranges\n",
        "acc_df['activity'] = acc_df['activity'].str.lower()\n",
        "default_min_plausible = 0\n",
        "default_max_plausible = 10\n",
        "acc_df['min_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'min': default_min_plausible})['min'])\n",
        "acc_df['max_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'max': default_max_plausible})['max'])\n",
        "\n",
        "# Debug: Print a sample of plausible ranges applied\n",
        "print(\"Sample of plausible ranges applied:\")\n",
        "print(acc_df[['activity', 'min_plausible', 'max_plausible']].drop_duplicates())\n",
        "\n",
        "# Determine if observations are outside plausible range\n",
        "acc_df['outside_plausible'] = (acc_df['magnitude'] < acc_df['min_plausible']) | (acc_df['magnitude'] > acc_df['max_plausible'])\n",
        "\n",
        "# Determine anomaly\n",
        "threshold = 2\n",
        "acc_df['is_anomaly'] = np.abs(acc_df['Z_score']) > threshold\n",
        "acc_df['is_anomaly_traditional'] = np.abs(acc_df['traditional_Z_score']) > threshold\n",
        "\n",
        "# Label noise\n",
        "acc_df['is_noise'] = acc_df['is_anomaly'] & acc_df['outside_plausible']\n",
        "acc_df['is_noise_traditional'] = acc_df['is_anomaly_traditional'] & acc_df['outside_plausible']\n",
        "\n",
        "# Round numerical columns in acc_df to 2 decimal places\n",
        "numerical_cols = acc_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "acc_df[numerical_cols] = acc_df[numerical_cols].round(2)\n",
        "\n",
        "# Save all results to first CSV\n",
        "all_results_path = '/content/drive/My Drive/ColabNotebooks/all_results_clustered_optimized_dbscan_gn.csv'\n",
        "try:\n",
        "    acc_df.to_csv(all_results_path, index=False)\n",
        "    print(f\"All results saved to '{all_results_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving all results: {e}\")\n",
        "\n",
        "# Function to introduce Generalized Normal noise\n",
        "def introduce_noise(df, noise_level=0.1, noise_fraction=0.05):\n",
        "    noisy_df = df.copy()\n",
        "    mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "    noise = gennorm.rvs(beta=1.5, loc=0, scale=noise_level, size=len(noisy_df))\n",
        "    noisy_df['magnitude'] = noisy_df['magnitude'] + noise * mask\n",
        "    noisy_df['is_true_noise'] = mask.astype(int)  # Ground truth: 1 for noisy, 0 for normal\n",
        "    return noisy_df\n",
        "\n",
        "# List of noise fractions to test\n",
        "noise_fractions = [0.02, 0.05, 0.10, 0.20]\n",
        "results_list = []\n",
        "\n",
        "# Iterate over noise levels\n",
        "for noise_fraction in noise_fractions:\n",
        "    # Introduce noise to the dataset\n",
        "    noisy_df = introduce_noise(acc_df, noise_level=1.5 * global_magnitude_std, noise_fraction=noise_fraction)\n",
        "\n",
        "    # Apply anomaly detection - Hybrid method\n",
        "    noisy_df['Z_score'] = (noisy_df['magnitude'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma'].replace(0, np.nan)\n",
        "    noisy_df['is_anomaly'] = np.abs(noisy_df['Z_score']) > threshold\n",
        "    noisy_df['outside_plausible'] = (noisy_df['magnitude'] < noisy_df['min_plausible']) | (noisy_df['magnitude'] > noisy_df['max_plausible'])\n",
        "    noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "\n",
        "    # Apply anomaly detection - Traditional method\n",
        "    noisy_df['traditional_Z_score'] = (noisy_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "    noisy_df['is_anomaly_traditional'] = np.abs(noisy_df['traditional_Z_score']) > threshold\n",
        "    noisy_df['is_noise_traditional'] = noisy_df['is_anomaly_traditional'] & noisy_df['outside_plausible']\n",
        "\n",
        "    # Evaluate performance\n",
        "    y_true = noisy_df['is_true_noise']\n",
        "    y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "    y_pred_traditional = noisy_df['is_noise_traditional'].astype(int)\n",
        "\n",
        "    precision_hybrid = precision_score(y_true, y_pred_hybrid)\n",
        "    recall_hybrid = recall_score(y_true, y_pred_hybrid)\n",
        "    f1_hybrid = f1_score(y_true, y_pred_hybrid)\n",
        "\n",
        "    precision_traditional = precision_score(y_true, y_pred_traditional)\n",
        "    recall_traditional = recall_score(y_true, y_pred_traditional)\n",
        "    f1_traditional = f1_score(y_true, y_pred_traditional)\n",
        "\n",
        "    # Calculate percentage of noise detected\n",
        "    total_noise_points = y_true.sum()\n",
        "    hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "    traditional_detected = y_pred_traditional[y_true == 1].sum()\n",
        "    hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "    traditional_noise_percentage = (traditional_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "    # Store results for this noise level\n",
        "    results_list.append({\n",
        "        'Algorithm': 'DBSCAN',\n",
        "        'Noise Fraction (%)': noise_fraction * 100,\n",
        "        'Method': 'Hybrid (Cluster-based)',\n",
        "        'Precision': precision_hybrid,\n",
        "        'Recall': recall_hybrid,\n",
        "        'F1 Score': f1_hybrid,\n",
        "        'Noise Detected (%)': hybrid_noise_percentage\n",
        "    })\n",
        "    results_list.append({\n",
        "        'Algorithm': 'DBSCAN',\n",
        "        'Noise Fraction (%)': noise_fraction * 100,\n",
        "        'Method': 'Traditional (Global)',\n",
        "        'Precision': precision_traditional,\n",
        "        'Recall': recall_traditional,\n",
        "        'F1 Score': f1_traditional,\n",
        "        'Noise Detected (%)': traditional_noise_percentage\n",
        "    })\n",
        "\n",
        "# Create consolidated results table\n",
        "results_df = pd.DataFrame(results_list)\n",
        "print(\"\\nConsolidated Evaluation Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Save noisy dataset and results\n",
        "try:\n",
        "    noisy_df.to_csv('/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv', index=False)\n",
        "    print(f\"Noisy dataset saved to '/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv'.\")\n",
        "    results_df.to_csv('/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv', index=False)\n",
        "    print(f\"Evaluation results saved to '/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")\n",
        "\n",
        "# Debug: Print summary for the last noise level\n",
        "print(\"\\nZ-score stats (noisy dataset):\\n\", noisy_df[['Z_score', 'traditional_Z_score']].describe())\n",
        "print(\"Noise detection summary (hybrid, noisy dataset):\\n\", noisy_df['is_noise'].value_counts())\n",
        "print(\"Noise detection summary (traditional, noisy dataset):\\n\", noisy_df['is_noise_traditional'].value_counts())\n",
        "print(f\"Total noise points introduced: {total_noise_points}\")\n",
        "print(f\"Hybrid method detected: {hybrid_detected} ({hybrid_noise_percentage:.2f}%)\")\n",
        "print(f\"Traditional method detected: {traditional_detected} ({traditional_noise_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERJJyNJ7z5nz",
        "outputId": "cfaa3308-bb14-4cff-ca7e-7ea384c534bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        timestamp                        sensor_id  \\\n",
            "0        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "1        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "2        2017-10-01T12:00:00.000Z       P001_Trial_1_dws_Gyroscope   \n",
            "3        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "4        2017-10-01T12:00:00.000Z   P001_Trial_1_dws_Accelerometer   \n",
            "...                           ...                              ...   \n",
            "4605955  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605956  2017-10-01T16:15:53.180Z      P024_Trial_16_jog_Gyroscope   \n",
            "4605957  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605958  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "4605959  2017-10-01T16:15:53.180Z  P024_Trial_16_jog_Accelerometer   \n",
            "\n",
            "         measurement           property  \n",
            "0           1.528132       AttitudeRoll  \n",
            "1          -0.733896      AttitudePitch  \n",
            "2           0.696372        AttitudeYaw  \n",
            "3           0.294894  UserAccelerationX  \n",
            "4          -0.184493  UserAccelerationY  \n",
            "...              ...                ...  \n",
            "4605955    -1.435057      AttitudePitch  \n",
            "4605956     0.377682        AttitudeYaw  \n",
            "4605957     0.043053  UserAccelerationX  \n",
            "4605958     3.977308  UserAccelerationY  \n",
            "4605959     0.174186  UserAccelerationZ  \n",
            "\n",
            "[4605960 rows x 4 columns]\n",
            "Global magnitude stats: Mean = 0.753240, Std = 0.638734\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Full dataset size: 767660\n",
            "Training set size: 614128, Test set size: 153532\n",
            "Global magnitude stats: Mean = 0.753240, Std = 0.638734\n",
            "Suggested eps based on 95th percentile of k-distances: 0.066\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Best parameters: eps=0.15, min_samples=15, Best Silhouette Score=0.280\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Best parameters: eps=0.15, min_samples=15, Best Silhouette Score=0.280\n",
            "Sample of plausible ranges applied:\n",
            "       activity  min_plausible  max_plausible\n",
            "100203      jog            0.0            4.5\n",
            "304006      wlk            0.0            2.0\n",
            "360630      ups            0.0            2.5\n",
            "34387       dws            0.0            2.0\n",
            "All results saved to '/content/drive/My Drive/ColabNotebooks/all_results_clustered_optimized_dbscan_gn.csv'.\n",
            "\n",
            "Consolidated Evaluation Results:\n",
            "  Algorithm  Noise Fraction (%)                  Method  Precision    Recall  \\\n",
            "0    DBSCAN                 2.0  Hybrid (Cluster-based)   0.266287  0.240762   \n",
            "1    DBSCAN                 2.0    Traditional (Global)   0.163460  0.138921   \n",
            "2    DBSCAN                 5.0  Hybrid (Cluster-based)   0.479666  0.238674   \n",
            "3    DBSCAN                 5.0    Traditional (Global)   0.337249  0.140950   \n",
            "4    DBSCAN                10.0  Hybrid (Cluster-based)   0.659178  0.239909   \n",
            "5    DBSCAN                10.0    Traditional (Global)   0.516547  0.141719   \n",
            "6    DBSCAN                20.0  Hybrid (Cluster-based)   0.810896  0.236484   \n",
            "7    DBSCAN                20.0    Traditional (Global)   0.702441  0.139397   \n",
            "\n",
            "   F1 Score  Noise Detected (%)  \n",
            "0  0.252882           24.076220  \n",
            "1  0.150195           13.892108  \n",
            "2  0.318746           23.867415  \n",
            "3  0.198809           14.094971  \n",
            "4  0.351786           23.990946  \n",
            "5  0.222416           14.171889  \n",
            "6  0.366179           23.648420  \n",
            "7  0.232629           13.939670  \n",
            "Noisy dataset saved to '/content/drive/My Drive/ColabNotebooks/noisy_results_dbscan_gn.csv'.\n",
            "Evaluation results saved to '/content/drive/My Drive/ColabNotebooks/evaluation_results_dbscan_gn.csv'.\n",
            "\n",
            "Z-score stats (noisy dataset):\n",
            "              Z_score  traditional_Z_score\n",
            "count  767660.000000        767660.000000\n",
            "mean        0.015082             0.001422\n",
            "std         2.520272             1.154552\n",
            "min      -102.162797            -9.100584\n",
            "25%        -0.812500            -0.662623\n",
            "50%        -0.173077            -0.257228\n",
            "75%         0.668772             0.401982\n",
            "max        43.608400            14.648906\n",
            "Noise detection summary (hybrid, noisy dataset):\n",
            " is_noise\n",
            "False    722801\n",
            "True      44859\n",
            "Name: count, dtype: int64\n",
            "Noise detection summary (traditional, noisy dataset):\n",
            " is_noise_traditional\n",
            "False    737135\n",
            "True      30525\n",
            "Name: count, dtype: int64\n",
            "Total noise points introduced: 153820\n",
            "Hybrid method detected: 36376 (23.65%)\n",
            "Traditional method detected: 21442 (13.94%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdflib import Graph\n",
        "from google.colab import drive\n",
        "from scipy.stats import gennorm\n",
        "import itertools\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Paths to sensor reading csv file data and knowledge graph\n",
        "data_path = \"/content/drive/My Drive/ColabNotebooks/motion_sensor_readings.csv\"\n",
        "kg_path = \"/content/drive/My Drive/ColabNotebooks/motion_sense_ssn_kg.ttl\"\n",
        "\n",
        "# Load knowledge graph\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(kg_path, format='turtle')\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing knowledge graph: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for participant characteristics\n",
        "query = \"\"\"\n",
        "PREFIX ms: <http://example.org/motion-sense#>\n",
        "SELECT ?p ?code ?age ?gender ?height ?weight\n",
        "WHERE {\n",
        "    ?p a ms:Participant ;\n",
        "       ms:hasCode ?code ;\n",
        "       ms:hasAge ?age ;\n",
        "       ms:hasGender ?gender ;\n",
        "       ms:hasHeightCm ?height ;\n",
        "       ms:hasWeightKg ?weight .\n",
        "}\n",
        "\"\"\"\n",
        "results = g.query(query)\n",
        "participants = []\n",
        "for row in results:\n",
        "    participants.append({\n",
        "        'participant': str(row.code).strip().upper(),\n",
        "        'age': int(row.age),\n",
        "        'gender': int(row.gender),\n",
        "        'height': float(row.height),\n",
        "        'weight': float(row.weight)\n",
        "    })\n",
        "participants_df = pd.DataFrame(participants)\n",
        "\n",
        "if participants_df.empty:\n",
        "    print(\"Error: No participants found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Query for activity plausible ranges\n",
        "query_ranges = \"\"\"\n",
        "PREFIX activity: <http://example.org/activity-recognition#>\n",
        "SELECT ?activityCode ?min ?max\n",
        "WHERE {\n",
        "    ?act a activity:Activity ;\n",
        "         activity:hasActivityCode ?activityCode ;\n",
        "         activity:hasMinAcceleration ?min ;\n",
        "         activity:hasMaxAcceleration ?max .\n",
        "}\n",
        "\"\"\"\n",
        "results_ranges = g.query(query_ranges)\n",
        "activity_ranges = {}\n",
        "for row in results_ranges:\n",
        "    activity_code = str(row.activityCode).strip().lower()\n",
        "    activity_ranges[activity_code] = {\n",
        "        'min': float(row.min),\n",
        "        'max': float(row.max)\n",
        "    }\n",
        "\n",
        "if not activity_ranges:\n",
        "    print(\"Error: No activity ranges found in knowledge graph.\")\n",
        "    exit(1)\n",
        "\n",
        "# Load sensor readings csv file data\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sensor data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Parse sensor_id with validation\n",
        "def parse_sensor_id(sid):\n",
        "    parts = sid.split('_')\n",
        "    if len(parts) >= 5:\n",
        "        return parts[0].strip().upper(), parts[3], parts[4]\n",
        "    return None, None, None\n",
        "\n",
        "df['participant'], df['activity'], df['sensor_type'] = zip(*df['sensor_id'].apply(parse_sensor_id))\n",
        "df = df.dropna(subset=['participant'])\n",
        "\n",
        "# Filter for accelerometer data\n",
        "df = df[(df['sensor_type'] == 'Accelerometer') &\n",
        "        (df['property'].isin(['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']))]\n",
        "\n",
        "# Convert measurement to numeric, drop invalid entries\n",
        "df['measurement'] = pd.to_numeric(df['measurement'], errors='coerce')\n",
        "df = df.dropna(subset=['measurement'])\n",
        "\n",
        "if df.empty:\n",
        "    print(\"Error: No valid accelerometer data after cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Pivot to get X, Y, Z\n",
        "acc_df = df.pivot_table(index=['timestamp', 'sensor_id', 'participant', 'activity'],\n",
        "                        columns='property', values='measurement', aggfunc='first').reset_index()\n",
        "\n",
        "# Drop rows with missing or invalid acceleration components\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "for col in ['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ']:\n",
        "    acc_df[col] = pd.to_numeric(acc_df[col], errors='coerce')\n",
        "acc_df = acc_df.dropna(subset=['UserAccelerationX', 'UserAccelerationY', 'UserAccelerationZ'])\n",
        "\n",
        "if acc_df.empty:\n",
        "    print(\"Error: No valid data after pivot and cleaning.\")\n",
        "    exit(1)\n",
        "\n",
        "# Compute magnitude\n",
        "acc_df['magnitude'] = np.sqrt(acc_df['UserAccelerationX']**2 +\n",
        "                              acc_df['UserAccelerationY']**2 +\n",
        "                              acc_df['UserAccelerationZ']**2)\n",
        "\n",
        "# Compute global mean and std for magnitude (for traditional Z-score)\n",
        "global_magnitude_mean = acc_df['magnitude'].mean()\n",
        "global_magnitude_std = acc_df['magnitude'].std()\n",
        "print(f\"Global magnitude stats: Mean = {global_magnitude_mean:.6f}, Std = {global_magnitude_std:.6f}\")\n",
        "\n",
        "# Compute per-activity stats for reference on cleaned data\n",
        "activity_stats_df = acc_df.groupby('activity')['magnitude'].agg(['mean', 'std', 'count']).reset_index()\n",
        "activity_stats_df.columns = ['activity', 'actual_mean', 'actual_std', 'sample_count']\n",
        "\n",
        "# Ensure sufficient samples and smooth std\n",
        "activity_stats_df = activity_stats_df[activity_stats_df['sample_count'] >= 100]\n",
        "global_activity_std = activity_stats_df['actual_std'].mean()\n",
        "activity_stats_df['actual_std'] = activity_stats_df['actual_std'].apply(lambda x: max(x, global_activity_std * 0.1))\n",
        "\n",
        "# Debug: Print per-activity stats\n",
        "print(\"Per-activity magnitude stats:\\n\", activity_stats_df[['activity', 'actual_mean', 'actual_std', 'sample_count']])\n",
        "\n",
        "# Merge with participant characteristics for clustering\n",
        "merged_df = pd.merge(acc_df, participants_df, on='participant', how='inner')\n",
        "\n",
        "if merged_df.empty:\n",
        "    print(\"Error: No matching participants found after merging.\")\n",
        "    exit(1)\n",
        "\n",
        "# Subsample the data to reduce memory usage\n",
        "subsample_fraction = 0.1  # Use 99% of the data\n",
        "subsampled_df = merged_df.sample(frac=subsample_fraction, random_state=42)\n",
        "print(f\"Subsampled data size: {len(subsampled_df)}\")\n",
        "\n",
        "# Define feature sets to test\n",
        "semantic_features = ['age', 'gender', 'height', 'weight', 'activity']\n",
        "feature_combinations = []\n",
        "# Include magnitude-only as baseline\n",
        "feature_combinations.append(['magnitude'])\n",
        "# Test magnitude with each semantic feature\n",
        "for r in range(1, len(semantic_features) + 1):\n",
        "    for combo in itertools.combinations(semantic_features, r):\n",
        "        feature_combinations.append(['magnitude'] + list(combo))\n",
        "\n",
        "# Function to preprocess features for clustering\n",
        "def preprocess_features(df, features, activity_train=None):\n",
        "    numerical_features = [f for f in features if f in ['magnitude', 'age', 'height', 'weight']]\n",
        "    categorical_features = [f for f in features if f in ['gender', 'activity']]\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    X_numeric = scaler.fit_transform(df[numerical_features]) if numerical_features else np.array([])\n",
        "\n",
        "    X_categorical = []\n",
        "    if 'gender' in categorical_features:\n",
        "        X_gender = df['gender'].values.reshape(-1, 1)\n",
        "        X_categorical.append(X_gender)\n",
        "\n",
        "    if 'activity' in categorical_features:\n",
        "        X_activity = pd.get_dummies(df['activity'], prefix='activity')\n",
        "        if activity_train is not None:\n",
        "            X_activity = X_activity.reindex(columns=activity_train.columns, fill_value=0)\n",
        "        else:\n",
        "            activity_train = X_activity\n",
        "        X_categorical.append(X_activity.values)\n",
        "\n",
        "    if X_categorical:\n",
        "        X_cat = np.hstack(X_categorical) if len(X_categorical) > 1 else X_categorical[0]\n",
        "        X_scaled = np.hstack([X_numeric, X_cat]) if len(X_numeric) > 0 else X_cat\n",
        "    else:\n",
        "        X_scaled = X_numeric\n",
        "\n",
        "    return X_scaled, scaler, activity_train\n",
        "\n",
        "# Function to introduce GNN noise\n",
        "def introduce_gnn_noise(df, beta=1.5, scale=0.1, noise_fraction=0.05):\n",
        "    noisy_df = df.copy()\n",
        "    mask = np.random.rand(len(noisy_df)) < noise_fraction\n",
        "    noise = gennorm.rvs(beta=beta, loc=0, scale=scale, size=len(noisy_df))\n",
        "    noisy_df['magnitude'] = noisy_df['magnitude'] + noise * mask\n",
        "    noisy_df['is_true_noise'] = mask.astype(int)\n",
        "    return noisy_df\n",
        "\n",
        "# Evaluate each feature combination\n",
        "results_list = []\n",
        "noise_fraction = 0.2  # Test at 20% noise level\n",
        "threshold = 2\n",
        "\n",
        "for features in feature_combinations:\n",
        "    print(f\"\\nTesting feature combination: {features}\")\n",
        "\n",
        "    # Cross-validation setup\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_f1_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(subsampled_df)):\n",
        "        train_df = subsampled_df.iloc[train_idx]\n",
        "        val_df = subsampled_df.iloc[val_idx]\n",
        "\n",
        "        # Preprocess training data\n",
        "        X_scaled_train, scaler, activity_train = preprocess_features(train_df, features)\n",
        "\n",
        "        # Generate k-distance plot to determine initial eps\n",
        "        min_samples = 50  # Starting point for min_samples\n",
        "        neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X_scaled_train)\n",
        "        distances, indices = neighbors.kneighbors(X_scaled_train)\n",
        "        k_distances = np.sort(distances[:, min_samples - 1])\n",
        "        suggested_eps = np.percentile(k_distances, 95)\n",
        "\n",
        "        # Parameter tuning using silhouette score\n",
        "        eps_values = np.linspace(suggested_eps * 0.5, suggested_eps * 1.5, 5)\n",
        "        min_samples_values = [20, 30, 50, 60, 70]\n",
        "        best_score = -1\n",
        "        best_params = {'eps': suggested_eps, 'min_samples': min_samples}\n",
        "\n",
        "        for eps in eps_values:\n",
        "            for min_samples in min_samples_values:\n",
        "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "                labels = dbscan.fit_predict(X_scaled_train)\n",
        "                if len(np.unique(labels)) > 1 and -1 in labels:\n",
        "                    score = silhouette_score(X_scaled_train, labels, sample_size=5000)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "\n",
        "        # Use best parameters for clustering\n",
        "        dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
        "        train_cluster_labels = dbscan.fit_predict(X_scaled_train)\n",
        "\n",
        "        # Assign clusters to validation set\n",
        "        X_scaled_val, _, _ = preprocess_features(val_df, features, activity_train)\n",
        "        val_cluster_labels = dbscan.fit_predict(X_scaled_val)\n",
        "\n",
        "        train_df = train_df.copy()\n",
        "        val_df = val_df.copy()\n",
        "        train_df['cluster'] = train_cluster_labels\n",
        "        val_df['cluster'] = val_cluster_labels\n",
        "\n",
        "        # Combine for anomaly detection\n",
        "        acc_df = pd.concat([train_df, val_df], axis=0)\n",
        "\n",
        "        # Compute cluster stats\n",
        "        cluster_means = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].mean().to_dict()\n",
        "        cluster_stds = train_df[train_df['cluster'] != -1].groupby('cluster')['magnitude'].std().to_dict()\n",
        "        acc_df['cluster_mu'] = acc_df['cluster'].map(cluster_means)\n",
        "        acc_df['cluster_sigma'] = acc_df['cluster'].map(cluster_stds).fillna(global_activity_std)\n",
        "        acc_df.loc[acc_df['cluster'] == -1, 'cluster_mu'] = global_magnitude_mean\n",
        "        acc_df.loc[acc_df['cluster'] == -1, 'cluster_sigma'] = global_magnitude_std\n",
        "\n",
        "        # Compute Z-scores\n",
        "        acc_df['Z_score'] = (acc_df['magnitude'] - acc_df['cluster_mu']) / acc_df['cluster_sigma'].replace(0, np.nan)\n",
        "        acc_df['traditional_Z_score'] = (acc_df['magnitude'] - global_magnitude_mean) / global_magnitude_std\n",
        "\n",
        "        # Apply activity-specific plausible ranges\n",
        "        acc_df['activity'] = acc_df['activity'].str.lower()\n",
        "        default_min_plausible = 0\n",
        "        default_max_plausible = 10\n",
        "        acc_df['min_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'min': default_min_plausible})['min'])\n",
        "        acc_df['max_plausible'] = acc_df['activity'].map(lambda x: activity_ranges.get(x, {'max': default_max_plausible})['max'])\n",
        "\n",
        "        acc_df['outside_plausible'] = (acc_df['magnitude'] < acc_df['min_plausible']) | (acc_df['magnitude'] > acc_df['max_plausible'])\n",
        "\n",
        "        # Introduce GNN noise\n",
        "        noisy_df = introduce_gnn_noise(acc_df, beta=1.5, scale=2 * global_magnitude_std, noise_fraction=noise_fraction)\n",
        "\n",
        "        # Apply anomaly detection - Hybrid method\n",
        "        noisy_df['Z_score'] = (noisy_df['magnitude'] - noisy_df['cluster_mu']) / noisy_df['cluster_sigma'].replace(0, np.nan)\n",
        "        noisy_df['is_anomaly'] = np.abs(noisy_df['Z_score']) > threshold\n",
        "        noisy_df['outside_plausible'] = (noisy_df['magnitude'] < noisy_df['min_plausible']) | (noisy_df['magnitude'] > noisy_df['max_plausible'])\n",
        "        noisy_df['is_noise'] = noisy_df['is_anomaly'] & noisy_df['outside_plausible']\n",
        "\n",
        "        # Evaluate performance\n",
        "        y_true = noisy_df['is_true_noise']\n",
        "        y_pred_hybrid = noisy_df['is_noise'].astype(int)\n",
        "\n",
        "        precision_hybrid = precision_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        recall_hybrid = recall_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "        f1_hybrid = f1_score(y_true, y_pred_hybrid, zero_division=0)\n",
        "\n",
        "        total_noise_points = y_true.sum()\n",
        "        hybrid_detected = y_pred_hybrid[y_true == 1].sum()\n",
        "        hybrid_noise_percentage = (hybrid_detected / total_noise_points * 100) if total_noise_points > 0 else 0\n",
        "\n",
        "        # Print results for this fold\n",
        "        print(f\"Fold {fold + 1} Results - Features: {features}\")\n",
        "        print(f\"  Precision: {precision_hybrid:.6f}\")\n",
        "        print(f\"  Recall: {recall_hybrid:.6f}\")\n",
        "        print(f\"  F1 Score: {f1_hybrid:.6f}\")\n",
        "        print(f\"  Noise Detected (%): {hybrid_noise_percentage:.6f}\")\n",
        "\n",
        "        cv_f1_scores.append(f1_hybrid)\n",
        "\n",
        "    # Average F1-score across folds\n",
        "    avg_f1_score = np.mean(cv_f1_scores)\n",
        "\n",
        "    # Store results for this feature combination\n",
        "    results_list.append({\n",
        "        'Features': ', '.join(features),\n",
        "        'Avg F1 Score': avg_f1_score,\n",
        "        'Precision': precision_hybrid,  # From last fold\n",
        "        'Recall': recall_hybrid,\n",
        "        'Noise Detected (%)': hybrid_noise_percentage\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df = results_df.sort_values(by='Avg F1 Score', ascending=False)\n",
        "print(\"\\nFeature Selection Results (sorted by Avg F1 Score):\")\n",
        "print(results_df)\n",
        "\n",
        "# Save results\n",
        "try:\n",
        "    results_df.to_csv('/content/drive/My Drive/ColabNotebooks/feature_selection_results_dbscan.csv', index=False)\n",
        "    print(f\"Feature selection results saved to '/content/drive/My Drive/ColabNotebooks/feature_selection_results_dbscan.csv'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving files: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a1aR6w3mcvw",
        "outputId": "22f4a804-1893-4a8f-c5cc-17b3e50cfd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Global magnitude stats: Mean = 0.753240, Std = 0.638734\n",
            "Per-activity magnitude stats:\n",
            "   activity  actual_mean  actual_std  sample_count\n",
            "0      dws     0.575685    0.426152        131856\n",
            "1      jog     1.445474    0.905656        134231\n",
            "2      ups     0.447230    0.337824        157285\n",
            "3      wlk     0.691151    0.471883        344288\n",
            "Subsampled data size: 76766\n",
            "\n",
            "Testing feature combination: ['magnitude']\n",
            "Fold 1 Results - Features: ['magnitude']\n",
            "  Precision: 0.838053\n",
            "  Recall: 0.271237\n",
            "  F1 Score: 0.409831\n",
            "  Noise Detected (%): 27.123677\n",
            "Fold 2 Results - Features: ['magnitude']\n",
            "  Precision: 0.829287\n",
            "  Recall: 0.277687\n",
            "  F1 Score: 0.416058\n",
            "  Noise Detected (%): 27.768746\n",
            "Fold 3 Results - Features: ['magnitude']\n",
            "  Precision: 0.829352\n",
            "  Recall: 0.271794\n",
            "  F1 Score: 0.409415\n",
            "  Noise Detected (%): 27.179420\n",
            "Fold 4 Results - Features: ['magnitude']\n",
            "  Precision: 0.838993\n",
            "  Recall: 0.274725\n",
            "  F1 Score: 0.413915\n",
            "  Noise Detected (%): 27.472527\n",
            "Fold 5 Results - Features: ['magnitude']\n",
            "  Precision: 0.835517\n",
            "  Recall: 0.278342\n",
            "  F1 Score: 0.417575\n",
            "  Noise Detected (%): 27.834246\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age']\n",
            "Fold 1 Results - Features: ['magnitude', 'age']\n",
            "  Precision: 0.832065\n",
            "  Recall: 0.289831\n",
            "  F1 Score: 0.429912\n",
            "  Noise Detected (%): 28.983107\n",
            "Fold 2 Results - Features: ['magnitude', 'age']\n",
            "  Precision: 0.835422\n",
            "  Recall: 0.295984\n",
            "  F1 Score: 0.437105\n",
            "  Noise Detected (%): 29.598433\n",
            "Fold 3 Results - Features: ['magnitude', 'age']\n",
            "  Precision: 0.829950\n",
            "  Recall: 0.282769\n",
            "  F1 Score: 0.421821\n",
            "  Noise Detected (%): 28.276897\n",
            "Fold 4 Results - Features: ['magnitude', 'age']\n",
            "  Precision: 0.831410\n",
            "  Recall: 0.288093\n",
            "  F1 Score: 0.427911\n",
            "  Noise Detected (%): 28.809336\n",
            "Fold 5 Results - Features: ['magnitude', 'age']\n",
            "  Precision: 0.838524\n",
            "  Recall: 0.303117\n",
            "  F1 Score: 0.445273\n",
            "  Noise Detected (%): 30.311707\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender']\n",
            "  Precision: 0.842287\n",
            "  Recall: 0.303056\n",
            "  F1 Score: 0.445736\n",
            "  Noise Detected (%): 30.305603\n",
            "Fold 2 Results - Features: ['magnitude', 'gender']\n",
            "  Precision: 0.844529\n",
            "  Recall: 0.302117\n",
            "  F1 Score: 0.445031\n",
            "  Noise Detected (%): 30.211657\n",
            "Fold 3 Results - Features: ['magnitude', 'gender']\n",
            "  Precision: 0.846167\n",
            "  Recall: 0.307380\n",
            "  F1 Score: 0.450948\n",
            "  Noise Detected (%): 30.737995\n",
            "Fold 4 Results - Features: ['magnitude', 'gender']\n",
            "  Precision: 0.857687\n",
            "  Recall: 0.294730\n",
            "  F1 Score: 0.438706\n",
            "  Noise Detected (%): 29.472995\n",
            "Fold 5 Results - Features: ['magnitude', 'gender']\n",
            "  Precision: 0.846845\n",
            "  Recall: 0.281860\n",
            "  F1 Score: 0.422947\n",
            "  Noise Detected (%): 28.185956\n",
            "\n",
            "Testing feature combination: ['magnitude', 'height']\n",
            "Fold 1 Results - Features: ['magnitude', 'height']\n",
            "  Precision: 0.826618\n",
            "  Recall: 0.278035\n",
            "  F1 Score: 0.416110\n",
            "  Noise Detected (%): 27.803468\n",
            "Fold 2 Results - Features: ['magnitude', 'height']\n",
            "  Precision: 0.825871\n",
            "  Recall: 0.280060\n",
            "  F1 Score: 0.418278\n",
            "  Noise Detected (%): 28.005970\n",
            "Fold 3 Results - Features: ['magnitude', 'height']\n",
            "  Precision: 0.823720\n",
            "  Recall: 0.280630\n",
            "  F1 Score: 0.418636\n",
            "  Noise Detected (%): 28.062984\n",
            "Fold 4 Results - Features: ['magnitude', 'height']\n",
            "  Precision: 0.831480\n",
            "  Recall: 0.286410\n",
            "  F1 Score: 0.426060\n",
            "  Noise Detected (%): 28.640997\n",
            "Fold 5 Results - Features: ['magnitude', 'height']\n",
            "  Precision: 0.826743\n",
            "  Recall: 0.285865\n",
            "  F1 Score: 0.424834\n",
            "  Noise Detected (%): 28.586526\n",
            "\n",
            "Testing feature combination: ['magnitude', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'weight']\n",
            "  Precision: 0.828657\n",
            "  Recall: 0.290606\n",
            "  F1 Score: 0.430306\n",
            "  Noise Detected (%): 29.060614\n",
            "Fold 2 Results - Features: ['magnitude', 'weight']\n",
            "  Precision: 0.817722\n",
            "  Recall: 0.260270\n",
            "  F1 Score: 0.394862\n",
            "  Noise Detected (%): 26.027044\n",
            "Fold 3 Results - Features: ['magnitude', 'weight']\n",
            "  Precision: 0.824332\n",
            "  Recall: 0.273767\n",
            "  F1 Score: 0.411028\n",
            "  Noise Detected (%): 27.376651\n",
            "Fold 4 Results - Features: ['magnitude', 'weight']\n",
            "  Precision: 0.831611\n",
            "  Recall: 0.299732\n",
            "  F1 Score: 0.440645\n",
            "  Noise Detected (%): 29.973215\n",
            "Fold 5 Results - Features: ['magnitude', 'weight']\n",
            "  Precision: 0.836616\n",
            "  Recall: 0.296363\n",
            "  F1 Score: 0.437682\n",
            "  Noise Detected (%): 29.636328\n",
            "\n",
            "Testing feature combination: ['magnitude', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'activity']\n",
            "  Precision: 0.853194\n",
            "  Recall: 0.329321\n",
            "  F1 Score: 0.475216\n",
            "  Noise Detected (%): 32.932145\n",
            "Fold 2 Results - Features: ['magnitude', 'activity']\n",
            "  Precision: 0.855556\n",
            "  Recall: 0.314123\n",
            "  F1 Score: 0.459527\n",
            "  Noise Detected (%): 31.412290\n",
            "Fold 3 Results - Features: ['magnitude', 'activity']\n",
            "  Precision: 0.848780\n",
            "  Recall: 0.324863\n",
            "  F1 Score: 0.469882\n",
            "  Noise Detected (%): 32.486282\n",
            "Fold 4 Results - Features: ['magnitude', 'activity']\n",
            "  Precision: 0.849674\n",
            "  Recall: 0.331530\n",
            "  F1 Score: 0.476958\n",
            "  Noise Detected (%): 33.153024\n",
            "Fold 5 Results - Features: ['magnitude', 'activity']\n",
            "  Precision: 0.851846\n",
            "  Recall: 0.330303\n",
            "  F1 Score: 0.476027\n",
            "  Noise Detected (%): 33.030303\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender']\n",
            "  Precision: 0.839497\n",
            "  Recall: 0.303150\n",
            "  F1 Score: 0.445446\n",
            "  Noise Detected (%): 30.315036\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender']\n",
            "  Precision: 0.837188\n",
            "  Recall: 0.298640\n",
            "  F1 Score: 0.440239\n",
            "  Noise Detected (%): 29.864044\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender']\n",
            "  Precision: 0.830654\n",
            "  Recall: 0.283041\n",
            "  F1 Score: 0.422215\n",
            "  Noise Detected (%): 28.304101\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender']\n",
            "  Precision: 0.826818\n",
            "  Recall: 0.277723\n",
            "  F1 Score: 0.415786\n",
            "  Noise Detected (%): 27.772306\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender']\n",
            "  Precision: 0.830530\n",
            "  Recall: 0.299000\n",
            "  F1 Score: 0.439702\n",
            "  Noise Detected (%): 29.899961\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'height']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'height']\n",
            "  Precision: 0.836613\n",
            "  Recall: 0.291953\n",
            "  F1 Score: 0.432853\n",
            "  Noise Detected (%): 29.195283\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'height']\n",
            "  Precision: 0.836446\n",
            "  Recall: 0.290316\n",
            "  F1 Score: 0.431029\n",
            "  Noise Detected (%): 29.031624\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'height']\n",
            "  Precision: 0.831410\n",
            "  Recall: 0.289022\n",
            "  F1 Score: 0.428934\n",
            "  Noise Detected (%): 28.902190\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'height']\n",
            "  Precision: 0.836817\n",
            "  Recall: 0.296981\n",
            "  F1 Score: 0.438382\n",
            "  Noise Detected (%): 29.698074\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'height']\n",
            "  Precision: 0.839929\n",
            "  Recall: 0.276977\n",
            "  F1 Score: 0.416581\n",
            "  Noise Detected (%): 27.697702\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'weight']\n",
            "  Precision: 0.842454\n",
            "  Recall: 0.315515\n",
            "  F1 Score: 0.459091\n",
            "  Noise Detected (%): 31.551465\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'weight']\n",
            "  Precision: 0.836795\n",
            "  Recall: 0.278659\n",
            "  F1 Score: 0.418090\n",
            "  Noise Detected (%): 27.865858\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'weight']\n",
            "  Precision: 0.836174\n",
            "  Recall: 0.299704\n",
            "  F1 Score: 0.441253\n",
            "  Noise Detected (%): 29.970443\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'weight']\n",
            "  Precision: 0.845294\n",
            "  Recall: 0.305010\n",
            "  F1 Score: 0.448269\n",
            "  Noise Detected (%): 30.500976\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'weight']\n",
            "  Precision: 0.837192\n",
            "  Recall: 0.296967\n",
            "  F1 Score: 0.438419\n",
            "  Noise Detected (%): 29.696695\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'activity']\n",
            "  Precision: 0.846370\n",
            "  Recall: 0.317897\n",
            "  F1 Score: 0.462194\n",
            "  Noise Detected (%): 31.789737\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'activity']\n",
            "  Precision: 0.849205\n",
            "  Recall: 0.317709\n",
            "  F1 Score: 0.462417\n",
            "  Noise Detected (%): 31.770935\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'activity']\n",
            "  Precision: 0.846797\n",
            "  Recall: 0.315394\n",
            "  F1 Score: 0.459605\n",
            "  Noise Detected (%): 31.539359\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'activity']\n",
            "  Precision: 0.847520\n",
            "  Recall: 0.316124\n",
            "  F1 Score: 0.460487\n",
            "  Noise Detected (%): 31.612438\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'activity']\n",
            "  Precision: 0.848553\n",
            "  Recall: 0.319639\n",
            "  F1 Score: 0.464360\n",
            "  Noise Detected (%): 31.963915\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender', 'height']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender', 'height']\n",
            "  Precision: 0.833490\n",
            "  Recall: 0.287646\n",
            "  F1 Score: 0.427692\n",
            "  Noise Detected (%): 28.764641\n",
            "Fold 2 Results - Features: ['magnitude', 'gender', 'height']\n",
            "  Precision: 0.829176\n",
            "  Recall: 0.286554\n",
            "  F1 Score: 0.425916\n",
            "  Noise Detected (%): 28.655391\n",
            "Fold 3 Results - Features: ['magnitude', 'gender', 'height']\n",
            "  Precision: 0.823821\n",
            "  Recall: 0.281558\n",
            "  F1 Score: 0.419681\n",
            "  Noise Detected (%): 28.155783\n",
            "Fold 4 Results - Features: ['magnitude', 'gender', 'height']\n",
            "  Precision: 0.827463\n",
            "  Recall: 0.287621\n",
            "  F1 Score: 0.426866\n",
            "  Noise Detected (%): 28.762128\n",
            "Fold 5 Results - Features: ['magnitude', 'gender', 'height']\n",
            "  Precision: 0.823178\n",
            "  Recall: 0.288995\n",
            "  F1 Score: 0.427801\n",
            "  Noise Detected (%): 28.899534\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender', 'weight']\n",
            "  Precision: 0.823553\n",
            "  Recall: 0.272763\n",
            "  F1 Score: 0.409799\n",
            "  Noise Detected (%): 27.276285\n",
            "Fold 2 Results - Features: ['magnitude', 'gender', 'weight']\n",
            "  Precision: 0.829181\n",
            "  Recall: 0.287820\n",
            "  F1 Score: 0.427314\n",
            "  Noise Detected (%): 28.782022\n",
            "Fold 3 Results - Features: ['magnitude', 'gender', 'weight']\n",
            "  Precision: 0.819202\n",
            "  Recall: 0.276258\n",
            "  F1 Score: 0.413180\n",
            "  Noise Detected (%): 27.625824\n",
            "Fold 4 Results - Features: ['magnitude', 'gender', 'weight']\n",
            "  Precision: 0.825302\n",
            "  Recall: 0.280999\n",
            "  F1 Score: 0.419251\n",
            "  Noise Detected (%): 28.099876\n",
            "Fold 5 Results - Features: ['magnitude', 'gender', 'weight']\n",
            "  Precision: 0.828283\n",
            "  Recall: 0.287570\n",
            "  F1 Score: 0.426919\n",
            "  Noise Detected (%): 28.756981\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender', 'activity']\n",
            "  Precision: 0.848997\n",
            "  Recall: 0.327612\n",
            "  F1 Score: 0.472785\n",
            "  Noise Detected (%): 32.761180\n",
            "Fold 2 Results - Features: ['magnitude', 'gender', 'activity']\n",
            "  Precision: 0.849432\n",
            "  Recall: 0.331162\n",
            "  F1 Score: 0.476539\n",
            "  Noise Detected (%): 33.116164\n",
            "Fold 3 Results - Features: ['magnitude', 'gender', 'activity']\n",
            "  Precision: 0.850343\n",
            "  Recall: 0.339050\n",
            "  F1 Score: 0.484800\n",
            "  Noise Detected (%): 33.905048\n",
            "Fold 4 Results - Features: ['magnitude', 'gender', 'activity']\n",
            "  Precision: 0.849582\n",
            "  Recall: 0.324804\n",
            "  F1 Score: 0.469944\n",
            "  Noise Detected (%): 32.480418\n",
            "Fold 5 Results - Features: ['magnitude', 'gender', 'activity']\n",
            "  Precision: 0.852713\n",
            "  Recall: 0.334996\n",
            "  F1 Score: 0.481020\n",
            "  Noise Detected (%): 33.499644\n",
            "\n",
            "Testing feature combination: ['magnitude', 'height', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'height', 'weight']\n",
            "  Precision: 0.836878\n",
            "  Recall: 0.292218\n",
            "  F1 Score: 0.433180\n",
            "  Noise Detected (%): 29.221828\n",
            "Fold 2 Results - Features: ['magnitude', 'height', 'weight']\n",
            "  Precision: 0.838419\n",
            "  Recall: 0.278149\n",
            "  F1 Score: 0.417718\n",
            "  Noise Detected (%): 27.814913\n",
            "Fold 3 Results - Features: ['magnitude', 'height', 'weight']\n",
            "  Precision: 0.833024\n",
            "  Recall: 0.294929\n",
            "  F1 Score: 0.435626\n",
            "  Noise Detected (%): 29.492906\n",
            "Fold 4 Results - Features: ['magnitude', 'height', 'weight']\n",
            "  Precision: 0.837153\n",
            "  Recall: 0.293843\n",
            "  F1 Score: 0.435000\n",
            "  Noise Detected (%): 29.384286\n",
            "Fold 5 Results - Features: ['magnitude', 'height', 'weight']\n",
            "  Precision: 0.834646\n",
            "  Recall: 0.291743\n",
            "  F1 Score: 0.432359\n",
            "  Noise Detected (%): 29.174312\n",
            "\n",
            "Testing feature combination: ['magnitude', 'height', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'height', 'activity']\n",
            "  Precision: 0.842087\n",
            "  Recall: 0.321958\n",
            "  F1 Score: 0.465818\n",
            "  Noise Detected (%): 32.195826\n",
            "Fold 2 Results - Features: ['magnitude', 'height', 'activity']\n",
            "  Precision: 0.843771\n",
            "  Recall: 0.323946\n",
            "  F1 Score: 0.468155\n",
            "  Noise Detected (%): 32.394642\n",
            "Fold 3 Results - Features: ['magnitude', 'height', 'activity']\n",
            "  Precision: 0.843995\n",
            "  Recall: 0.325632\n",
            "  F1 Score: 0.469947\n",
            "  Noise Detected (%): 32.563177\n",
            "Fold 4 Results - Features: ['magnitude', 'height', 'activity']\n",
            "  Precision: 0.836283\n",
            "  Recall: 0.306361\n",
            "  F1 Score: 0.448441\n",
            "  Noise Detected (%): 30.636063\n",
            "Fold 5 Results - Features: ['magnitude', 'height', 'activity']\n",
            "  Precision: 0.839225\n",
            "  Recall: 0.311918\n",
            "  F1 Score: 0.454799\n",
            "  Noise Detected (%): 31.191812\n",
            "\n",
            "Testing feature combination: ['magnitude', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'weight', 'activity']\n",
            "  Precision: 0.845031\n",
            "  Recall: 0.318763\n",
            "  F1 Score: 0.462907\n",
            "  Noise Detected (%): 31.876270\n",
            "Fold 2 Results - Features: ['magnitude', 'weight', 'activity']\n",
            "  Precision: 0.843778\n",
            "  Recall: 0.305979\n",
            "  F1 Score: 0.449100\n",
            "  Noise Detected (%): 30.597875\n",
            "Fold 3 Results - Features: ['magnitude', 'weight', 'activity']\n",
            "  Precision: 0.839205\n",
            "  Recall: 0.303595\n",
            "  F1 Score: 0.445884\n",
            "  Noise Detected (%): 30.359477\n",
            "Fold 4 Results - Features: ['magnitude', 'weight', 'activity']\n",
            "  Precision: 0.849789\n",
            "  Recall: 0.329042\n",
            "  F1 Score: 0.474396\n",
            "  Noise Detected (%): 32.904196\n",
            "Fold 5 Results - Features: ['magnitude', 'weight', 'activity']\n",
            "  Precision: 0.844463\n",
            "  Recall: 0.324623\n",
            "  F1 Score: 0.468969\n",
            "  Noise Detected (%): 32.462344\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender', 'height']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender', 'height']\n",
            "  Precision: 0.838598\n",
            "  Recall: 0.300157\n",
            "  F1 Score: 0.442081\n",
            "  Noise Detected (%): 30.015684\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender', 'height']\n",
            "  Precision: 0.834605\n",
            "  Recall: 0.294938\n",
            "  F1 Score: 0.435851\n",
            "  Noise Detected (%): 29.493754\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender', 'height']\n",
            "  Precision: 0.840150\n",
            "  Recall: 0.292756\n",
            "  F1 Score: 0.434209\n",
            "  Noise Detected (%): 29.275628\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender', 'height']\n",
            "  Precision: 0.844368\n",
            "  Recall: 0.305156\n",
            "  F1 Score: 0.448297\n",
            "  Noise Detected (%): 30.515638\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender', 'height']\n",
            "  Precision: 0.837932\n",
            "  Recall: 0.296897\n",
            "  F1 Score: 0.438445\n",
            "  Noise Detected (%): 29.689741\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender', 'weight']\n",
            "  Precision: 0.842534\n",
            "  Recall: 0.304089\n",
            "  F1 Score: 0.446887\n",
            "  Noise Detected (%): 30.408937\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender', 'weight']\n",
            "  Precision: 0.844194\n",
            "  Recall: 0.303534\n",
            "  F1 Score: 0.446520\n",
            "  Noise Detected (%): 30.353403\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender', 'weight']\n",
            "  Precision: 0.839179\n",
            "  Recall: 0.300514\n",
            "  F1 Score: 0.442549\n",
            "  Noise Detected (%): 30.051409\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender', 'weight']\n",
            "  Precision: 0.841200\n",
            "  Recall: 0.297933\n",
            "  F1 Score: 0.440021\n",
            "  Noise Detected (%): 29.793300\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender', 'weight']\n",
            "  Precision: 0.843610\n",
            "  Recall: 0.310813\n",
            "  F1 Score: 0.454261\n",
            "  Noise Detected (%): 31.081259\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender', 'activity']\n",
            "  Precision: 0.851494\n",
            "  Recall: 0.331505\n",
            "  F1 Score: 0.477219\n",
            "  Noise Detected (%): 33.150522\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender', 'activity']\n",
            "  Precision: 0.855660\n",
            "  Recall: 0.334599\n",
            "  F1 Score: 0.481077\n",
            "  Noise Detected (%): 33.459927\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender', 'activity']\n",
            "  Precision: 0.857143\n",
            "  Recall: 0.334503\n",
            "  F1 Score: 0.481211\n",
            "  Noise Detected (%): 33.450292\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender', 'activity']\n",
            "  Precision: 0.852014\n",
            "  Recall: 0.315254\n",
            "  F1 Score: 0.460221\n",
            "  Noise Detected (%): 31.525402\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender', 'activity']\n",
            "  Precision: 0.850726\n",
            "  Recall: 0.324559\n",
            "  F1 Score: 0.469861\n",
            "  Noise Detected (%): 32.455855\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'height', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'height', 'weight']\n",
            "  Precision: 0.847656\n",
            "  Recall: 0.317325\n",
            "  F1 Score: 0.461780\n",
            "  Noise Detected (%): 31.732463\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'height', 'weight']\n",
            "  Precision: 0.845005\n",
            "  Recall: 0.311493\n",
            "  F1 Score: 0.455190\n",
            "  Noise Detected (%): 31.149260\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'height', 'weight']\n",
            "  Precision: 0.847512\n",
            "  Recall: 0.329613\n",
            "  F1 Score: 0.474633\n",
            "  Noise Detected (%): 32.961300\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'height', 'weight']\n",
            "  Precision: 0.844593\n",
            "  Recall: 0.318612\n",
            "  F1 Score: 0.462683\n",
            "  Noise Detected (%): 31.861219\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'height', 'weight']\n",
            "  Precision: 0.844759\n",
            "  Recall: 0.319217\n",
            "  F1 Score: 0.463345\n",
            "  Noise Detected (%): 31.921697\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'height', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'height', 'activity']\n",
            "  Precision: 0.849944\n",
            "  Recall: 0.298017\n",
            "  F1 Score: 0.441300\n",
            "  Noise Detected (%): 29.801715\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'height', 'activity']\n",
            "  Precision: 0.851915\n",
            "  Recall: 0.319134\n",
            "  F1 Score: 0.464327\n",
            "  Noise Detected (%): 31.913394\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'height', 'activity']\n",
            "  Precision: 0.843827\n",
            "  Recall: 0.317560\n",
            "  F1 Score: 0.461458\n",
            "  Noise Detected (%): 31.755956\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'height', 'activity']\n",
            "  Precision: 0.847419\n",
            "  Recall: 0.317934\n",
            "  F1 Score: 0.462389\n",
            "  Noise Detected (%): 31.793372\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'height', 'activity']\n",
            "  Precision: 0.848277\n",
            "  Recall: 0.320114\n",
            "  F1 Score: 0.464819\n",
            "  Noise Detected (%): 32.011386\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'weight', 'activity']\n",
            "  Precision: 0.853062\n",
            "  Recall: 0.326225\n",
            "  F1 Score: 0.471963\n",
            "  Noise Detected (%): 32.622490\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'weight', 'activity']\n",
            "  Precision: 0.851948\n",
            "  Recall: 0.322187\n",
            "  F1 Score: 0.467556\n",
            "  Noise Detected (%): 32.218746\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'weight', 'activity']\n",
            "  Precision: 0.847264\n",
            "  Recall: 0.311812\n",
            "  F1 Score: 0.455858\n",
            "  Noise Detected (%): 31.181194\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'weight', 'activity']\n",
            "  Precision: 0.851997\n",
            "  Recall: 0.326209\n",
            "  F1 Score: 0.471784\n",
            "  Noise Detected (%): 32.620915\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'weight', 'activity']\n",
            "  Precision: 0.854041\n",
            "  Recall: 0.323752\n",
            "  F1 Score: 0.469518\n",
            "  Noise Detected (%): 32.375229\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender', 'height', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender', 'height', 'weight']\n",
            "  Precision: 0.840780\n",
            "  Recall: 0.298071\n",
            "  F1 Score: 0.440114\n",
            "  Noise Detected (%): 29.807066\n",
            "Fold 2 Results - Features: ['magnitude', 'gender', 'height', 'weight']\n",
            "  Precision: 0.834615\n",
            "  Recall: 0.288296\n",
            "  F1 Score: 0.428558\n",
            "  Noise Detected (%): 28.829587\n",
            "Fold 3 Results - Features: ['magnitude', 'gender', 'height', 'weight']\n",
            "  Precision: 0.836783\n",
            "  Recall: 0.300289\n",
            "  F1 Score: 0.441972\n",
            "  Noise Detected (%): 30.028925\n",
            "Fold 4 Results - Features: ['magnitude', 'gender', 'height', 'weight']\n",
            "  Precision: 0.838578\n",
            "  Recall: 0.297459\n",
            "  F1 Score: 0.439145\n",
            "  Noise Detected (%): 29.745880\n",
            "Fold 5 Results - Features: ['magnitude', 'gender', 'height', 'weight']\n",
            "  Precision: 0.835163\n",
            "  Recall: 0.292748\n",
            "  F1 Score: 0.433531\n",
            "  Noise Detected (%): 29.274814\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender', 'height', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender', 'height', 'activity']\n",
            "  Precision: 0.848708\n",
            "  Recall: 0.329470\n",
            "  F1 Score: 0.474672\n",
            "  Noise Detected (%): 32.946998\n",
            "Fold 2 Results - Features: ['magnitude', 'gender', 'height', 'activity']\n",
            "  Precision: 0.845408\n",
            "  Recall: 0.325989\n",
            "  F1 Score: 0.470538\n",
            "  Noise Detected (%): 32.598859\n",
            "Fold 3 Results - Features: ['magnitude', 'gender', 'height', 'activity']\n",
            "  Precision: 0.850233\n",
            "  Recall: 0.332423\n",
            "  F1 Score: 0.477970\n",
            "  Noise Detected (%): 33.242347\n",
            "Fold 4 Results - Features: ['magnitude', 'gender', 'height', 'activity']\n",
            "  Precision: 0.849660\n",
            "  Recall: 0.331458\n",
            "  F1 Score: 0.476882\n",
            "  Noise Detected (%): 33.145813\n",
            "Fold 5 Results - Features: ['magnitude', 'gender', 'height', 'activity']\n",
            "  Precision: 0.853413\n",
            "  Recall: 0.337242\n",
            "  F1 Score: 0.483443\n",
            "  Noise Detected (%): 33.724209\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.841593\n",
            "  Recall: 0.311334\n",
            "  F1 Score: 0.454524\n",
            "  Noise Detected (%): 31.133373\n",
            "Fold 2 Results - Features: ['magnitude', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.836347\n",
            "  Recall: 0.302252\n",
            "  F1 Score: 0.444033\n",
            "  Noise Detected (%): 30.225184\n",
            "Fold 3 Results - Features: ['magnitude', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.837838\n",
            "  Recall: 0.311057\n",
            "  F1 Score: 0.453680\n",
            "  Noise Detected (%): 31.105719\n",
            "Fold 4 Results - Features: ['magnitude', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.843586\n",
            "  Recall: 0.325679\n",
            "  F1 Score: 0.469933\n",
            "  Noise Detected (%): 32.567877\n",
            "Fold 5 Results - Features: ['magnitude', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.844305\n",
            "  Recall: 0.318403\n",
            "  F1 Score: 0.462419\n",
            "  Noise Detected (%): 31.840273\n",
            "\n",
            "Testing feature combination: ['magnitude', 'height', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'height', 'weight', 'activity']\n",
            "  Precision: 0.841888\n",
            "  Recall: 0.306962\n",
            "  F1 Score: 0.449890\n",
            "  Noise Detected (%): 30.696244\n",
            "Fold 2 Results - Features: ['magnitude', 'height', 'weight', 'activity']\n",
            "  Precision: 0.833210\n",
            "  Recall: 0.290571\n",
            "  F1 Score: 0.430878\n",
            "  Noise Detected (%): 29.057067\n",
            "Fold 3 Results - Features: ['magnitude', 'height', 'weight', 'activity']\n",
            "  Precision: 0.834134\n",
            "  Recall: 0.303248\n",
            "  F1 Score: 0.444792\n",
            "  Noise Detected (%): 30.324793\n",
            "Fold 4 Results - Features: ['magnitude', 'height', 'weight', 'activity']\n",
            "  Precision: 0.842378\n",
            "  Recall: 0.314899\n",
            "  F1 Score: 0.458427\n",
            "  Noise Detected (%): 31.489857\n",
            "Fold 5 Results - Features: ['magnitude', 'height', 'weight', 'activity']\n",
            "  Precision: 0.840014\n",
            "  Recall: 0.315404\n",
            "  F1 Score: 0.458611\n",
            "  Noise Detected (%): 31.540374\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender', 'height', 'weight']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight']\n",
            "  Precision: 0.841662\n",
            "  Recall: 0.306823\n",
            "  F1 Score: 0.449708\n",
            "  Noise Detected (%): 30.682338\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight']\n",
            "  Precision: 0.840619\n",
            "  Recall: 0.300612\n",
            "  F1 Score: 0.442856\n",
            "  Noise Detected (%): 30.061230\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight']\n",
            "  Precision: 0.851129\n",
            "  Recall: 0.325415\n",
            "  F1 Score: 0.470820\n",
            "  Noise Detected (%): 32.541537\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight']\n",
            "  Precision: 0.845328\n",
            "  Recall: 0.309676\n",
            "  F1 Score: 0.453293\n",
            "  Noise Detected (%): 30.967576\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight']\n",
            "  Precision: 0.854031\n",
            "  Recall: 0.330802\n",
            "  F1 Score: 0.476886\n",
            "  Noise Detected (%): 33.080169\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender', 'height', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender', 'height', 'activity']\n",
            "  Precision: 0.851659\n",
            "  Recall: 0.330999\n",
            "  F1 Score: 0.476720\n",
            "  Noise Detected (%): 33.099928\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender', 'height', 'activity']\n",
            "  Precision: 0.856334\n",
            "  Recall: 0.336055\n",
            "  F1 Score: 0.482687\n",
            "  Noise Detected (%): 33.605540\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender', 'height', 'activity']\n",
            "  Precision: 0.849737\n",
            "  Recall: 0.326220\n",
            "  F1 Score: 0.471448\n",
            "  Noise Detected (%): 32.622031\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender', 'height', 'activity']\n",
            "  Precision: 0.854973\n",
            "  Recall: 0.332446\n",
            "  F1 Score: 0.478740\n",
            "  Noise Detected (%): 33.244646\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender', 'height', 'activity']\n",
            "  Precision: 0.853874\n",
            "  Recall: 0.331260\n",
            "  F1 Score: 0.477337\n",
            "  Noise Detected (%): 33.126012\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.854402\n",
            "  Recall: 0.329311\n",
            "  F1 Score: 0.475392\n",
            "  Noise Detected (%): 32.931102\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.851942\n",
            "  Recall: 0.320736\n",
            "  F1 Score: 0.466025\n",
            "  Noise Detected (%): 32.073624\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.851820\n",
            "  Recall: 0.318792\n",
            "  F1 Score: 0.463952\n",
            "  Noise Detected (%): 31.879238\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.853194\n",
            "  Recall: 0.328531\n",
            "  F1 Score: 0.474392\n",
            "  Noise Detected (%): 32.853063\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender', 'weight', 'activity']\n",
            "  Precision: 0.851908\n",
            "  Recall: 0.325127\n",
            "  F1 Score: 0.470638\n",
            "  Noise Detected (%): 32.512728\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'height', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'height', 'weight', 'activity']\n",
            "  Precision: 0.852481\n",
            "  Recall: 0.327137\n",
            "  F1 Score: 0.472827\n",
            "  Noise Detected (%): 32.713659\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'height', 'weight', 'activity']\n",
            "  Precision: 0.848335\n",
            "  Recall: 0.309262\n",
            "  F1 Score: 0.453280\n",
            "  Noise Detected (%): 30.926237\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'height', 'weight', 'activity']\n",
            "  Precision: 0.844288\n",
            "  Recall: 0.317687\n",
            "  F1 Score: 0.461661\n",
            "  Noise Detected (%): 31.768743\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'height', 'weight', 'activity']\n",
            "  Precision: 0.849932\n",
            "  Recall: 0.323489\n",
            "  F1 Score: 0.468619\n",
            "  Noise Detected (%): 32.348932\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'height', 'weight', 'activity']\n",
            "  Precision: 0.853521\n",
            "  Recall: 0.327246\n",
            "  F1 Score: 0.473102\n",
            "  Noise Detected (%): 32.724645\n",
            "\n",
            "Testing feature combination: ['magnitude', 'gender', 'height', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.843017\n",
            "  Recall: 0.325740\n",
            "  F1 Score: 0.469909\n",
            "  Noise Detected (%): 32.574017\n",
            "Fold 2 Results - Features: ['magnitude', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.840692\n",
            "  Recall: 0.309312\n",
            "  F1 Score: 0.452235\n",
            "  Noise Detected (%): 30.931185\n",
            "Fold 3 Results - Features: ['magnitude', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.838343\n",
            "  Recall: 0.308367\n",
            "  F1 Score: 0.450885\n",
            "  Noise Detected (%): 30.836659\n",
            "Fold 4 Results - Features: ['magnitude', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.849808\n",
            "  Recall: 0.328188\n",
            "  F1 Score: 0.473510\n",
            "  Noise Detected (%): 32.818757\n",
            "Fold 5 Results - Features: ['magnitude', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.844007\n",
            "  Recall: 0.319982\n",
            "  F1 Score: 0.464037\n",
            "  Noise Detected (%): 31.998182\n",
            "\n",
            "Testing feature combination: ['magnitude', 'age', 'gender', 'height', 'weight', 'activity']\n",
            "Fold 1 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.853141\n",
            "  Recall: 0.324131\n",
            "  F1 Score: 0.469780\n",
            "  Noise Detected (%): 32.413081\n",
            "Fold 2 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.853912\n",
            "  Recall: 0.327058\n",
            "  F1 Score: 0.472965\n",
            "  Noise Detected (%): 32.705836\n",
            "Fold 3 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.848314\n",
            "  Recall: 0.328146\n",
            "  F1 Score: 0.473234\n",
            "  Noise Detected (%): 32.814570\n",
            "Fold 4 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.852946\n",
            "  Recall: 0.328558\n",
            "  F1 Score: 0.474383\n",
            "  Noise Detected (%): 32.855835\n",
            "Fold 5 Results - Features: ['magnitude', 'age', 'gender', 'height', 'weight', 'activity']\n",
            "  Precision: 0.855068\n",
            "  Recall: 0.330430\n",
            "  F1 Score: 0.476660\n",
            "  Noise Detected (%): 33.042973\n",
            "\n",
            "Feature Selection Results (sorted by Avg F1 Score):\n",
            "                                            Features  Avg F1 Score  Precision  \\\n",
            "27          magnitude, age, gender, height, activity      0.477387   0.853874   \n",
            "12                       magnitude, gender, activity      0.477018   0.852713   \n",
            "23               magnitude, gender, height, activity      0.476701   0.853413   \n",
            "18                  magnitude, age, gender, activity      0.473918   0.850726   \n",
            "31  magnitude, age, gender, height, weight, activity      0.473404   0.855068   \n",
            "5                                magnitude, activity      0.471522   0.851846   \n",
            "28          magnitude, age, gender, weight, activity      0.470080   0.851908   \n",
            "21                  magnitude, age, weight, activity      0.467336   0.854041   \n",
            "29          magnitude, age, height, weight, activity      0.465898   0.853521   \n",
            "19                    magnitude, age, height, weight      0.463526   0.844759   \n",
            "30       magnitude, gender, height, weight, activity      0.462115   0.844007   \n",
            "9                           magnitude, age, activity      0.461813   0.848553   \n",
            "14                       magnitude, height, activity      0.461432   0.839225   \n",
            "15                       magnitude, weight, activity      0.460251   0.844463   \n",
            "20                  magnitude, age, height, activity      0.458859   0.848277   \n",
            "26            magnitude, age, gender, height, weight      0.458713   0.854031   \n",
            "24               magnitude, gender, weight, activity      0.456918   0.844305   \n",
            "25               magnitude, height, weight, activity      0.448520   0.840014   \n",
            "17                    magnitude, age, gender, weight      0.446048   0.843610   \n",
            "8                             magnitude, age, weight      0.441025   0.837192   \n",
            "2                                  magnitude, gender      0.440674   0.846845   \n",
            "16                    magnitude, age, gender, height      0.439777   0.837932   \n",
            "22                 magnitude, gender, height, weight      0.436664   0.835163   \n",
            "6                             magnitude, age, gender      0.432678   0.830530   \n",
            "1                                     magnitude, age      0.432404   0.838524   \n",
            "13                         magnitude, height, weight      0.430777   0.834646   \n",
            "7                             magnitude, age, height      0.429556   0.839929   \n",
            "10                         magnitude, gender, height      0.425591   0.823178   \n",
            "4                                  magnitude, weight      0.422905   0.836616   \n",
            "3                                  magnitude, height      0.420784   0.826743   \n",
            "11                         magnitude, gender, weight      0.419293   0.828283   \n",
            "0                                          magnitude      0.413359   0.835517   \n",
            "\n",
            "      Recall  Noise Detected (%)  \n",
            "27  0.331260           33.126012  \n",
            "12  0.334996           33.499644  \n",
            "23  0.337242           33.724209  \n",
            "18  0.324559           32.455855  \n",
            "31  0.330430           33.042973  \n",
            "5   0.330303           33.030303  \n",
            "28  0.325127           32.512728  \n",
            "21  0.323752           32.375229  \n",
            "29  0.327246           32.724645  \n",
            "19  0.319217           31.921697  \n",
            "30  0.319982           31.998182  \n",
            "9   0.319639           31.963915  \n",
            "14  0.311918           31.191812  \n",
            "15  0.324623           32.462344  \n",
            "20  0.320114           32.011386  \n",
            "26  0.330802           33.080169  \n",
            "24  0.318403           31.840273  \n",
            "25  0.315404           31.540374  \n",
            "17  0.310813           31.081259  \n",
            "8   0.296967           29.696695  \n",
            "2   0.281860           28.185956  \n",
            "16  0.296897           29.689741  \n",
            "22  0.292748           29.274814  \n",
            "6   0.299000           29.899961  \n",
            "1   0.303117           30.311707  \n",
            "13  0.291743           29.174312  \n",
            "7   0.276977           27.697702  \n",
            "10  0.288995           28.899534  \n",
            "4   0.296363           29.636328  \n",
            "3   0.285865           28.586526  \n",
            "11  0.287570           28.756981  \n",
            "0   0.278342           27.834246  \n",
            "Feature selection results saved to '/content/drive/My Drive/ColabNotebooks/feature_selection_results_dbscan.csv'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPuuSBgmdB017cGZBUV7gcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}